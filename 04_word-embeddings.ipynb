{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ae298c",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "===============\n",
    "\n",
    "Our sessions so far have worked off the idea of document annotation to produce new, sometimes highly useful metdata about texts. We've used this information for everything from information retrieval tasks (Chapter 2) to predictive classification (Chapter 3). Along the way, we've also made some passing discussions about how such annotations work to quantify or identify the semantics of those tasks (our work with POS tags, for example). But what we haven't yet done is produce a model of semantic meaning ourselves. This is another core task of NLP, and there are several different ways to approach building a statistical representation of tokens' meanings. The present chapter discusses one of the most popular methods of doing so: **word embeddings**. Below, we'll overview what word embeddings are, demonstrate how to build and use them, talk about important considerations regarding bias, and apply all this to a document clustering task.\n",
    "\n",
    "The corpus we'll use is Melanie Walsh’s [collection] of ~380 obituaries from the _New York Times_. If you participated in our Getting Started with Textual Data series, you'll be familiar with this corpus: we used it [in the context of tf-idf scores]. Our return to it here is meant to chime with that discussion, for word embeddings enable us to perform a similar kind of text vectorization. Though, as we'll discuss, the resultant vectors will be considerably more feature-rich than what we could achieve with tf-idf alone.\n",
    "\n",
    "[collection]: https://melaniewalsh.github.io/Intro-Cultural-Analytics/00-Datasets/00-Datasets.html\n",
    "[in the context of tf-idf scores]: https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/05_clustering-and-classification.html\n",
    "\n",
    "```{admonition} Learning objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "+ Explain what word embeddings are\n",
    "+ Use `gensim` to train and load word embeddings models\n",
    "+ Identify and analyze word relationships in these models\n",
    "+ Recognize how bias can inhere in embeddings\n",
    "+ Encode documents with a word embeddings model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784095c",
   "metadata": {},
   "source": [
    "Word Embeddings: Introduction\n",
    "-------------------------------------\n",
    "\n",
    "Prior to the advent of [Transformer] models, word embedding served as a state-of-the-art technique for representing semantic relationships between tokens. The technique was first introduced in 2013, and it spawned a host of different variants that completely flooded the field of NLP until about 2018. In part, word embedding's popularity stems from the relatively simple intuition behind it, which is known as the **distributional hypothesis**: \"a word is characterized by the company it keeps\" (Firth). Words that appear in similar contexts, in other words, have similar meanings, and what word embeddings do is represent that context-specific information through a set of features. As a result, similar words share similar data representations, and we can leverage that similarity to explore the semantic space of a corpus, to encode documents with feature-rich data, and more.\n",
    "\n",
    "If you're familiar with [tf-idf vectors], the underlying data structure of word embeddings is the same: every word is represented by a vector of _n_ features. But a key difference lies in the **sparsity** of the vectors – or, in the case of word embeddings, the _lack_ of sparsity. As we saw in the last chapter, tf-idf vectors can suffer from the [curse of dimensionality], something that's compounded by the fact that such vectors must contain features for every word in corpus, regardless of whether a document has that word. This means tf-idf vectors are highly sparse: they contain many 0s. Word embeddings, on the other hand, do not. They're what we call **dense** representations. Each one is a fixed-length, non-sparse vector (of 50-300 dimensions, usually) that is much more information-rich than tf-idf. As a result, embeddings tend to be capable of representing more nuanced relationships between corpus words – a performance improvement that is further boosted by the fact that many of the most popular models had the advantage of being trained on billions and billions of tokens.\n",
    "\n",
    "The other major difference between tehse vectors and tf-idf lies in how the former are created. While at root, word embeddings represent token co-occurence data (just like a document-term matrix), they are the product of millions of guesses made by a neural network. Training this network involves making predictions about a target word, based on that word's context. We are not going to delve into the math behind these predictions (though [this post] does); however, it is worth noting that there are two different training set ups for a word embedding model:\n",
    "\n",
    "```{margin} For more on CBOW vs. skip-gram\n",
    "Check out this blog post, [Words as Vectors].\n",
    "\n",
    "[Words as Vectors]: https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "```\n",
    "\n",
    "1. **Common Bag of Words (CBOW)**: given a window of words on either side of a target, the network tries to predict what word the target should be\n",
    "2. **Skip-grams**: the network starts with the word in the middle of a window and picks random words within this window to use as its prediction targets\n",
    "\n",
    "As you may have noticed, these are just mirrored versions of one another. CBOW starts from context, while skip-gram tries to rebuild context. Regardless, in both cases the network attempts to maximize the likelihood of its predictions, updating its weights accordingly over the course of training. Words that repeatedly appear in similar contexts will help shape thse weights, and in turn the model will associate such words with similar vector representations. If you'd like to see all this in action, Xin Rong has produced a [fantastic, interactive visualization] of how word embedding models learn.\n",
    "\n",
    "Of course, the other way to understand how word embeddings work is to use them yourself. We'll move on to doing so now.\n",
    "\n",
    "[Transformer]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
    "[tf-idf vectors]: https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/04_corpus-analytics.html#weighted-metrics-tf-idf-scores\n",
    "[curse of dimensionality]: https://en.wikipedia.org/wiki/Curse_of_dimensionality\n",
    "[this post]: https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b\n",
    "[fantastic, interactive visualization]: https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ff353",
   "metadata": {},
   "source": [
    "Loading the Data\n",
    "--------------------\n",
    "\n",
    "Before we begin working with word embeddings in full, let's load a corpus manifest file, which will help us keep track of all the obituaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff68e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obituaries: 379 \n",
      "Date range: 1852--2007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manifest = pd.read_csv('data/session_three/manifest.csv', index_col=0)\n",
    "manifest = manifest.assign(YEAR = pd.to_datetime(manifest['YEAR'], format='%Y').dt.year)\n",
    "\n",
    "print(\n",
    "    f\"Number of obituaries: {len(manifest)}\",\n",
    "    f\"\\nDate range: {manifest['YEAR'].min()}--{manifest['YEAR'].max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9fb877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "manifest.groupby('YEAR').count().plot(\n",
    "    figsize=(15, 5),\n",
    "    y='NAME',\n",
    "    title='Obituaries per Year',\n",
    "    ylabel='Num. obituaries',\n",
    "    xlabel='Year',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7777",
   "metadata": {},
   "source": [
    "Here's a sampling of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18acb156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haile Selassie (1975)\n",
      "Andres Segovie (1987)\n",
      "Cole Porter (1964)\n",
      "Alf Landon (1987)\n",
      "David O Selznick (1965)\n",
      "David Eisenhower (1969)\n",
      "Geronimo (1909)\n",
      "Maurice Ravel (1937)\n",
      "Anwar el Sadat (1981)\n",
      "Stan Kenton (1979)\n"
     ]
    }
   ],
   "source": [
    "for idx in manifest.sample(10).index:\n",
    "    name, date = manifest.loc[idx, 'NAME'], manifest.loc[idx, 'YEAR']\n",
    "    print(f\"{name} ({date})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bd292",
   "metadata": {},
   "source": [
    "Now we can load the obituaries themselves. While the past two sessions have required full-text representations of documents, word embeddings work best with bags of words, especially when it comes to doing analysis with them. Accordingly, each of the files in the corpus have already processed by a text cleaning pipeline: they represent the lowercase, stopped, and lemmatized versions of the originals.\n",
    "\n",
    "No extra loading considerations are needed here either. We'll just use `glob` to get our file paths and iterate through the list, loading each document into a `corpus` list. Note that we still must split the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2575e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob('data/session_three/obits/*.txt')\n",
    "paths.sort()\n",
    "\n",
    "corpus = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as fin:\n",
    "        doc = fin.read()\n",
    "        doc = doc.split()\n",
    "        corpus.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d5031",
   "metadata": {},
   "source": [
    "With this done, we can move on to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01442f",
   "metadata": {},
   "source": [
    "Using an Embeddings Model\n",
    "----------------------------------\n",
    "\n",
    "At this point, we are at a crossroads. On the one hand, we could train a word embeddings model using our corpus documents as is. The `gensim` library offers functionality for this, and it's a relatively easy operation. On the other, we could use premade embeddings, which are usually trained on a more general – and much larger – set of documents. There is a tradeoff here:\n",
    "\n",
    "+ Training a corpus-specific model will more faithfully represent the token behavior of the texts we'd like to analyze, but these representations could be _too_ specific, especially if the model doesn't have enough data to train on; the resultant embeddings may be closer to topic models than to word-level semantics\n",
    "+ Using premade embeddings gives us the benefit of generalization: the vectors will cleave more closely to how we understand language; but such embeddings might a) miss out on certain nuances we'd like to capture, or b) introduce biases into our corpus (more on this below)\n",
    "\n",
    "In our case, the decision is difficult. When preparing this reader, we (Tyler and Carl) found that a model trained on the obituaries alone did not produce vectors that could fully demonstrate the capabilities of the word embedding technique. The corpus is just a little too specific, and perhaps a little too small. We could've used a larger corpus, but doing so would introduce slow-downs in the workshop session. Because of this, we decided to use a premade model, in this case, the Stanford [GloVe] embeddings (the 200-dimension version). GloVe was trained on billions of tokens, spanning Wikipedia data, newswire articles, even Twitter. More, the model's developers offer several different dimension sizes, which are helpful for selecting embeddings with the right amount of detail.\n",
    "\n",
    "That said, going with GloVe introduces its own problems. For one thing, we can't show you how to train a word embeddings model itself – at least not live. The code to do so, however, is reproduced below:\n",
    "\n",
    "```{margin} Model parameters\n",
    "There are many different parameters to select from in `gensim`. You can find them in the [Word2Vec documentation].\n",
    "\n",
    "[Word2Vec documentation]: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "```\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "n_dimensions = 100\n",
    "model = Word2Vec(n_dimensions)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_words=model.corpus_total_words, epochs=5)\n",
    "```\n",
    "\n",
    "Another problem has to do with the data GloVe was trained on. It's so large that we can't account for all the content, and this becomes particularly detrimental when it comes to bias. [Researchers have found] that general embeddings models reproduce gender-discriminatory language, even hate speech, by virtue of the fact that they are trained on huge amounts of text data, often without consideration of whether the content of such data is something one would endorse. GloVe is [known to be biased] in this way. We'll show an example later on in this chapter and will discuss this in much more detail during our live session, but for now just note that the effects of bias _do_ shape how we represent our corpus, and it's important to keep an eye out for this when working with the data.\n",
    "\n",
    "[GloVe]: https://nlp.stanford.edu/projects/glove/\n",
    "[Researchers have found]: https://www.technologyreview.com/2016/07/27/158634/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/\n",
    "[known to be biased]: http://arxiv.org/abs/1607.06520\n",
    "\n",
    "### Loading a model\n",
    "\n",
    "With all that said, we can move on. Below, we load GloVe embeddings into our workspace using a `gensim` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa49bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load('data/session_three/glove/glove-wiki-gigaword_200d.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e8a26",
   "metadata": {},
   "source": [
    "The `KeyedVectors` object acts almost like a dictionary. You can do certain Python operations directly on it, like using `len()` to find the number of tokens in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a4669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the model: 400,000\n"
     ]
    }
   ],
   "source": [
    "n_tokens = len(model)\n",
    "\n",
    "print(f\"Number of unique tokens in the model: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042060a1",
   "metadata": {},
   "source": [
    "### Token mappings\n",
    "\n",
    "Each token in the model (what `gensim` calls a \"key\") has an associated index. This mapping is accessible via the `.key_to_index` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82fb46d",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843d4e5",
   "metadata": {},
   "source": [
    "If you want to get the vector representation for a token, you can use either the key or the index. The syntax is just like a Python dictionary. Below, we randomly select a single token from the model vocabulary's `.index_to_key` attribute and find the index associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b78a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index position for 'u.a.e.' is 56135\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_token = random.choice(model.index_to_key)\n",
    "rand_idx = model.key_to_index[rand_token]\n",
    "\n",
    "print(f\"The index position for '{rand_token}' is {rand_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3faa4d",
   "metadata": {},
   "source": [
    "Here's its vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29dd6a22",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12157  ,  0.18633  ,  0.53312  ,  0.29472  , -0.1987   ,\n",
       "        0.1607   , -0.6274   ,  0.66265  , -0.07376  , -0.14793  ,\n",
       "        0.2536   ,  0.15215  ,  0.10033  ,  0.0032814,  0.099741 ,\n",
       "        0.24584  , -0.42355  , -0.046631 , -0.064696 ,  0.22395  ,\n",
       "        0.1096   , -0.15957  ,  0.032216 ,  0.20485  , -0.64055  ,\n",
       "       -0.07877  , -0.3653   ,  0.7252   ,  0.33259  , -0.055448 ,\n",
       "        0.37753  , -0.42584  ,  0.078015 , -0.22945  , -0.32251  ,\n",
       "       -0.023816 , -0.096102 , -0.62685  ,  0.81967  , -0.65914  ,\n",
       "        0.11942  , -0.28574  ,  0.092331 , -0.52182  , -0.21709  ,\n",
       "        0.063872 , -0.52587  ,  0.062461 , -0.5442   , -0.13443  ,\n",
       "       -0.18762  , -0.49456  ,  0.092859 , -0.028247 , -0.14954  ,\n",
       "       -0.75058  , -0.37829  , -0.58429  ,  0.068078 , -0.55234  ,\n",
       "        0.11006  , -0.17333  ,  0.078244 ,  0.23563  ,  0.11247  ,\n",
       "        0.087    ,  0.25611  , -0.25113  , -0.24408  , -0.28521  ,\n",
       "       -0.30043  , -0.075735 , -0.27854  ,  0.174    , -0.15264  ,\n",
       "        0.30882  ,  0.22562  ,  0.77446  ,  0.3524   , -0.66199  ,\n",
       "       -0.71989  , -0.47639  ,  0.78197  , -0.41512  , -0.053788 ,\n",
       "        0.34719  ,  0.21788  ,  0.20174  , -0.33795  ,  0.40801  ,\n",
       "       -0.020254 , -0.20206  ,  0.061559 ,  0.16523  , -0.20753  ,\n",
       "        0.47363  ,  0.21529  ,  0.3498   ,  0.066861 , -0.3185   ,\n",
       "       -0.23961  ,  0.84396  , -0.11071  ,  0.041772 , -0.40624  ,\n",
       "        0.41604  ,  0.68039  , -0.1323   ,  0.11113  , -0.89187  ,\n",
       "       -0.54896  , -0.28911  , -0.66509  , -0.005998 ,  0.08664  ,\n",
       "       -0.01959  ,  0.34147  ,  0.70463  ,  0.0016425, -0.13765  ,\n",
       "       -0.011576 , -0.053636 ,  0.064369 ,  0.42049  , -0.7035   ,\n",
       "        0.43037  , -0.43676  , -0.59816  , -0.3017   , -0.19991  ,\n",
       "       -0.34602  ,  0.12486  , -0.65142  , -0.15272  , -0.35982  ,\n",
       "       -0.37575  , -0.2738   , -0.20596  ,  0.365    , -0.46283  ,\n",
       "       -0.10622  , -0.2174   ,  0.20997  ,  0.18927  , -0.25528  ,\n",
       "       -0.38726  ,  0.5616   ,  0.85623  , -0.26363  ,  0.88517  ,\n",
       "        0.10973  , -0.51759  ,  0.14658  ,  0.50088  , -0.73493  ,\n",
       "        0.24685  ,  0.11732  ,  0.28352  ,  0.28549  ,  0.30875  ,\n",
       "        0.69197  ,  0.050793 ,  0.62716  ,  0.20645  , -0.37074  ,\n",
       "        0.12952  , -0.12855  ,  0.3438   , -0.049977 , -0.68666  ,\n",
       "        0.16248  ,  0.088556 , -0.26295  ,  0.082377 ,  0.36326  ,\n",
       "       -0.011784 , -0.32664  , -0.8156   , -0.041987 , -0.53716  ,\n",
       "       -0.094179 , -0.11098  ,  0.17537  ,  0.0040366, -0.055121 ,\n",
       "       -0.18736  , -0.49539  , -0.20035  , -0.48322  , -0.75144  ,\n",
       "        0.075091 ,  0.71182  , -0.086412 , -0.1032   , -0.50825  ,\n",
       "       -0.086991 , -0.26002  , -0.36623  , -0.20033  , -0.24542  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3732b60",
   "metadata": {},
   "source": [
    "And here we show that accessing this vector with either the index or key produces the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b87abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array_equal(model[rand_idx], model[rand_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9c9b7",
   "metadata": {},
   "source": [
    "Finally, we can store the entire model vocabulary in a `set` and show a few examples of the tokens therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50e56cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bazarova\n",
      "gyngell\n",
      "stepan\n",
      "hagana\n",
      "donnay\n",
      "newtownhamilton\n",
      "sacranie\n",
      "illiteracy\n",
      "ausgleich\n",
      "brohi\n"
     ]
    }
   ],
   "source": [
    "model_vocab = set(model.index_to_key)\n",
    "\n",
    "for token in random.sample(model_vocab, 10):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4de567",
   "metadata": {},
   "source": [
    "You may find some unexpected tokens in this output. Though it has been ostensibly trained on an English corpus, GloVe contains multilingual text. It also contains lots of noisy tokens, which range from erroneous segmentations (\"drummer/percussionist\" is one token, for example) to password-like strings and even HTML markup. Depending on your task, you may not notice these tokens, but they do in fact influence the overall shape of the model, and sometimes you'll find them cropping up when you're hunting around for similar terms and the like (more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae401f",
   "metadata": {},
   "source": [
    "### Out-of-vocabulary tokens\n",
    "\n",
    "While GloVe's vocabulary sometimes seems _too_ expansive, there are other instances where it's too restricted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abca184",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not in vocabulary!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_5927/744621433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'unshaped'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not in vocabulary!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Not in vocabulary!"
     ]
    }
   ],
   "source": [
    "assert 'unshaped' in model, \"Not in vocabulary!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c8da8",
   "metadata": {},
   "source": [
    "If the model wasn't trained on a particular word, it won't have a corresponding vector for that word either. This is crucial. Because models like GloVe only know what they've been trained on, you need to be aware of any potential discrepancies between their vocabularies and your corpus data. If you don't keep this in mind, sending unseen, or **out-of-vocabulary**, tokens to GloVe will throw errors in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9210b8a9",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'unshaped' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_5927/3553718512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unshaped'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'unshaped' not present\""
     ]
    }
   ],
   "source": [
    "model['unshaped']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd017ccd",
   "metadata": {},
   "source": [
    "There are a few ways to handle this problem. The most common is to simply _not encode_ tokens in your corpus that don't have a corresponding vector in GloVe. Below, we construct three dictionaries for our corpus data. The first contains all tokens, while the second and third are comprised of tokens that are and are not in Glove, respectively. We identify whether the model has a token using its `.has_index_for()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92e2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the corpus vocabulary: 29,330 \n",
      "Number of corpus words in GloVe: 27,488 \n",
      "Number of corpus words not in GloVe: 1,842\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab = set(token for doc in corpus for token in doc)\n",
    "in_glove = set(token for token in corpus_vocab if model.has_index_for(token))\n",
    "no_glove = set(token for token in corpus_vocab if model.has_index_for(token) == False)\n",
    "\n",
    "print(\n",
    "    f\"Total words in the corpus vocabulary: {len(corpus_vocab):,}\",\n",
    "    f\"\\nNumber of corpus words in GloVe: {len(in_glove):,}\",\n",
    "    f\"\\nNumber of corpus words not in GloVe: {len(no_glove):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac92b5f",
   "metadata": {},
   "source": [
    "Any subsequent code we write will need to reference these dictionaries to determine whether it should encode a token.\n",
    "\n",
    "While this is what we'll indeed do below, obviously it isn't an ideal situation. But it's one of the consequences of using premade models. There are, however, a few other ways to handle out-of-vocabulary terms. Some models offer special \"UNK\" tokens, which you could associate with all of your problem tokens. This, at the very least, enables you to have _some_ representation of your data. A more complex approach involves taking the mean embedding of the word vectors surrounding an unknown token; and depending on the model, you can also train it further, adding extra tokens from your domain-specific text. Instructions for this last option are available [here] in the `gensim` documentation.\n",
    "\n",
    "[here]: https://radimrehurek.com/gensim/models/word2vec.html#usage-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268817e9",
   "metadata": {},
   "source": [
    "Word relationships\n",
    "----------------------\n",
    "\n",
    "Later on we'll use GloVe to encode our corpus texts. But before we do, it's worth demonstrating more generally some of the properties of word vectors. Vector representations of text allow us to perform various mathematical operations on our corpus that approximate (though maybe _only_ approximate) semantics. The most common among these operations is finding the **cosine similarity** between two vectors. Our Getting Started with Textual Data series has a whole [chapter] on this measure, so if you haven't encountered it before, we recommend you read that. But in short: cosine similarity measures the difference between vectors' orientation in a feature space (here, the feature space is comprised of each of the vectors' 200 dimentions). The closer two vectors are, the more likely they are to share semantic similarities.\n",
    "\n",
    "[chapter]: https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/05_clustering-and-classification.html#\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "`gensim` provides easy access to this measure and other such vector space operations, and we can use this functionality to explore relationships between words in a model. To find the cosine similarity between the vectors for two words in GloVe, simply use the model's `.similarity()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c30643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for 'calculate' and 'compute': 0.6991\n"
     ]
    }
   ],
   "source": [
    "a, b = 'calculate', 'compute'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for '{a}' and '{b}': {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bc5ae",
   "metadata": {},
   "source": [
    "The only difference between the score above and the one that you might produce, say, with `scikit-learn`'s cosine similarity implementation is that `gensim` bounds its values from `[-1,1]`, whereas the latter uses a `[0,1]` scale. While in `gensim` it's still the case that similar words score closer to `1`, highly dissimilar words will be closer to `-1`.\n",
    "\n",
    "At any rate, we can get the top _n_ most similar words for a word using `.most_similar()`. The function defaults to 10 entries, but you can change that with the `topn` paramter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "412f067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'quo':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quid</td>\n",
       "      <td>0.660184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vadis</td>\n",
       "      <td>0.540180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>warranto</td>\n",
       "      <td>0.480098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>status</td>\n",
       "      <td>0.467674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sovereignty</td>\n",
       "      <td>0.456606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strait</td>\n",
       "      <td>0.442135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>precedent</td>\n",
       "      <td>0.440231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>facto</td>\n",
       "      <td>0.430301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>maintaining</td>\n",
       "      <td>0.429412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>acceptable</td>\n",
       "      <td>0.426003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0         quid  0.660184\n",
       "1        vadis  0.540180\n",
       "2     warranto  0.480098\n",
       "3       status  0.467674\n",
       "4  sovereignty  0.456606\n",
       "5       strait  0.442135\n",
       "6    precedent  0.440231\n",
       "7        facto  0.430301\n",
       "8  maintaining  0.429412\n",
       "9   acceptable  0.426003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'moonbeams':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>composter</td>\n",
       "      <td>0.470914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bellbottoms</td>\n",
       "      <td>0.469778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sabaoth</td>\n",
       "      <td>0.466961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>odessey</td>\n",
       "      <td>0.466899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>melty</td>\n",
       "      <td>0.463860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>snowsuit</td>\n",
       "      <td>0.457512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summercamp</td>\n",
       "      <td>0.457165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>black/white</td>\n",
       "      <td>0.455277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>schmear</td>\n",
       "      <td>0.450975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lateralism</td>\n",
       "      <td>0.450314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0    composter  0.470914\n",
       "1  bellbottoms  0.469778\n",
       "2      sabaoth  0.466961\n",
       "3      odessey  0.466899\n",
       "4        melty  0.463860\n",
       "5     snowsuit  0.457512\n",
       "6   summercamp  0.457165\n",
       "7  black/white  0.455277\n",
       "8      schmear  0.450975\n",
       "9   lateralism  0.450314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'classify':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>categorize</td>\n",
       "      <td>0.780370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characterize</td>\n",
       "      <td>0.597309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classifying</td>\n",
       "      <td>0.595644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distinguish</td>\n",
       "      <td>0.575743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identify</td>\n",
       "      <td>0.567878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>differentiate</td>\n",
       "      <td>0.560544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>define</td>\n",
       "      <td>0.535191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quantify</td>\n",
       "      <td>0.525734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>categorise</td>\n",
       "      <td>0.525407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>analyze</td>\n",
       "      <td>0.522795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WORD     SCORE\n",
       "0     categorize  0.780370\n",
       "1   characterize  0.597309\n",
       "2    classifying  0.595644\n",
       "3    distinguish  0.575743\n",
       "4       identify  0.567878\n",
       "5  differentiate  0.560544\n",
       "6         define  0.535191\n",
       "7       quantify  0.525734\n",
       "8     categorise  0.525407\n",
       "9        analyze  0.522795"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'connotation':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connotations</td>\n",
       "      <td>0.896568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pejorative</td>\n",
       "      <td>0.766490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denotation</td>\n",
       "      <td>0.621579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overtones</td>\n",
       "      <td>0.610599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>derogatory</td>\n",
       "      <td>0.544127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meanings</td>\n",
       "      <td>0.540334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>undertones</td>\n",
       "      <td>0.518471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>implication</td>\n",
       "      <td>0.508834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>symbolism</td>\n",
       "      <td>0.497179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>metaphorical</td>\n",
       "      <td>0.495735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0  connotations  0.896568\n",
       "1    pejorative  0.766490\n",
       "2    denotation  0.621579\n",
       "3     overtones  0.610599\n",
       "4    derogatory  0.544127\n",
       "5      meanings  0.540334\n",
       "6    undertones  0.518471\n",
       "7   implication  0.508834\n",
       "8     symbolism  0.497179\n",
       "9  metaphorical  0.495735"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'registrar':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>registrars</td>\n",
       "      <td>0.535881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notary</td>\n",
       "      <td>0.507943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clerk</td>\n",
       "      <td>0.499431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tobaiwa</td>\n",
       "      <td>0.487310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probate</td>\n",
       "      <td>0.474675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>registry</td>\n",
       "      <td>0.469386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mudede</td>\n",
       "      <td>0.436448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>registration</td>\n",
       "      <td>0.436189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>solicitor</td>\n",
       "      <td>0.420806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>magistrate</td>\n",
       "      <td>0.415965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0    registrars  0.535881\n",
       "1        notary  0.507943\n",
       "2         clerk  0.499431\n",
       "3       tobaiwa  0.487310\n",
       "4       probate  0.474675\n",
       "5      registry  0.469386\n",
       "6        mudede  0.436448\n",
       "7  registration  0.436189\n",
       "8     solicitor  0.420806\n",
       "9    magistrate  0.415965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = random.sample(in_glove, 5)\n",
    "\n",
    "for token in targets:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3983f5",
   "metadata": {},
   "source": [
    "We can also find the _least_ similar word. This is useful to show, because it pressures our idea of what counts as similarity. Mathematical similarity does not always align with concepts like synonyms and antonyms. For example, it's probably safe to say that the semantic opposite of \"good\" – that is, its antonym – is \"evil.\" But in the world of vector spaces, the least similar word to \"good\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ddb5788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cw96', -0.6553234457969666)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good', topn=len(model))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afacde",
   "metadata": {},
   "source": [
    "Just noise! Relatively speaking, the vectors for \"good\" and \"evil\" are actually quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfd03710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for good and evil: 0.3378\n"
     ]
    }
   ],
   "source": [
    "a, b = 'good', 'evil'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for {a} and {b}: {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ef20e",
   "metadata": {},
   "source": [
    "How do we make sense of this? Well, it has to do with the way the word embeddings are created. Since embeddings models are ultimately trained on co-occurrence data, words that tend to appear in similar kinds of contexts will be more similar in a mathematical sense than those that don't.\n",
    "\n",
    "Keeping this in mind is also important for considerations of bias. Since, in one sense, _embeddings reflect the interchangeability between tokens_, they will reinforce negative, even harmful patterns in the data (which is to say in culture at large). For example, consider the most similar words for \"doctor\" and \"nurse.\" The latter is locked up within gendered language: according to GloVe, a nurse is like a midwife is like a mother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831246c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'doctor':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.736021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.672406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surgeon</td>\n",
       "      <td>0.655147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dr.</td>\n",
       "      <td>0.652498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.648189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.636380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.619159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.584747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>psychiatrist</td>\n",
       "      <td>0.568571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0     physician  0.736021\n",
       "1       doctors  0.672406\n",
       "2       surgeon  0.655147\n",
       "3           dr.  0.652498\n",
       "4         nurse  0.651449\n",
       "5       medical  0.648189\n",
       "6      hospital  0.636380\n",
       "7       patient  0.619159\n",
       "8       dentist  0.584747\n",
       "9  psychiatrist  0.568571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'nurse':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurses</td>\n",
       "      <td>0.714051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.626937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midwife</td>\n",
       "      <td>0.614592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anesthetist</td>\n",
       "      <td>0.610603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.610359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.609222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.586503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>therapist</td>\n",
       "      <td>0.580488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.573556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0       nurses  0.714051\n",
       "1       doctor  0.651449\n",
       "2      nursing  0.626937\n",
       "3      midwife  0.614592\n",
       "4  anesthetist  0.610603\n",
       "5    physician  0.610359\n",
       "6     hospital  0.609222\n",
       "7       mother  0.586503\n",
       "8    therapist  0.580488\n",
       "9      dentist  0.573556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for token in ['doctor', 'nurse']:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f765b0",
   "metadata": {},
   "source": [
    "### Visualizing the vector space\n",
    "\n",
    "One way to start getting a feel for all this is to visualize the word vectors. We do so below by sampling a portion of the GloVe vectors and then reducing them into two-dimensional data, which we can plot. First, let's build two functions.\n",
    "\n",
    "```{margin} How we create the visualization data\n",
    "`sample_embeddings()` takes a sample from GloVe:\n",
    "\n",
    "1. First it randomly selects indices in the model\n",
    "2. Then it uses these to subset the vectors\n",
    "3. Finally it associates the tokens with their respective indices to produce a set of labels\n",
    "\n",
    "`prepare_vis_data()` takes the sampled vectors and their labels and reduces them with a t-SNE embedder\n",
    "\n",
    "1. The `TSNE()` portion of the code does the work of reducing our 200-dimension vectors into only two dimensions\n",
    "2. Then the function converts the two-dimensional data into a dataframe and associates the labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e575b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def sample_embeddings(vectors, samp=1000):\n",
    "    n_vectors = vectors.shape[0]\n",
    "    mask = random.sample(range(n_vectors), samp)\n",
    "    vectors = vectors[mask]\n",
    "    vocab = [model.index_to_key[idx] for idx in mask]\n",
    "    \n",
    "    return vectors, vocab\n",
    "\n",
    "def prepare_vis_data(vectors, labels):\n",
    "    reduced = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate='auto',\n",
    "        init='random',\n",
    "        random_state=357\n",
    "    ).fit_transform(vectors)\n",
    "    \n",
    "    vis_data = pd.DataFrame(reduced, columns=['X', 'Y'])\n",
    "    vis_data['LABEL'] = labels\n",
    "    \n",
    "    return vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a867847",
   "metadata": {},
   "source": [
    "Now we can retrieve all the vectors from GloVe using the `.key_to_index` attribute. With those stored in a `numpy` array, it's time to sample them and create the visualization data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "590ae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.array([model[idx] for idx in model.key_to_index])\n",
    "\n",
    "sampled, sampled_vocab = sample_embeddings(all_vectors)\n",
    "vis_data = prepare_vis_data(sampled, sampled_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f25012",
   "metadata": {},
   "source": [
    "```{margin} This visualization is partial\n",
    "Since we've run a t-SNE reduction on the sampled embeddings, the graph layout only takes into account the relative differences between that sampling, not all of GloVe. Going the latter route would take a while to compute – there would be 400,000 embeddings to reduce!\n",
    "```\n",
    "\n",
    "With the reduced embeddings made, it's time to plot them. Have a look around at the results. What seems right to you? What surprises you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18195af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ee7f5b5bd3ce42988b4d6b773da1a5c1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ee7f5b5bd3ce42988b4d6b773da1a5c1\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ee7f5b5bd3ce42988b4d6b773da1a5c1\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a0a541559522c3ff88bb2b3bb46a5806\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"LABEL\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a0a541559522c3ff88bb2b3bb46a5806\": [{\"X\": -0.8341574668884277, \"Y\": -5.640989780426025, \"LABEL\": \"pwl\"}, {\"X\": -0.6832514405250549, \"Y\": -2.466233491897583, \"LABEL\": \"konduz\"}, {\"X\": -0.7664766907691956, \"Y\": 0.8105050921440125, \"LABEL\": \"pixy\"}, {\"X\": -1.4573144912719727, \"Y\": 1.65822434425354, \"LABEL\": \"chicho\"}, {\"X\": 2.55952787399292, \"Y\": -0.2860061526298523, \"LABEL\": \"lg03\"}, {\"X\": -0.5439713001251221, \"Y\": -4.954339504241943, \"LABEL\": \"monaural\"}, {\"X\": 2.560262441635132, \"Y\": -0.29053962230682373, \"LABEL\": \"pers\"}, {\"X\": 1.2331420183181763, \"Y\": 3.1407666206359863, \"LABEL\": \"liversidge\"}, {\"X\": -6.794559955596924, \"Y\": 0.2640365958213806, \"LABEL\": \"required\"}, {\"X\": 1.6108797788619995, \"Y\": -0.14266736805438995, \"LABEL\": \"icsa\"}, {\"X\": -0.533678412437439, \"Y\": 3.649872303009033, \"LABEL\": \"petar\"}, {\"X\": 4.700603008270264, \"Y\": -2.6406376361846924, \"LABEL\": \"destruct\"}, {\"X\": 1.2648454904556274, \"Y\": 1.9396984577178955, \"LABEL\": \"karimova\"}, {\"X\": -2.9079172611236572, \"Y\": -0.19863714277744293, \"LABEL\": \"violaceae\"}, {\"X\": 6.290111064910889, \"Y\": -1.132955551147461, \"LABEL\": \"yachts\"}, {\"X\": 3.445561647415161, \"Y\": -0.39069458842277527, \"LABEL\": \"wilderness\"}, {\"X\": -2.932133674621582, \"Y\": 5.2121477127075195, \"LABEL\": \"foxpro\"}, {\"X\": 1.3529973030090332, \"Y\": -4.655101299285889, \"LABEL\": \"36.53\"}, {\"X\": 0.4310813248157501, \"Y\": -3.380643367767334, \"LABEL\": \"mutilate\"}, {\"X\": 0.7647846341133118, \"Y\": 4.934394836425781, \"LABEL\": \"swindell\"}, {\"X\": -0.3240927457809448, \"Y\": -0.6911982893943787, \"LABEL\": \"tr2\"}, {\"X\": -0.479547381401062, \"Y\": 0.2738405168056488, \"LABEL\": \"huanca\"}, {\"X\": 0.09879004955291748, \"Y\": 2.7679100036621094, \"LABEL\": \"josif\"}, {\"X\": 2.6692042350769043, \"Y\": -0.3678935766220093, \"LABEL\": \"crs\"}, {\"X\": 1.3836153745651245, \"Y\": -2.339155435562134, \"LABEL\": \"3db\"}, {\"X\": 1.8188036680221558, \"Y\": -0.820834755897522, \"LABEL\": \"eigenvalue\"}, {\"X\": 2.374445915222168, \"Y\": -4.790798187255859, \"LABEL\": \"euro145\"}, {\"X\": 1.9272445440292358, \"Y\": 5.538386821746826, \"LABEL\": \"beathard\"}, {\"X\": 0.8918768167495728, \"Y\": -1.5871914625167847, \"LABEL\": \"semitones\"}, {\"X\": -3.01347017288208, \"Y\": 3.2020485401153564, \"LABEL\": \"hss\"}, {\"X\": -1.4779715538024902, \"Y\": -0.19756144285202026, \"LABEL\": \"duyfken\"}, {\"X\": -0.3229660987854004, \"Y\": 1.6967180967330933, \"LABEL\": \"dijck\"}, {\"X\": 0.34948766231536865, \"Y\": -4.788936614990234, \"LABEL\": \"uncatchable\"}, {\"X\": 4.349098205566406, \"Y\": -1.8294867277145386, \"LABEL\": \"illogic\"}, {\"X\": -3.1204376220703125, \"Y\": 1.943183183670044, \"LABEL\": \"zetkin\"}, {\"X\": 2.8653221130371094, \"Y\": 2.1345741748809814, \"LABEL\": \"bakal\"}, {\"X\": -2.6327226161956787, \"Y\": 2.3881142139434814, \"LABEL\": \"p-3\"}, {\"X\": 1.823285460472107, \"Y\": 4.806952953338623, \"LABEL\": \"demschar\"}, {\"X\": 3.4984171390533447, \"Y\": -2.3244683742523193, \"LABEL\": \"sub-units\"}, {\"X\": 1.3334938287734985, \"Y\": -5.008127212524414, \"LABEL\": \"29.56\"}, {\"X\": -2.9444870948791504, \"Y\": -2.9341793060302734, \"LABEL\": \"copperbelt\"}, {\"X\": 0.7364541292190552, \"Y\": 2.547668218612671, \"LABEL\": \"crerand\"}, {\"X\": 1.5101639032363892, \"Y\": 1.6920140981674194, \"LABEL\": \"khatir\"}, {\"X\": -5.259297847747803, \"Y\": -2.3504772186279297, \"LABEL\": \"yodel\"}, {\"X\": -2.6386444568634033, \"Y\": -1.18440580368042, \"LABEL\": \"satilla\"}, {\"X\": 3.729796886444092, \"Y\": 0.18203216791152954, \"LABEL\": \"looney\"}, {\"X\": 3.670487642288208, \"Y\": 5.0447211265563965, \"LABEL\": \"carret\"}, {\"X\": 1.7613381147384644, \"Y\": 4.024972915649414, \"LABEL\": \"ghiorso\"}, {\"X\": -2.0607519149780273, \"Y\": 2.7734670639038086, \"LABEL\": \"berliet\"}, {\"X\": -6.714200973510742, \"Y\": 0.04821852967143059, \"LABEL\": \"nationality\"}, {\"X\": 1.0607954263687134, \"Y\": -2.1746227741241455, \"LABEL\": \"wjac-tv\"}, {\"X\": -2.839250087738037, \"Y\": -1.6399420499801636, \"LABEL\": \"mehtar\"}, {\"X\": -0.8986293077468872, \"Y\": 0.14568206667900085, \"LABEL\": \"doulos\"}, {\"X\": 3.7229971885681152, \"Y\": -1.8896358013153076, \"LABEL\": \"electropop\"}, {\"X\": 0.5321581959724426, \"Y\": 5.547680854797363, \"LABEL\": \"kayaker\"}, {\"X\": 4.676517009735107, \"Y\": 4.857110977172852, \"LABEL\": \"wirtz\"}, {\"X\": -1.2465980052947998, \"Y\": 1.2703896760940552, \"LABEL\": \"rectus\"}, {\"X\": 2.13431453704834, \"Y\": -2.461561918258667, \"LABEL\": \"1998/1999\"}, {\"X\": 1.4587997198104858, \"Y\": -5.537082672119141, \"LABEL\": \"1.93\"}, {\"X\": 4.502736568450928, \"Y\": -1.8872311115264893, \"LABEL\": \"interposition\"}, {\"X\": 3.865527391433716, \"Y\": -3.2777371406555176, \"LABEL\": \"10percent\"}, {\"X\": 5.833987236022949, \"Y\": 2.291649580001831, \"LABEL\": \"grazia\"}, {\"X\": 1.2112318277359009, \"Y\": 1.195410132408142, \"LABEL\": \"freebasing\"}, {\"X\": 1.4405311346054077, \"Y\": -4.095297813415527, \"LABEL\": \"84.06\"}, {\"X\": -0.8802893161773682, \"Y\": -0.19942013919353485, \"LABEL\": \"paeonia\"}, {\"X\": -0.5448934435844421, \"Y\": 4.148071765899658, \"LABEL\": \"lell\"}, {\"X\": -0.9101085066795349, \"Y\": 5.160912990570068, \"LABEL\": \"luigi\"}, {\"X\": 1.661991834640503, \"Y\": 5.180056095123291, \"LABEL\": \"xiangfu\"}, {\"X\": 1.9011904001235962, \"Y\": 5.828551769256592, \"LABEL\": \"lani\"}, {\"X\": 1.2875109910964966, \"Y\": -0.06702937930822372, \"LABEL\": \"simulink\"}, {\"X\": 3.1141130924224854, \"Y\": -1.1186531782150269, \"LABEL\": \"humberside\"}, {\"X\": -1.0549570322036743, \"Y\": 2.458542823791504, \"LABEL\": \"queir\\u00f3s\"}, {\"X\": 3.598151206970215, \"Y\": 0.8035955429077148, \"LABEL\": \"gall\"}, {\"X\": 0.1595384180545807, \"Y\": 1.5528801679611206, \"LABEL\": \"covan\"}, {\"X\": 2.4880518913269043, \"Y\": -2.884277582168579, \"LABEL\": \"2,400-square\"}, {\"X\": -3.849005937576294, \"Y\": 0.7521065473556519, \"LABEL\": \"kalashnikov\"}, {\"X\": 0.6684975624084473, \"Y\": -1.2477214336395264, \"LABEL\": \"encryptions\"}, {\"X\": -1.5941673517227173, \"Y\": -3.3845999240875244, \"LABEL\": \"podhale\"}, {\"X\": 1.1385356187820435, \"Y\": 0.24417774379253387, \"LABEL\": \"b.u.\"}, {\"X\": -1.3752236366271973, \"Y\": 2.677015781402588, \"LABEL\": \"violeta\"}, {\"X\": -5.132133960723877, \"Y\": -1.2284826040267944, \"LABEL\": \"arouses\"}, {\"X\": -0.08743367344141006, \"Y\": -0.6887300610542297, \"LABEL\": \"lezgin\"}, {\"X\": 2.6595821380615234, \"Y\": 0.15416443347930908, \"LABEL\": \"85-yard\"}, {\"X\": -4.352968215942383, \"Y\": -3.9934825897216797, \"LABEL\": \"refusals\"}, {\"X\": -0.2606852948665619, \"Y\": -0.35921671986579895, \"LABEL\": \"sk8\"}, {\"X\": -1.0197643041610718, \"Y\": -0.9599089026451111, \"LABEL\": \"ultra-left\"}, {\"X\": 2.334359884262085, \"Y\": -4.753499507904053, \"LABEL\": \"14,750\"}, {\"X\": -2.0018222332000732, \"Y\": 1.7366067171096802, \"LABEL\": \"jenapharm\"}, {\"X\": -0.3125905990600586, \"Y\": 3.7468631267547607, \"LABEL\": \"bonf\\u00e1\"}, {\"X\": 2.2465004920959473, \"Y\": 2.898933172225952, \"LABEL\": \"blankstein\"}, {\"X\": 0.8486287593841553, \"Y\": 5.2920379638671875, \"LABEL\": \"felber\"}, {\"X\": 1.2951421737670898, \"Y\": -4.865202903747559, \"LABEL\": \"42.12\"}, {\"X\": -0.37712010741233826, \"Y\": 2.7976105213165283, \"LABEL\": \"malas\"}, {\"X\": 1.7430336475372314, \"Y\": 3.713787317276001, \"LABEL\": \"nagarajan\"}, {\"X\": 2.299452781677246, \"Y\": 6.538066864013672, \"LABEL\": \"tietze\"}, {\"X\": -3.1557891368865967, \"Y\": -2.4826645851135254, \"LABEL\": \"borl\\u00e4nge\"}, {\"X\": 0.7929971218109131, \"Y\": -1.1027542352676392, \"LABEL\": \"peridotites\"}, {\"X\": -3.3728466033935547, \"Y\": -0.5048366189002991, \"LABEL\": \"stoneridge\"}, {\"X\": -0.538048505783081, \"Y\": -0.7069514393806458, \"LABEL\": \"ilinden\"}, {\"X\": -3.6972100734710693, \"Y\": 0.4733959436416626, \"LABEL\": \"eight-track\"}, {\"X\": -0.6052747964859009, \"Y\": -1.7654622793197632, \"LABEL\": \"nasopharynx\"}, {\"X\": -3.3558802604675293, \"Y\": -0.9675375819206238, \"LABEL\": \"shakyamuni\"}, {\"X\": 2.9816524982452393, \"Y\": -3.2544612884521484, \"LABEL\": \"update4\"}, {\"X\": 4.075355052947998, \"Y\": -3.311828136444092, \"LABEL\": \"bloodier\"}, {\"X\": 4.295048713684082, \"Y\": 3.8745648860931396, \"LABEL\": \"valek\"}, {\"X\": 0.5693964958190918, \"Y\": 0.37398383021354675, \"LABEL\": \"crimini\"}, {\"X\": 1.5383381843566895, \"Y\": 3.1306703090667725, \"LABEL\": \"euforia\"}, {\"X\": 2.552415370941162, \"Y\": 0.5933626294136047, \"LABEL\": \"jesi\"}, {\"X\": 1.069987177848816, \"Y\": 2.9805748462677, \"LABEL\": \"cravin\"}, {\"X\": 2.6998555660247803, \"Y\": 5.60491943359375, \"LABEL\": \"barnas\"}, {\"X\": -1.751889705657959, \"Y\": 2.97843337059021, \"LABEL\": \"cleisthenes\"}, {\"X\": -3.8264336585998535, \"Y\": 1.7726470232009888, \"LABEL\": \"beltransgaz\"}, {\"X\": 2.7579636573791504, \"Y\": 2.2228877544403076, \"LABEL\": \"naseem\"}, {\"X\": 0.27316802740097046, \"Y\": 2.8641085624694824, \"LABEL\": \"hultberg\"}, {\"X\": -3.1879470348358154, \"Y\": -0.22442084550857544, \"LABEL\": \"hylidae\"}, {\"X\": -0.20639966428279877, \"Y\": 3.663867235183716, \"LABEL\": \"eber\"}, {\"X\": 2.0013818740844727, \"Y\": 2.4840171337127686, \"LABEL\": \"beh\\u00e7et\"}, {\"X\": -1.2316182851791382, \"Y\": -0.004740084055811167, \"LABEL\": \"oreochromis\"}, {\"X\": 5.13350772857666, \"Y\": 0.6508594751358032, \"LABEL\": \"sensorineural\"}, {\"X\": 2.85832142829895, \"Y\": 1.6908438205718994, \"LABEL\": \"reponsible\"}, {\"X\": 0.741049587726593, \"Y\": -3.5368313789367676, \"LABEL\": \"fwe\"}, {\"X\": 1.2706652879714966, \"Y\": 1.8622868061065674, \"LABEL\": \"antipathies\"}, {\"X\": -0.6416144371032715, \"Y\": 6.855907440185547, \"LABEL\": \"schuettler\"}, {\"X\": 0.0987987220287323, \"Y\": -3.232395648956299, \"LABEL\": \"shoemaking\"}, {\"X\": 3.971707820892334, \"Y\": -0.9579402208328247, \"LABEL\": \"dwells\"}, {\"X\": 1.4914151430130005, \"Y\": 3.1943886280059814, \"LABEL\": \"glasberg\"}, {\"X\": 1.4390805959701538, \"Y\": -4.202808380126953, \"LABEL\": \"98.33\"}, {\"X\": 3.8295583724975586, \"Y\": -4.533789157867432, \"LABEL\": \"pataca\"}, {\"X\": 0.4206639528274536, \"Y\": -2.9873533248901367, \"LABEL\": \"55,400\"}, {\"X\": -1.946234941482544, \"Y\": 1.631150722503662, \"LABEL\": \"front-running\"}, {\"X\": 1.2702839374542236, \"Y\": -0.7535796165466309, \"LABEL\": \"eavesdropper\"}, {\"X\": -0.5085470080375671, \"Y\": 1.0974544286727905, \"LABEL\": \"salafia\"}, {\"X\": 3.004154920578003, \"Y\": -7.2145586013793945, \"LABEL\": \"16-7\"}, {\"X\": -0.5006929636001587, \"Y\": 0.9080226421356201, \"LABEL\": \"ayush\"}, {\"X\": 1.5363272428512573, \"Y\": 1.6440536975860596, \"LABEL\": \"perfections\"}, {\"X\": -2.6298720836639404, \"Y\": -3.074099540710449, \"LABEL\": \"gbarpolu\"}, {\"X\": 3.44419264793396, \"Y\": -2.552682876586914, \"LABEL\": \"redesigns\"}, {\"X\": -2.3261213302612305, \"Y\": 0.7186317443847656, \"LABEL\": \"evangelio\"}, {\"X\": 0.20664562284946442, \"Y\": 4.073826789855957, \"LABEL\": \"xiaomin\"}, {\"X\": 0.05915544554591179, \"Y\": -1.0271388292312622, \"LABEL\": \"meerschaum\"}, {\"X\": 1.5451520681381226, \"Y\": -4.192006587982178, \"LABEL\": \"1.5475\"}, {\"X\": 0.14553794264793396, \"Y\": 4.192619800567627, \"LABEL\": \"baozhong\"}, {\"X\": -0.432973176240921, \"Y\": 2.0174062252044678, \"LABEL\": \"butba\"}, {\"X\": 1.775727391242981, \"Y\": -4.290595054626465, \"LABEL\": \"5.4375\"}, {\"X\": -6.852977752685547, \"Y\": 0.2819008529186249, \"LABEL\": \"tasks\"}, {\"X\": 2.613497495651245, \"Y\": -0.35815709829330444, \"LABEL\": \"formosa\"}, {\"X\": 0.49427080154418945, \"Y\": 1.803836464881897, \"LABEL\": \"songaila\"}, {\"X\": 4.875535011291504, \"Y\": 2.7169201374053955, \"LABEL\": \"libran\"}, {\"X\": 6.245098114013672, \"Y\": -1.067275047302246, \"LABEL\": \"docks\"}, {\"X\": 3.0340232849121094, \"Y\": -1.6936508417129517, \"LABEL\": \"mystify\"}, {\"X\": 0.5287790298461914, \"Y\": -0.596611499786377, \"LABEL\": \"ncop\"}, {\"X\": 0.7273706197738647, \"Y\": -1.1314139366149902, \"LABEL\": \"semi-solid\"}, {\"X\": 3.358769416809082, \"Y\": 0.6343281269073486, \"LABEL\": \"fender\"}, {\"X\": 2.8514533042907715, \"Y\": 3.5768940448760986, \"LABEL\": \"jez\"}, {\"X\": -0.11939310282468796, \"Y\": 3.6196789741516113, \"LABEL\": \"reiche\"}, {\"X\": -2.6743788719177246, \"Y\": -0.9457720518112183, \"LABEL\": \"niraj\"}, {\"X\": 1.5438865423202515, \"Y\": 3.051650047302246, \"LABEL\": \"tasco\"}, {\"X\": 2.2590911388397217, \"Y\": 2.793898820877075, \"LABEL\": \"fayek\"}, {\"X\": -0.6480883955955505, \"Y\": 0.27248314023017883, \"LABEL\": \"sarsi\"}, {\"X\": -6.211056232452393, \"Y\": 2.1084859371185303, \"LABEL\": \"researcher\"}, {\"X\": -1.5445456504821777, \"Y\": -2.2633345127105713, \"LABEL\": \"smyrni\"}, {\"X\": -1.5243706703186035, \"Y\": 1.8572577238082886, \"LABEL\": \"bee-line\"}, {\"X\": -1.814243197441101, \"Y\": 6.267709732055664, \"LABEL\": \"breton\"}, {\"X\": -0.151366725564003, \"Y\": 2.403592824935913, \"LABEL\": \"poliziano\"}, {\"X\": 4.08027982711792, \"Y\": -0.6113278865814209, \"LABEL\": \"woo\"}, {\"X\": -3.554133176803589, \"Y\": 2.632394790649414, \"LABEL\": \"aremissoft\"}, {\"X\": 1.18804931640625, \"Y\": -1.1741447448730469, \"LABEL\": \"client-server\"}, {\"X\": -0.1776277720928192, \"Y\": 1.3772751092910767, \"LABEL\": \"christianunion\"}, {\"X\": -3.119807481765747, \"Y\": -0.8880442380905151, \"LABEL\": \"investimentos\"}, {\"X\": 3.532808303833008, \"Y\": 6.181199550628662, \"LABEL\": \"gimpel\"}, {\"X\": 1.3298567533493042, \"Y\": -2.9853763580322266, \"LABEL\": \"myongji\"}, {\"X\": 3.449561834335327, \"Y\": 4.413099765777588, \"LABEL\": \"naranjo\"}, {\"X\": 2.432490110397339, \"Y\": -4.686059951782227, \"LABEL\": \"2,625\"}, {\"X\": -2.8025059700012207, \"Y\": -0.9701471924781799, \"LABEL\": \"dease\"}, {\"X\": -6.674236297607422, \"Y\": 0.4720742404460907, \"LABEL\": \"operations\"}, {\"X\": 0.04141183942556381, \"Y\": -0.7918419241905212, \"LABEL\": \"therd\"}, {\"X\": 0.9502813816070557, \"Y\": 0.3962189555168152, \"LABEL\": \"recombines\"}, {\"X\": -1.6820204257965088, \"Y\": 0.3136887848377228, \"LABEL\": \"baccalaureus\"}, {\"X\": 0.6634771227836609, \"Y\": 2.9371654987335205, \"LABEL\": \"ygor\"}, {\"X\": 2.0623106956481934, \"Y\": -7.160521030426025, \"LABEL\": \"pawns\"}, {\"X\": -1.4137401580810547, \"Y\": 1.172905683517456, \"LABEL\": \"rocketship\"}, {\"X\": 2.006452798843384, \"Y\": -4.866091728210449, \"LABEL\": \"226.2\"}, {\"X\": -0.04073285683989525, \"Y\": 0.8234747052192688, \"LABEL\": \"bermoy\"}, {\"X\": 0.0664549246430397, \"Y\": -0.6814521551132202, \"LABEL\": \"hogfish\"}, {\"X\": 1.924666404724121, \"Y\": -4.835564613342285, \"LABEL\": \"158.1\"}, {\"X\": -5.409414291381836, \"Y\": 0.7581028938293457, \"LABEL\": \".71098\"}, {\"X\": -1.116141438484192, \"Y\": -2.4236273765563965, \"LABEL\": \"tripa\"}, {\"X\": 2.5993247032165527, \"Y\": 0.3175610303878784, \"LABEL\": \"h2s\"}, {\"X\": -0.3914813995361328, \"Y\": -1.4152895212173462, \"LABEL\": \"hill-fort\"}, {\"X\": -0.3775038421154022, \"Y\": 2.4918758869171143, \"LABEL\": \"dezs\\u0151\"}, {\"X\": -0.4393281638622284, \"Y\": -1.7138069868087769, \"LABEL\": \"padum\"}, {\"X\": 1.195216178894043, \"Y\": -4.398707866668701, \"LABEL\": \"43.97\"}, {\"X\": 4.273993015289307, \"Y\": -0.14012087881565094, \"LABEL\": \"resembling\"}, {\"X\": -1.2983201742172241, \"Y\": -2.2801713943481445, \"LABEL\": \"mont\\u00e9limar\"}, {\"X\": 2.3034474849700928, \"Y\": 4.186949729919434, \"LABEL\": \"amoss\"}, {\"X\": 0.5941663384437561, \"Y\": -0.06957561522722244, \"LABEL\": \"hexed\"}, {\"X\": -3.462432861328125, \"Y\": -2.3827579021453857, \"LABEL\": \"catalana\"}, {\"X\": 0.507089376449585, \"Y\": -1.3676015138626099, \"LABEL\": \"wavefronts\"}, {\"X\": -0.11188981682062149, \"Y\": 1.4101423025131226, \"LABEL\": \"laverda\"}, {\"X\": 3.0612924098968506, \"Y\": 3.5581634044647217, \"LABEL\": \"corriero\"}, {\"X\": -0.41728994250297546, \"Y\": -1.0686079263687134, \"LABEL\": \"roughrider\"}, {\"X\": -4.171320915222168, \"Y\": -3.0732808113098145, \"LABEL\": \"monophthongs\"}, {\"X\": 0.3031098246574402, \"Y\": 1.0411378145217896, \"LABEL\": \"nivola\"}, {\"X\": -3.0423898696899414, \"Y\": -4.868953227996826, \"LABEL\": \"squab\"}, {\"X\": 1.9317708015441895, \"Y\": -1.3584579229354858, \"LABEL\": \"lashinda\"}, {\"X\": -0.65964674949646, \"Y\": -1.6504064798355103, \"LABEL\": \"malpais\"}, {\"X\": 2.8221819400787354, \"Y\": 0.5846834778785706, \"LABEL\": \"washroom\"}, {\"X\": -0.660065770149231, \"Y\": 3.258801221847534, \"LABEL\": \"ycaza\"}, {\"X\": -2.292144775390625, \"Y\": 5.051689147949219, \"LABEL\": \"tournaire\"}, {\"X\": -0.43044623732566833, \"Y\": 4.061995983123779, \"LABEL\": \"rakovsky\"}, {\"X\": -0.2864668071269989, \"Y\": 4.682761192321777, \"LABEL\": \"sabel\"}, {\"X\": -3.4875192642211914, \"Y\": -2.572049140930176, \"LABEL\": \"kalmar\"}, {\"X\": 1.5777091979980469, \"Y\": -6.957333564758301, \"LABEL\": \"--------------------------------------------------\"}, {\"X\": 0.0788361132144928, \"Y\": 5.37105655670166, \"LABEL\": \"karolina\"}, {\"X\": -1.4315212965011597, \"Y\": 0.8011528849601746, \"LABEL\": \"sanguinis\"}, {\"X\": -3.4927985668182373, \"Y\": 0.8272814154624939, \"LABEL\": \"m75\"}, {\"X\": 3.934638738632202, \"Y\": -3.905954122543335, \"LABEL\": \"777s\"}, {\"X\": -0.19145749509334564, \"Y\": -0.49596303701400757, \"LABEL\": \"yanka\"}, {\"X\": -0.5203495621681213, \"Y\": 4.45693826675415, \"LABEL\": \"rosendo\"}, {\"X\": -1.847728967666626, \"Y\": -5.9189677238464355, \"LABEL\": \"ciampino\"}, {\"X\": -0.7723945379257202, \"Y\": 1.5453991889953613, \"LABEL\": \"zeybek\"}, {\"X\": 1.0680383443832397, \"Y\": -2.536806344985962, \"LABEL\": \"kcnc\"}, {\"X\": -2.891649007797241, \"Y\": -1.7422155141830444, \"LABEL\": \"d\\u00e2mbovi\\u0163a\"}, {\"X\": 5.406553745269775, \"Y\": 1.7220027446746826, \"LABEL\": \"shakespear\"}, {\"X\": 2.735668420791626, \"Y\": 4.229511737823486, \"LABEL\": \"gelfond\"}, {\"X\": -3.0081772804260254, \"Y\": 5.879095077514648, \"LABEL\": \"byu\"}, {\"X\": -4.33452844619751, \"Y\": 4.8686628341674805, \"LABEL\": \"cultivars\"}, {\"X\": 0.6595131158828735, \"Y\": 5.681056022644043, \"LABEL\": \"zellweger\"}, {\"X\": 1.1417144536972046, \"Y\": 4.504126071929932, \"LABEL\": \"murphey\"}, {\"X\": 0.06720037013292313, \"Y\": 0.5425319075584412, \"LABEL\": \"egra\"}, {\"X\": 5.933426856994629, \"Y\": -2.2894198894500732, \"LABEL\": \"soundest\"}, {\"X\": -1.9498471021652222, \"Y\": 2.1505820751190186, \"LABEL\": \"formanova\"}, {\"X\": 6.030379295349121, \"Y\": -1.827079176902771, \"LABEL\": \"internets\"}, {\"X\": 3.5550601482391357, \"Y\": 5.205469131469727, \"LABEL\": \"bailard\"}, {\"X\": -0.10940296947956085, \"Y\": 4.685319900512695, \"LABEL\": \"reith\"}, {\"X\": -6.547614574432373, \"Y\": 0.30943089723587036, \"LABEL\": \"collectively\"}, {\"X\": -5.032617092132568, \"Y\": 1.490846872329712, \"LABEL\": \"outdoors\"}, {\"X\": -2.3286244869232178, \"Y\": -2.049168348312378, \"LABEL\": \"majar\"}, {\"X\": 2.9465391635894775, \"Y\": 4.164383411407471, \"LABEL\": \"linskey\"}, {\"X\": 1.9201728105545044, \"Y\": 1.0954103469848633, \"LABEL\": \"dorien\"}, {\"X\": 2.3623995780944824, \"Y\": -0.07390221208333969, \"LABEL\": \"opi\"}, {\"X\": -6.130965709686279, \"Y\": 1.4241201877593994, \"LABEL\": \"probed\"}, {\"X\": 0.3732457756996155, \"Y\": 0.8194500207901001, \"LABEL\": \"hult\\u00e9n\"}, {\"X\": -1.7453410625457764, \"Y\": -2.98152494430542, \"LABEL\": \"betong\"}, {\"X\": 1.8354028463363647, \"Y\": 3.0650691986083984, \"LABEL\": \"delius\"}, {\"X\": 0.7667743563652039, \"Y\": 0.09606995433568954, \"LABEL\": \"on-hold\"}, {\"X\": -0.23088091611862183, \"Y\": 0.5999721884727478, \"LABEL\": \"bookspan\"}, {\"X\": -4.148889541625977, \"Y\": 4.323439121246338, \"LABEL\": \"jalen\"}, {\"X\": -2.825704574584961, \"Y\": 5.912882328033447, \"LABEL\": \"viette\"}, {\"X\": 1.9547775983810425, \"Y\": -4.541258811950684, \"LABEL\": \"215.6\"}, {\"X\": 2.0663232803344727, \"Y\": 4.743513584136963, \"LABEL\": \"labrecque\"}, {\"X\": 4.536716461181641, \"Y\": -3.1699233055114746, \"LABEL\": \"strays\"}, {\"X\": 1.2863616943359375, \"Y\": 2.8882670402526855, \"LABEL\": \"novitzky\"}, {\"X\": -3.3015904426574707, \"Y\": -2.478025197982788, \"LABEL\": \"coru\\u00f1a\"}, {\"X\": 1.888540506362915, \"Y\": -2.134464740753174, \"LABEL\": \"bielik\"}, {\"X\": 3.2864887714385986, \"Y\": -2.125537157058716, \"LABEL\": \"edgiest\"}, {\"X\": 1.4641331434249878, \"Y\": -2.4021146297454834, \"LABEL\": \"lfl\"}, {\"X\": 0.6750640273094177, \"Y\": -4.093905448913574, \"LABEL\": \"startline\"}, {\"X\": 1.1361252069473267, \"Y\": 1.2957518100738525, \"LABEL\": \"gherardi\"}, {\"X\": 4.655553340911865, \"Y\": 1.981085181236267, \"LABEL\": \"tomer\"}, {\"X\": 2.314876079559326, \"Y\": -5.605704307556152, \"LABEL\": \"89.7\"}, {\"X\": 4.852121829986572, \"Y\": 1.352144718170166, \"LABEL\": \"kvasha\"}, {\"X\": 0.5959616303443909, \"Y\": -2.246324062347412, \"LABEL\": \"oceanology\"}, {\"X\": 3.0402040481567383, \"Y\": 6.364861011505127, \"LABEL\": \"urenco\"}, {\"X\": -0.2738756835460663, \"Y\": 2.45145583152771, \"LABEL\": \"clitherow\"}, {\"X\": 0.21048709750175476, \"Y\": 0.5077793002128601, \"LABEL\": \"tingyi\"}, {\"X\": 0.5547500848770142, \"Y\": -2.4878904819488525, \"LABEL\": \"kobelco\"}, {\"X\": 4.81450080871582, \"Y\": -1.1357539892196655, \"LABEL\": \"godly\"}, {\"X\": -2.9641010761260986, \"Y\": -3.5496723651885986, \"LABEL\": \"zawady\"}, {\"X\": 1.7482936382293701, \"Y\": -1.2379242181777954, \"LABEL\": \"z2\"}, {\"X\": 1.6495925188064575, \"Y\": -4.318349838256836, \"LABEL\": \"100.70\"}, {\"X\": 2.338655710220337, \"Y\": 2.408027172088623, \"LABEL\": \"krukowski\"}, {\"X\": 1.36948823928833, \"Y\": 2.1939914226531982, \"LABEL\": \"segarelli\"}, {\"X\": -0.1355932652950287, \"Y\": 0.7080828547477722, \"LABEL\": \"thatha\"}, {\"X\": 0.9344355463981628, \"Y\": 1.3057795763015747, \"LABEL\": \"guadagnino\"}, {\"X\": -0.23368386924266815, \"Y\": -3.080075740814209, \"LABEL\": \"instalacion\"}, {\"X\": 0.5197368860244751, \"Y\": 3.3522815704345703, \"LABEL\": \"peikoff\"}, {\"X\": -3.899196147918701, \"Y\": 2.490874767303467, \"LABEL\": \"arvinmeritor\"}, {\"X\": 2.67333984375, \"Y\": -5.1846723556518555, \"LABEL\": \"957\"}, {\"X\": 1.9804414510726929, \"Y\": 1.3158067464828491, \"LABEL\": \"chusid\"}, {\"X\": 2.618364095687866, \"Y\": -4.225559711456299, \"LABEL\": \"3,937\"}, {\"X\": 1.350249171257019, \"Y\": 6.792627334594727, \"LABEL\": \"huna\"}, {\"X\": -1.2780590057373047, \"Y\": 4.8098883628845215, \"LABEL\": \"edoardo\"}, {\"X\": 3.3552401065826416, \"Y\": -7.489198207855225, \"LABEL\": \"36-point\"}, {\"X\": -1.277424931526184, \"Y\": -1.0943533182144165, \"LABEL\": \"ghum\"}, {\"X\": 1.6273692846298218, \"Y\": -4.069469928741455, \"LABEL\": \"1.4281\"}, {\"X\": 3.108731269836426, \"Y\": -0.8068886399269104, \"LABEL\": \"mutation\"}, {\"X\": -6.997635364532471, \"Y\": 0.030403807759284973, \"LABEL\": \"distances\"}, {\"X\": 1.8715648651123047, \"Y\": 5.237730503082275, \"LABEL\": \"cavender\"}, {\"X\": 3.8189992904663086, \"Y\": 1.3749520778656006, \"LABEL\": \"bessie\"}, {\"X\": -6.7995405197143555, \"Y\": 0.1619521975517273, \"LABEL\": \"extremely\"}, {\"X\": 0.7522003650665283, \"Y\": 3.094602584838867, \"LABEL\": \"buccieri\"}, {\"X\": 1.5964851379394531, \"Y\": 1.6470104455947876, \"LABEL\": \"macan\"}, {\"X\": 0.4974757134914398, \"Y\": 1.1693100929260254, \"LABEL\": \"siw\"}, {\"X\": 5.666508197784424, \"Y\": -2.3908944129943848, \"LABEL\": \"attrition\"}, {\"X\": 2.7847797870635986, \"Y\": 2.697979688644409, \"LABEL\": \"badr\"}, {\"X\": -6.298070907592773, \"Y\": 0.6000655889511108, \"LABEL\": \"targeted\"}, {\"X\": -0.7019875645637512, \"Y\": 3.7535252571105957, \"LABEL\": \"rozsa\"}, {\"X\": 1.323876976966858, \"Y\": -1.8180509805679321, \"LABEL\": \"fefe\"}, {\"X\": 5.425824165344238, \"Y\": -4.448381423950195, \"LABEL\": \"landsat\"}, {\"X\": -0.4075537621974945, \"Y\": 1.737304449081421, \"LABEL\": \"lehoux\"}, {\"X\": 1.8828092813491821, \"Y\": 3.862104654312134, \"LABEL\": \"kordestani\"}, {\"X\": 3.295966148376465, \"Y\": 4.8814873695373535, \"LABEL\": \"karevoll\"}, {\"X\": -1.1452401876449585, \"Y\": 4.000583171844482, \"LABEL\": \"gertrudis\"}, {\"X\": 2.493502616882324, \"Y\": 2.817469835281372, \"LABEL\": \"gizbert\"}, {\"X\": 5.850224494934082, \"Y\": 3.537335157394409, \"LABEL\": \"shadid\"}, {\"X\": -2.549166202545166, \"Y\": -3.1950724124908447, \"LABEL\": \"avasin\"}, {\"X\": -0.4036034941673279, \"Y\": 1.2722147703170776, \"LABEL\": \"finansbank\"}, {\"X\": 0.35222023725509644, \"Y\": -6.191296100616455, \"LABEL\": \"ocbc\"}, {\"X\": 0.4893631041049957, \"Y\": 2.7076611518859863, \"LABEL\": \"broyard\"}, {\"X\": 4.401623725891113, \"Y\": 5.103367328643799, \"LABEL\": \"stifel\"}, {\"X\": 1.8980889320373535, \"Y\": 3.031337022781372, \"LABEL\": \"nozick\"}, {\"X\": 0.6660794019699097, \"Y\": 2.3093721866607666, \"LABEL\": \"hatmaker\"}, {\"X\": 0.5672381520271301, \"Y\": -5.836025714874268, \"LABEL\": \"tamanoshima\"}, {\"X\": -2.3282546997070312, \"Y\": -4.379241943359375, \"LABEL\": \"unibond\"}, {\"X\": 4.331693172454834, \"Y\": -0.7545916438102722, \"LABEL\": \"demystifies\"}, {\"X\": 3.7053096294403076, \"Y\": -0.48272013664245605, \"LABEL\": \"1a\"}, {\"X\": 2.7833151817321777, \"Y\": -0.5877468585968018, \"LABEL\": \"reuters\"}, {\"X\": 0.2536660432815552, \"Y\": -5.3728346824646, \"LABEL\": \"career-threatening\"}, {\"X\": -1.3631060123443604, \"Y\": -5.168335914611816, \"LABEL\": \"outsources\"}, {\"X\": -2.357832431793213, \"Y\": -2.94209623336792, \"LABEL\": \"xianyang\"}, {\"X\": -4.044990539550781, \"Y\": 0.03291115164756775, \"LABEL\": \"townsmen\"}, {\"X\": 0.5016952157020569, \"Y\": 3.2557690143585205, \"LABEL\": \"pilzer\"}, {\"X\": 4.617712020874023, \"Y\": 4.071313381195068, \"LABEL\": \"capshaw\"}, {\"X\": -2.232851028442383, \"Y\": 1.5962921380996704, \"LABEL\": \"sulpicians\"}, {\"X\": 0.34171420335769653, \"Y\": 2.712932825088501, \"LABEL\": \"thicknesse\"}, {\"X\": 0.6475635170936584, \"Y\": -7.06744909286499, \"LABEL\": \"1.4100\"}, {\"X\": -0.013392816297709942, \"Y\": -1.807276964187622, \"LABEL\": \"massada\"}, {\"X\": -0.5029288530349731, \"Y\": 3.592238187789917, \"LABEL\": \"agrippina\"}, {\"X\": 1.3195252418518066, \"Y\": -1.2449949979782104, \"LABEL\": \"pull-down\"}, {\"X\": 1.7416160106658936, \"Y\": -1.8108203411102295, \"LABEL\": \"t10\"}, {\"X\": 6.088401794433594, \"Y\": -0.15557290613651276, \"LABEL\": \"cliffie\"}, {\"X\": -1.0788695812225342, \"Y\": 2.187934160232544, \"LABEL\": \"v\\u0103c\\u0103rescu\"}, {\"X\": 3.268172025680542, \"Y\": 0.25484901666641235, \"LABEL\": \"tamar\"}, {\"X\": 4.789547443389893, \"Y\": -1.701256513595581, \"LABEL\": \"bravura\"}, {\"X\": -2.265807867050171, \"Y\": 2.7843363285064697, \"LABEL\": \"phalangists\"}, {\"X\": 1.326335072517395, \"Y\": 6.075955867767334, \"LABEL\": \"shevah\"}, {\"X\": -1.7485812902450562, \"Y\": -3.248533010482788, \"LABEL\": \"lavant\"}, {\"X\": 2.293394088745117, \"Y\": 2.0709447860717773, \"LABEL\": \"bistrong\"}, {\"X\": 0.7050478458404541, \"Y\": -2.1602389812469482, \"LABEL\": \"tlm\"}, {\"X\": 2.1841158866882324, \"Y\": -3.466420888900757, \"LABEL\": \"10,927\"}, {\"X\": -0.46106746792793274, \"Y\": 3.1167826652526855, \"LABEL\": \"hirzebruch\"}, {\"X\": 7.404720306396484, \"Y\": 2.5970852375030518, \"LABEL\": \"7-foot\"}, {\"X\": 0.7857396602630615, \"Y\": 1.120802879333496, \"LABEL\": \"uslu\"}, {\"X\": 3.345311403274536, \"Y\": 4.86302375793457, \"LABEL\": \"terrile\"}, {\"X\": 1.9153637886047363, \"Y\": 1.3119045495986938, \"LABEL\": \"vasoli\"}, {\"X\": 2.638829231262207, \"Y\": 1.3215190172195435, \"LABEL\": \"vavasseur\"}, {\"X\": 0.9225525259971619, \"Y\": -0.39433231949806213, \"LABEL\": \"pussyfooting\"}, {\"X\": 3.370100259780884, \"Y\": -0.2924511134624481, \"LABEL\": \"#ffffff\"}, {\"X\": 2.5868313312530518, \"Y\": 1.6464996337890625, \"LABEL\": \"breese\"}, {\"X\": -1.4410879611968994, \"Y\": -2.178947925567627, \"LABEL\": \"salsette\"}, {\"X\": 4.334538459777832, \"Y\": -2.1747043132781982, \"LABEL\": \"ploy\"}, {\"X\": 6.1575093269348145, \"Y\": -0.2378363311290741, \"LABEL\": \"ashlar\"}, {\"X\": 0.8080100417137146, \"Y\": 0.7275372743606567, \"LABEL\": \"eurhythmics\"}, {\"X\": 2.4045088291168213, \"Y\": 0.447758287191391, \"LABEL\": \"four-volume\"}, {\"X\": 1.103159785270691, \"Y\": -4.552323818206787, \"LABEL\": \"80.61\"}, {\"X\": 0.8432682752609253, \"Y\": 4.404050827026367, \"LABEL\": \"fadden\"}, {\"X\": 0.9323850870132446, \"Y\": 2.064486265182495, \"LABEL\": \"wellbeloved\"}, {\"X\": 1.2996368408203125, \"Y\": 5.215869903564453, \"LABEL\": \"helenio\"}, {\"X\": 0.20719046890735626, \"Y\": 3.62674880027771, \"LABEL\": \"kasas\"}, {\"X\": 2.7206039428710938, \"Y\": 4.679429054260254, \"LABEL\": \"portney\"}, {\"X\": 2.3506083488464355, \"Y\": 4.945435047149658, \"LABEL\": \"zanoyan\"}, {\"X\": 0.19628265500068665, \"Y\": 2.3997936248779297, \"LABEL\": \"nyiszli\"}, {\"X\": 2.6639301776885986, \"Y\": 5.5249762535095215, \"LABEL\": \"sirola\"}, {\"X\": -1.9925827980041504, \"Y\": 0.14591415226459503, \"LABEL\": \"gottes\"}, {\"X\": -1.9602041244506836, \"Y\": -3.6761796474456787, \"LABEL\": \"marakwet\"}, {\"X\": 5.097282409667969, \"Y\": -1.2119207382202148, \"LABEL\": \"headstrong\"}, {\"X\": 2.166407585144043, \"Y\": -0.559556245803833, \"LABEL\": \"rrf\"}, {\"X\": 1.1347066164016724, \"Y\": 0.8181185126304626, \"LABEL\": \"zoi\"}, {\"X\": 2.0737287998199463, \"Y\": 1.9707285165786743, \"LABEL\": \"riegelwood\"}, {\"X\": 3.918726682662964, \"Y\": 4.885054588317871, \"LABEL\": \"ivor\"}, {\"X\": 3.1119370460510254, \"Y\": 0.33440977334976196, \"LABEL\": \"xvii\"}, {\"X\": 5.360928058624268, \"Y\": 3.050189733505249, \"LABEL\": \"clay-court\"}, {\"X\": 0.8876743912696838, \"Y\": -0.3943233788013458, \"LABEL\": \"sensationalised\"}, {\"X\": -0.40417784452438354, \"Y\": -2.713242769241333, \"LABEL\": \"dolomites\"}, {\"X\": -6.8209547996521, \"Y\": 0.8021942377090454, \"LABEL\": \"2009\"}, {\"X\": -3.8889684677124023, \"Y\": 2.4729790687561035, \"LABEL\": \"allergan\"}, {\"X\": -0.9788051843643188, \"Y\": -0.9070079326629639, \"LABEL\": \"485-yard\"}, {\"X\": 0.22238270938396454, \"Y\": 2.542813777923584, \"LABEL\": \"desideri\"}, {\"X\": -2.330430507659912, \"Y\": -0.8813690543174744, \"LABEL\": \"antlfinger\"}, {\"X\": -2.7192447185516357, \"Y\": 0.3219296634197235, \"LABEL\": \"aerovias\"}, {\"X\": -4.385717868804932, \"Y\": -3.2937872409820557, \"LABEL\": \"latencies\"}, {\"X\": 1.5614392757415771, \"Y\": 2.9108569622039795, \"LABEL\": \"chumbley\"}, {\"X\": 2.6819381713867188, \"Y\": -4.1384429931640625, \"LABEL\": \"4,146\"}, {\"X\": 1.9531253576278687, \"Y\": 4.252765655517578, \"LABEL\": \"parente\"}, {\"X\": -0.5330323576927185, \"Y\": 0.8065242767333984, \"LABEL\": \"kandu\"}, {\"X\": 2.3198845386505127, \"Y\": -0.6102616786956787, \"LABEL\": \"dodgernote\"}, {\"X\": -1.7326890230178833, \"Y\": -0.6044562458992004, \"LABEL\": \"killifish\"}, {\"X\": 3.5048749446868896, \"Y\": -1.7314989566802979, \"LABEL\": \"floetry\"}, {\"X\": -2.1906721591949463, \"Y\": -7.014582633972168, \"LABEL\": \"1209\"}, {\"X\": -1.070164680480957, \"Y\": -1.5979151725769043, \"LABEL\": \"depopulating\"}, {\"X\": 0.680397629737854, \"Y\": -0.7304176092147827, \"LABEL\": \"deep-rooted\"}, {\"X\": 4.152795314788818, \"Y\": -1.5440282821655273, \"LABEL\": \"hangdog\"}, {\"X\": -1.5614490509033203, \"Y\": 2.8849799633026123, \"LABEL\": \"drest\"}, {\"X\": 3.8175482749938965, \"Y\": 0.8313263654708862, \"LABEL\": \"morneau\"}, {\"X\": 2.42044734954834, \"Y\": 3.47836971282959, \"LABEL\": \"schuon\"}, {\"X\": 5.689149856567383, \"Y\": 1.05062735080719, \"LABEL\": \"ophuls\"}, {\"X\": 2.316732406616211, \"Y\": 3.772691488265991, \"LABEL\": \"pinsky\"}, {\"X\": 0.7040148377418518, \"Y\": -3.0596117973327637, \"LABEL\": \"akshardham\"}, {\"X\": 1.4942580461502075, \"Y\": -5.327556133270264, \"LABEL\": \"10.66\"}, {\"X\": -0.8860684633255005, \"Y\": 3.3108861446380615, \"LABEL\": \"kathe\"}, {\"X\": 2.974705696105957, \"Y\": -4.067750930786133, \"LABEL\": \"points-scoring\"}, {\"X\": 0.4915670156478882, \"Y\": 0.253656268119812, \"LABEL\": \"yowza\"}, {\"X\": 3.049197196960449, \"Y\": -2.7531845569610596, \"LABEL\": \"round-trip\"}, {\"X\": -0.030790045857429504, \"Y\": 5.626523494720459, \"LABEL\": \"xiaoqing\"}, {\"X\": -3.387500762939453, \"Y\": -1.3980836868286133, \"LABEL\": \"doon\"}, {\"X\": -0.08477681875228882, \"Y\": -4.272541522979736, \"LABEL\": \"goaltampa\"}, {\"X\": 2.1696319580078125, \"Y\": 1.2144243717193604, \"LABEL\": \"cascarino\"}, {\"X\": 0.892259418964386, \"Y\": -0.4668886661529541, \"LABEL\": \"fictionist\"}, {\"X\": 1.4330172538757324, \"Y\": -2.8630409240722656, \"LABEL\": \"vh1.com\"}, {\"X\": -1.7006043195724487, \"Y\": 0.7664531469345093, \"LABEL\": \"trionfo\"}, {\"X\": 1.8731203079223633, \"Y\": -0.06721392273902893, \"LABEL\": \"f22\"}, {\"X\": 0.7222315073013306, \"Y\": -0.055011458694934845, \"LABEL\": \"additively\"}, {\"X\": -2.1854686737060547, \"Y\": -0.17275381088256836, \"LABEL\": \"niverville\"}, {\"X\": 2.4243860244750977, \"Y\": 6.720552444458008, \"LABEL\": \"terje\"}, {\"X\": 5.529038429260254, \"Y\": -2.676847457885742, \"LABEL\": \"secreting\"}, {\"X\": -0.6202602982521057, \"Y\": 0.42684102058410645, \"LABEL\": \"reko\"}, {\"X\": -0.2843624949455261, \"Y\": 0.26549166440963745, \"LABEL\": \"boshi\"}, {\"X\": 2.8372766971588135, \"Y\": 4.967695236206055, \"LABEL\": \"tarazi\"}, {\"X\": 1.4138734340667725, \"Y\": 4.553879261016846, \"LABEL\": \"haubner\"}, {\"X\": 3.5442655086517334, \"Y\": 5.842828273773193, \"LABEL\": \"peverill\"}, {\"X\": 1.4271366596221924, \"Y\": 0.2436240017414093, \"LABEL\": \"grb2\"}, {\"X\": 1.9454498291015625, \"Y\": -1.2308212518692017, \"LABEL\": \"ssbn\"}, {\"X\": 2.8275539875030518, \"Y\": 0.12326999753713608, \"LABEL\": \"unattended\"}, {\"X\": 4.3512091636657715, \"Y\": -5.595941543579102, \"LABEL\": \"45-room\"}, {\"X\": 1.133912205696106, \"Y\": -0.5050506591796875, \"LABEL\": \"alkyne\"}, {\"X\": 4.1263556480407715, \"Y\": 3.365375518798828, \"LABEL\": \"saroyan\"}, {\"X\": -1.1920056343078613, \"Y\": 4.774106025695801, \"LABEL\": \"ganguli\"}, {\"X\": -6.71690559387207, \"Y\": 0.1463243067264557, \"LABEL\": \"circumstances\"}, {\"X\": -1.1497094631195068, \"Y\": 5.067713737487793, \"LABEL\": \"soler\"}, {\"X\": 1.1547175645828247, \"Y\": 2.228715419769287, \"LABEL\": \"garric\"}, {\"X\": 0.49229729175567627, \"Y\": -0.4049600660800934, \"LABEL\": \"lodestone\"}, {\"X\": -2.492387533187866, \"Y\": -2.8583076000213623, \"LABEL\": \"luquan\"}, {\"X\": 2.435171604156494, \"Y\": -4.673206329345703, \"LABEL\": \"2,467\"}, {\"X\": -1.2007373571395874, \"Y\": -2.892606496810913, \"LABEL\": \"386-member\"}, {\"X\": 0.6684868335723877, \"Y\": 3.500880241394043, \"LABEL\": \"babich\"}, {\"X\": -0.3672890365123749, \"Y\": 6.718193054199219, \"LABEL\": \"siemerink\"}, {\"X\": 2.7001473903656006, \"Y\": -0.3436751663684845, \"LABEL\": \"members.aol.com\"}, {\"X\": -2.6543893814086914, \"Y\": 0.2559621036052704, \"LABEL\": \"h\\u00f4tel\"}, {\"X\": -1.221866488456726, \"Y\": 1.6540014743804932, \"LABEL\": \"qingli\"}, {\"X\": 3.081923246383667, \"Y\": -5.582451343536377, \"LABEL\": \"291\"}, {\"X\": 2.5273053646087646, \"Y\": 3.510591745376587, \"LABEL\": \"kyamko\"}, {\"X\": 0.3319140672683716, \"Y\": -0.8595917224884033, \"LABEL\": \"pre-novice\"}, {\"X\": -3.996337652206421, \"Y\": -4.6890387535095215, \"LABEL\": \"chuckwalla\"}, {\"X\": -0.10382182896137238, \"Y\": 4.858615398406982, \"LABEL\": \"mechthild\"}, {\"X\": -0.7219805717468262, \"Y\": -0.7456979155540466, \"LABEL\": \"nereid\"}, {\"X\": 1.0144017934799194, \"Y\": -2.9611408710479736, \"LABEL\": \"wkpr\"}, {\"X\": -0.01110850926488638, \"Y\": 0.07434298098087311, \"LABEL\": \"pistou\"}, {\"X\": -2.3313612937927246, \"Y\": -1.053794264793396, \"LABEL\": \"b\\u00e2rsa\"}, {\"X\": 3.789407968521118, \"Y\": -0.15518642961978912, \"LABEL\": \"skateboard\"}, {\"X\": -0.9869707822799683, \"Y\": 2.7859792709350586, \"LABEL\": \"snr\"}, {\"X\": 0.2242220789194107, \"Y\": 3.519078493118286, \"LABEL\": \"finestone\"}, {\"X\": 6.898923397064209, \"Y\": -1.732573390007019, \"LABEL\": \"featurettes\"}, {\"X\": -0.610605776309967, \"Y\": -5.286716938018799, \"LABEL\": \"whiteboard\"}, {\"X\": -1.3208755254745483, \"Y\": 0.005781541112810373, \"LABEL\": \"babylonica\"}, {\"X\": 0.4077589213848114, \"Y\": 3.632112741470337, \"LABEL\": \"pucher\"}, {\"X\": 2.096898078918457, \"Y\": -4.458312511444092, \"LABEL\": \"31-month\"}, {\"X\": -0.7071765065193176, \"Y\": 5.571490287780762, \"LABEL\": \"neisser\"}, {\"X\": -3.8421850204467773, \"Y\": 2.678405284881592, \"LABEL\": \"maemo\"}, {\"X\": 0.4328983426094055, \"Y\": 2.9783380031585693, \"LABEL\": \"mcqueary\"}, {\"X\": -0.5870309472084045, \"Y\": -3.7158589363098145, \"LABEL\": \"horrem\"}, {\"X\": -1.0736874341964722, \"Y\": -0.6091815233230591, \"LABEL\": \"ast\\u00e9rix\"}, {\"X\": 4.17218017578125, \"Y\": -4.687427043914795, \"LABEL\": \"berekum\"}, {\"X\": -3.731327772140503, \"Y\": 2.4102962017059326, \"LABEL\": \"myogen\"}, {\"X\": -2.1448850631713867, \"Y\": 3.4349887371063232, \"LABEL\": \"laomedon\"}, {\"X\": 3.825700283050537, \"Y\": -0.0750560536980629, \"LABEL\": \"gazebo\"}, {\"X\": 5.041383266448975, \"Y\": -1.5671907663345337, \"LABEL\": \"lyrical\"}, {\"X\": 4.664499759674072, \"Y\": -4.322763919830322, \"LABEL\": \"tadiran\"}, {\"X\": -0.6756851077079773, \"Y\": 6.871788024902344, \"LABEL\": \"sugiyama\"}, {\"X\": 3.3842573165893555, \"Y\": 3.673670530319214, \"LABEL\": \"garaufis\"}, {\"X\": -2.0038931369781494, \"Y\": 3.193173885345459, \"LABEL\": \"winchilsea\"}, {\"X\": 0.30288076400756836, \"Y\": 4.1502156257629395, \"LABEL\": \"taisuke\"}, {\"X\": 3.9502618312835693, \"Y\": 2.0554211139678955, \"LABEL\": \"avesnes\"}, {\"X\": 0.05334174260497093, \"Y\": -2.625579595565796, \"LABEL\": \"fisheye\"}, {\"X\": -4.021685600280762, \"Y\": -4.703893184661865, \"LABEL\": \"juancito\"}, {\"X\": 0.6116398572921753, \"Y\": 2.0834810733795166, \"LABEL\": \"mkhatshwa\"}, {\"X\": -2.767336130142212, \"Y\": 2.3995211124420166, \"LABEL\": \"paiutes\"}, {\"X\": 3.4154345989227295, \"Y\": 4.8617072105407715, \"LABEL\": \"rightmire\"}, {\"X\": 1.2708854675292969, \"Y\": -1.0649851560592651, \"LABEL\": \"scintillation\"}, {\"X\": -1.2038336992263794, \"Y\": 3.7339751720428467, \"LABEL\": \"ushkowitz\"}, {\"X\": -1.9501466751098633, \"Y\": -6.0989603996276855, \"LABEL\": \"flawless\"}, {\"X\": -1.487657904624939, \"Y\": 0.6281567811965942, \"LABEL\": \"anche\"}, {\"X\": -2.8646275997161865, \"Y\": 4.621053695678711, \"LABEL\": \"razzoli\"}, {\"X\": 0.8709179759025574, \"Y\": 4.309681415557861, \"LABEL\": \"mcgivern\"}, {\"X\": 2.136695384979248, \"Y\": -5.641120910644531, \"LABEL\": \"25.2\"}, {\"X\": 1.3257898092269897, \"Y\": -1.105731725692749, \"LABEL\": \"varve\"}, {\"X\": 1.617374062538147, \"Y\": 0.8931758999824524, \"LABEL\": \"arrazola\"}, {\"X\": 5.949397563934326, \"Y\": -0.5035739541053772, \"LABEL\": \"tautly\"}, {\"X\": 0.3470037281513214, \"Y\": 3.2390217781066895, \"LABEL\": \"timchenko\"}, {\"X\": 4.389121055603027, \"Y\": -2.2473320960998535, \"LABEL\": \"tartness\"}, {\"X\": 0.18585002422332764, \"Y\": -0.17680372297763824, \"LABEL\": \"university-affiliated\"}, {\"X\": 2.720513105392456, \"Y\": -2.6074042320251465, \"LABEL\": \"22-meter\"}, {\"X\": 2.22823166847229, \"Y\": 1.8921419382095337, \"LABEL\": \"hargov\"}, {\"X\": 4.597951889038086, \"Y\": 1.3735586404800415, \"LABEL\": \"mvo\"}, {\"X\": -0.01669260300695896, \"Y\": 0.28243300318717957, \"LABEL\": \"idrottsf\\u00f6rening\"}, {\"X\": 5.62348747253418, \"Y\": -3.0956923961639404, \"LABEL\": \"ivorians\"}, {\"X\": 4.1989264488220215, \"Y\": -2.211036205291748, \"LABEL\": \"gorgeousness\"}, {\"X\": 2.0326027870178223, \"Y\": 4.482320785522461, \"LABEL\": \"fauteux\"}, {\"X\": 0.5497974753379822, \"Y\": -1.63145112991333, \"LABEL\": \"cecs\"}, {\"X\": 1.231710433959961, \"Y\": 7.547107696533203, \"LABEL\": \"guernica\"}, {\"X\": 2.3873636722564697, \"Y\": -3.4004015922546387, \"LABEL\": \"8,032\"}, {\"X\": -2.6378445625305176, \"Y\": -2.448934555053711, \"LABEL\": \"ubay\"}, {\"X\": 4.564536094665527, \"Y\": 2.2259700298309326, \"LABEL\": \"eberts\"}, {\"X\": 4.8938679695129395, \"Y\": -1.6613904237747192, \"LABEL\": \"ritualistic\"}, {\"X\": -0.16939611732959747, \"Y\": 0.24918515980243683, \"LABEL\": \"tshogpa\"}, {\"X\": -0.9852527976036072, \"Y\": -2.7922346591949463, \"LABEL\": \"ikirun\"}, {\"X\": 1.0628342628479004, \"Y\": 2.5453054904937744, \"LABEL\": \"besler\"}, {\"X\": 2.626343011856079, \"Y\": 3.2439184188842773, \"LABEL\": \"isah\"}, {\"X\": 1.801439881324768, \"Y\": 3.583061695098877, \"LABEL\": \"cohon\"}, {\"X\": -0.9143248796463013, \"Y\": -6.952151775360107, \"LABEL\": \"5:12\"}, {\"X\": 5.134191989898682, \"Y\": 0.363077312707901, \"LABEL\": \"vinalhaven\"}, {\"X\": -0.9646295309066772, \"Y\": -2.834625244140625, \"LABEL\": \"aures\"}, {\"X\": 1.4982796907424927, \"Y\": 0.13531073927879333, \"LABEL\": \"telangiectasia\"}, {\"X\": -5.46599006652832, \"Y\": 0.33696696162223816, \"LABEL\": \"stark\"}, {\"X\": 1.0590349435806274, \"Y\": 2.137871265411377, \"LABEL\": \"oleksandra\"}, {\"X\": 3.2984631061553955, \"Y\": -1.571943998336792, \"LABEL\": \"extroversion\"}, {\"X\": -0.8952677249908447, \"Y\": -4.621051788330078, \"LABEL\": \"protectees\"}, {\"X\": 4.603542804718018, \"Y\": -0.23681192100048065, \"LABEL\": \"accented\"}, {\"X\": 0.9038111567497253, \"Y\": 3.1691601276397705, \"LABEL\": \"cayabyab\"}, {\"X\": -0.7584024667739868, \"Y\": -2.8304128646850586, \"LABEL\": \"torgau\"}, {\"X\": -0.2888166010379791, \"Y\": 0.39078494906425476, \"LABEL\": \"odde\"}, {\"X\": 0.8787877559661865, \"Y\": 0.4063917100429535, \"LABEL\": \"neomycin\"}, {\"X\": -1.0837688446044922, \"Y\": -1.1638034582138062, \"LABEL\": \"sufa\"}, {\"X\": 5.8779473304748535, \"Y\": 1.4004936218261719, \"LABEL\": \"odets\"}, {\"X\": 1.1176180839538574, \"Y\": 1.956891655921936, \"LABEL\": \"sowden\"}, {\"X\": -5.162918567657471, \"Y\": 0.8582786917686462, \"LABEL\": \"airfield\"}, {\"X\": 1.971254825592041, \"Y\": 4.555995941162109, \"LABEL\": \"karbowiak\"}, {\"X\": -1.4715607166290283, \"Y\": -2.155383586883545, \"LABEL\": \"gida\"}, {\"X\": 3.564995765686035, \"Y\": 3.422799825668335, \"LABEL\": \"durland\"}, {\"X\": -2.1988794803619385, \"Y\": 0.3448939919471741, \"LABEL\": \"termes\"}, {\"X\": -0.06544090062379837, \"Y\": 1.1735002994537354, \"LABEL\": \"ba'al\"}, {\"X\": -3.3059303760528564, \"Y\": 4.599452018737793, \"LABEL\": \"bran\"}, {\"X\": -2.3122265338897705, \"Y\": -2.6425747871398926, \"LABEL\": \"bel\\u00e9m\"}, {\"X\": 3.1391208171844482, \"Y\": -0.5303352475166321, \"LABEL\": \"macdougal\"}, {\"X\": -3.787992477416992, \"Y\": -1.4015660285949707, \"LABEL\": \"redefinition\"}, {\"X\": -0.9282009601593018, \"Y\": 2.944406747817993, \"LABEL\": \"kagekatsu\"}, {\"X\": -4.599332332611084, \"Y\": 3.28642201423645, \"LABEL\": \"butchers\"}, {\"X\": -0.5060707330703735, \"Y\": -4.863620758056641, \"LABEL\": \"unlocking\"}, {\"X\": -2.950792074203491, \"Y\": 0.9251293540000916, \"LABEL\": \"9-pound\"}, {\"X\": 1.9107275009155273, \"Y\": -0.9320465922355652, \"LABEL\": \"implosive\"}, {\"X\": 1.3779040575027466, \"Y\": 2.472954750061035, \"LABEL\": \"salvino\"}, {\"X\": 1.3154685497283936, \"Y\": 0.23588590323925018, \"LABEL\": \"adminstrative\"}, {\"X\": 0.8340660929679871, \"Y\": -0.3470047414302826, \"LABEL\": \"bim\"}, {\"X\": 0.6781876087188721, \"Y\": -0.3816535174846649, \"LABEL\": \"inculcates\"}, {\"X\": 1.9225900173187256, \"Y\": 3.7909042835235596, \"LABEL\": \"swander\"}, {\"X\": -2.6604135036468506, \"Y\": -1.150004267692566, \"LABEL\": \"levisa\"}, {\"X\": -1.4493190050125122, \"Y\": 0.3095302879810333, \"LABEL\": \"b\\u00fccher\"}, {\"X\": 1.2771430015563965, \"Y\": -5.820312023162842, \"LABEL\": \"3/8\"}, {\"X\": 1.745016098022461, \"Y\": 1.0012506246566772, \"LABEL\": \"huntleigh\"}, {\"X\": -1.1417165994644165, \"Y\": -2.2430598735809326, \"LABEL\": \"felgueiras\"}, {\"X\": 0.1940176784992218, \"Y\": -2.379592180252075, \"LABEL\": \"sanitise\"}, {\"X\": -1.6926264762878418, \"Y\": -3.0558505058288574, \"LABEL\": \"riihim\\u00e4ki\"}, {\"X\": 3.7597551345825195, \"Y\": 2.486565351486206, \"LABEL\": \"rozelle\"}, {\"X\": 1.1969988346099854, \"Y\": 2.4713189601898193, \"LABEL\": \"muguet\"}, {\"X\": 1.3662209510803223, \"Y\": -2.4366519451141357, \"LABEL\": \"biannual\"}, {\"X\": -1.8972612619400024, \"Y\": -1.3271620273590088, \"LABEL\": \"muanda\"}, {\"X\": 0.3297114968299866, \"Y\": 0.2721726596355438, \"LABEL\": \"tinissa\"}, {\"X\": 2.0867857933044434, \"Y\": -2.2324297428131104, \"LABEL\": \"ikos\"}, {\"X\": 1.0031633377075195, \"Y\": 4.069399356842041, \"LABEL\": \"hayakawa\"}, {\"X\": 3.09352970123291, \"Y\": -1.2210612297058105, \"LABEL\": \"evanescence\"}, {\"X\": -6.509546279907227, \"Y\": -0.11210766434669495, \"LABEL\": \"thoughts\"}, {\"X\": 4.411301612854004, \"Y\": 0.7116470336914062, \"LABEL\": \"trapper\"}, {\"X\": -5.831417560577393, \"Y\": -0.3583184480667114, \"LABEL\": \"conceivable\"}, {\"X\": 0.4572642743587494, \"Y\": 6.912830829620361, \"LABEL\": \"dani\"}, {\"X\": -0.37274447083473206, \"Y\": -0.5511460304260254, \"LABEL\": \"amrutha\"}, {\"X\": -0.1520489603281021, \"Y\": 0.37308192253112793, \"LABEL\": \"ri0\"}, {\"X\": 2.6717207431793213, \"Y\": -0.22104334831237793, \"LABEL\": \"cores\"}, {\"X\": 4.010628700256348, \"Y\": -3.336533308029175, \"LABEL\": \"sexier\"}, {\"X\": 0.729695737361908, \"Y\": -1.328230619430542, \"LABEL\": \"microporous\"}, {\"X\": -2.1092491149902344, \"Y\": 1.7757989168167114, \"LABEL\": \"dirgantara\"}, {\"X\": 1.0996884107589722, \"Y\": -1.945209264755249, \"LABEL\": \"idem\"}, {\"X\": -4.237066268920898, \"Y\": -0.1405419558286667, \"LABEL\": \"picketed\"}, {\"X\": 2.1644575595855713, \"Y\": -1.3485058546066284, \"LABEL\": \"artilleryman\"}, {\"X\": 3.5197763442993164, \"Y\": -1.5991791486740112, \"LABEL\": \"situational\"}, {\"X\": 1.9576542377471924, \"Y\": 3.751546859741211, \"LABEL\": \"ouriel\"}, {\"X\": -6.824506759643555, \"Y\": 0.3275521695613861, \"LABEL\": \"job\"}, {\"X\": 1.5106534957885742, \"Y\": -3.329068422317505, \"LABEL\": \"b-plus\"}, {\"X\": 1.9645564556121826, \"Y\": 1.5351084470748901, \"LABEL\": \"de'ath\"}, {\"X\": 4.484964370727539, \"Y\": -5.683821678161621, \"LABEL\": \"600-room\"}, {\"X\": -1.8542468547821045, \"Y\": -5.669597148895264, \"LABEL\": \"severny\"}, {\"X\": -2.166472911834717, \"Y\": -7.011395454406738, \"LABEL\": \"1357\"}, {\"X\": -1.0525168180465698, \"Y\": -0.10639221221208572, \"LABEL\": \"helicina\"}, {\"X\": 5.436440467834473, \"Y\": -2.5704152584075928, \"LABEL\": \"stealthily\"}, {\"X\": -2.9205210208892822, \"Y\": 1.0105986595153809, \"LABEL\": \"lippobank\"}, {\"X\": 1.0201729536056519, \"Y\": -0.0244159996509552, \"LABEL\": \"ghostscript\"}, {\"X\": -3.0667028427124023, \"Y\": 0.9906288981437683, \"LABEL\": \"auditorio\"}, {\"X\": 0.8636156916618347, \"Y\": -5.051912307739258, \"LABEL\": \"guangfa\"}, {\"X\": 4.229831218719482, \"Y\": -0.8376088738441467, \"LABEL\": \"objectification\"}, {\"X\": -2.5479464530944824, \"Y\": -4.643984317779541, \"LABEL\": \"co-offensive\"}, {\"X\": 2.887348175048828, \"Y\": -2.926297187805176, \"LABEL\": \"steeling\"}, {\"X\": -2.971587896347046, \"Y\": -3.3936288356781006, \"LABEL\": \"zamo\\u015b\\u0107\"}, {\"X\": -1.20649254322052, \"Y\": 3.28939151763916, \"LABEL\": \"laverde\"}, {\"X\": 1.3753989934921265, \"Y\": -0.18813464045524597, \"LABEL\": \"hyperkeratosis\"}, {\"X\": 1.5552576780319214, \"Y\": 0.18664449453353882, \"LABEL\": \"stop-gap\"}, {\"X\": 6.770982265472412, \"Y\": 0.6097332835197449, \"LABEL\": \"anthers\"}, {\"X\": 1.266075611114502, \"Y\": -1.9432090520858765, \"LABEL\": \"glideslope\"}, {\"X\": 1.023516297340393, \"Y\": -2.307429790496826, \"LABEL\": \"internatioanl\"}, {\"X\": -3.766247272491455, \"Y\": 2.7829151153564453, \"LABEL\": \"motorola\"}, {\"X\": 4.343757629394531, \"Y\": -2.239624261856079, \"LABEL\": \"rejiggering\"}, {\"X\": -0.14782895147800446, \"Y\": -4.445633411407471, \"LABEL\": \"free-throw\"}, {\"X\": -0.9954631328582764, \"Y\": -6.54843807220459, \"LABEL\": \"stadthalle\"}, {\"X\": -0.26996850967407227, \"Y\": 3.5208652019500732, \"LABEL\": \"heym\"}, {\"X\": 1.250433087348938, \"Y\": -4.4959893226623535, \"LABEL\": \"54.39\"}, {\"X\": 6.78607702255249, \"Y\": 0.6201834678649902, \"LABEL\": \"hair-like\"}, {\"X\": -1.5344310998916626, \"Y\": -4.421184062957764, \"LABEL\": \"5/5\"}, {\"X\": 0.22508729994297028, \"Y\": 0.2088339626789093, \"LABEL\": \"3.6730\"}, {\"X\": -1.2222319841384888, \"Y\": 5.695556163787842, \"LABEL\": \"hector\"}, {\"X\": 1.4580219984054565, \"Y\": 2.0869247913360596, \"LABEL\": \"naeim\"}, {\"X\": 2.5646231174468994, \"Y\": 5.216402053833008, \"LABEL\": \"gibowski\"}, {\"X\": -1.6755043268203735, \"Y\": 1.3541728258132935, \"LABEL\": \"pedicab\"}, {\"X\": -4.419611930847168, \"Y\": -2.211975574493408, \"LABEL\": \"domesticates\"}, {\"X\": -1.149236798286438, \"Y\": 0.6958897709846497, \"LABEL\": \"volksbanken\"}, {\"X\": -1.8838297128677368, \"Y\": 0.020560411736369133, \"LABEL\": \"stimmen\"}, {\"X\": -1.4550901651382446, \"Y\": -3.553602695465088, \"LABEL\": \"chorleywood\"}, {\"X\": -2.2741148471832275, \"Y\": 4.381314277648926, \"LABEL\": \"shor\"}, {\"X\": 4.686670303344727, \"Y\": 3.6830384731292725, \"LABEL\": \"blaylock\"}, {\"X\": -0.5898248553276062, \"Y\": -0.5346195697784424, \"LABEL\": \"swimmin\"}, {\"X\": -2.383415460586548, \"Y\": -2.6886062622070312, \"LABEL\": \"coevorden\"}, {\"X\": 4.271432876586914, \"Y\": -0.05862605571746826, \"LABEL\": \"murals\"}, {\"X\": 2.6809966564178467, \"Y\": -5.090750694274902, \"LABEL\": \"1,599\"}, {\"X\": 5.282447814941406, \"Y\": -1.2835793495178223, \"LABEL\": \"undisciplined\"}, {\"X\": -0.5333945751190186, \"Y\": -1.1713582277297974, \"LABEL\": \"tyrolian\"}, {\"X\": 1.7447742223739624, \"Y\": -3.327244997024536, \"LABEL\": \"nyt15\"}, {\"X\": 0.34578993916511536, \"Y\": 0.5529401302337646, \"LABEL\": \"stik\"}, {\"X\": 0.8990280032157898, \"Y\": 5.849163055419922, \"LABEL\": \"karla\"}, {\"X\": 1.8895803689956665, \"Y\": -0.8033458590507507, \"LABEL\": \"wabtec\"}, {\"X\": 2.4904372692108154, \"Y\": -2.6997857093811035, \"LABEL\": \"70-kilometre\"}, {\"X\": -2.4153544902801514, \"Y\": 2.115305185317993, \"LABEL\": \"motorrad\"}, {\"X\": -2.375237464904785, \"Y\": 3.6000468730926514, \"LABEL\": \"kiewit\"}, {\"X\": 4.185478210449219, \"Y\": -2.1885242462158203, \"LABEL\": \"mirthful\"}, {\"X\": 1.667198896408081, \"Y\": 1.2852567434310913, \"LABEL\": \"hoskote\"}, {\"X\": -0.6349367499351501, \"Y\": -0.47295036911964417, \"LABEL\": \"basestar\"}, {\"X\": -7.413496971130371, \"Y\": 1.3654389381408691, \"LABEL\": \"candidacy\"}, {\"X\": -1.7720165252685547, \"Y\": 3.159842014312744, \"LABEL\": \"seongjong\"}, {\"X\": -2.297722101211548, \"Y\": 0.4457896053791046, \"LABEL\": \"gestis\"}, {\"X\": -0.7498145699501038, \"Y\": 6.909345626831055, \"LABEL\": \"almagro\"}, {\"X\": -0.31805330514907837, \"Y\": 1.9228452444076538, \"LABEL\": \"westen\"}, {\"X\": 1.2900500297546387, \"Y\": 3.17819881439209, \"LABEL\": \"melancthon\"}, {\"X\": -1.4073739051818848, \"Y\": -3.416692018508911, \"LABEL\": \"a63\"}, {\"X\": 4.165248870849609, \"Y\": 1.8151901960372925, \"LABEL\": \"l.j.\"}, {\"X\": 4.332903861999512, \"Y\": 0.3571879267692566, \"LABEL\": \"alligator\"}, {\"X\": 4.118356227874756, \"Y\": 2.83804988861084, \"LABEL\": \"inglehart\"}, {\"X\": 2.593099355697632, \"Y\": -1.301708459854126, \"LABEL\": \"ultrahigh\"}, {\"X\": 1.333133578300476, \"Y\": -3.547335147857666, \"LABEL\": \"3rd-5th\"}, {\"X\": 2.9967806339263916, \"Y\": -7.191164970397949, \"LABEL\": \"28-13\"}, {\"X\": 0.6401090025901794, \"Y\": -2.640410900115967, \"LABEL\": \"donners\"}, {\"X\": 0.3284813165664673, \"Y\": -1.24153470993042, \"LABEL\": \"rtve\"}, {\"X\": 2.9884605407714844, \"Y\": 0.9525832533836365, \"LABEL\": \"blackie\"}, {\"X\": 0.19942790269851685, \"Y\": -3.8427343368530273, \"LABEL\": \"jiangling\"}, {\"X\": 0.17021621763706207, \"Y\": 2.4605040550231934, \"LABEL\": \"guiterrez\"}, {\"X\": -1.1328097581863403, \"Y\": 5.6966023445129395, \"LABEL\": \"andrade\"}, {\"X\": 3.191169023513794, \"Y\": 2.9019994735717773, \"LABEL\": \"balice\"}, {\"X\": 2.968409299850464, \"Y\": 0.189064159989357, \"LABEL\": \"valderrama\"}, {\"X\": 2.584782838821411, \"Y\": 3.5454025268554688, \"LABEL\": \"cornfeld\"}, {\"X\": 4.262943267822266, \"Y\": 3.0337555408477783, \"LABEL\": \"jerningham\"}, {\"X\": 1.1017186641693115, \"Y\": 2.431016445159912, \"LABEL\": \"bassons\"}, {\"X\": 3.14284348487854, \"Y\": 1.2501524686813354, \"LABEL\": \"j\\u017e\"}, {\"X\": -2.365211009979248, \"Y\": 1.3680561780929565, \"LABEL\": \"bagless\"}, {\"X\": -2.5715200901031494, \"Y\": -1.0712271928787231, \"LABEL\": \"bistri\\u021ba\"}, {\"X\": -0.41351863741874695, \"Y\": -1.227175235748291, \"LABEL\": \"synagogal\"}, {\"X\": 3.40120792388916, \"Y\": -0.6141301393508911, \"LABEL\": \"hs\"}, {\"X\": -0.12012641876935959, \"Y\": -0.07047250121831894, \"LABEL\": \"rg1\"}, {\"X\": -1.421887755393982, \"Y\": -2.795793056488037, \"LABEL\": \"stubbington\"}, {\"X\": -1.414931297302246, \"Y\": -3.067315101623535, \"LABEL\": \"waterfoot\"}, {\"X\": 0.28925034403800964, \"Y\": 0.706960916519165, \"LABEL\": \"va'enuku\"}, {\"X\": -2.2097413539886475, \"Y\": -1.2471084594726562, \"LABEL\": \"ambel\"}, {\"X\": 2.711019515991211, \"Y\": -6.644285678863525, \"LABEL\": \"108-97\"}, {\"X\": -0.3471228778362274, \"Y\": 3.0885469913482666, \"LABEL\": \"nikoli\\u0107\"}, {\"X\": -2.595433235168457, \"Y\": -0.1940295696258545, \"LABEL\": \"sac-winged\"}, {\"X\": -1.2451415061950684, \"Y\": -0.2179141342639923, \"LABEL\": \"n\\u00e9r\\u00e9ide\"}, {\"X\": 1.113753080368042, \"Y\": -0.3585355281829834, \"LABEL\": \"pervasively\"}, {\"X\": -2.2959275245666504, \"Y\": -4.484716892242432, \"LABEL\": \"chiangrai\"}, {\"X\": 3.339257001876831, \"Y\": 5.029336929321289, \"LABEL\": \"binzel\"}, {\"X\": 2.972315788269043, \"Y\": -7.17032527923584, \"LABEL\": \"23-17\"}, {\"X\": 1.7744388580322266, \"Y\": -2.469862222671509, \"LABEL\": \"odawara\"}, {\"X\": 0.7079980373382568, \"Y\": -4.49571418762207, \"LABEL\": \"starbase\"}, {\"X\": 0.9285810589790344, \"Y\": -0.9648697376251221, \"LABEL\": \"tocotrienols\"}, {\"X\": -2.981228828430176, \"Y\": -3.467606544494629, \"LABEL\": \"mak\\u00f3w\"}, {\"X\": -0.4087730050086975, \"Y\": -1.1045500040054321, \"LABEL\": \"reconstruction-era\"}, {\"X\": 0.4877021014690399, \"Y\": 4.529754638671875, \"LABEL\": \"t\\u00e1naiste\"}, {\"X\": 2.2876205444335938, \"Y\": -4.015368938446045, \"LABEL\": \"52,700\"}, {\"X\": -0.6823305487632751, \"Y\": 4.227080821990967, \"LABEL\": \"pashanski\"}, {\"X\": 0.4002804458141327, \"Y\": -2.5584893226623535, \"LABEL\": \"sanitize\"}, {\"X\": -1.5914770364761353, \"Y\": -0.636418342590332, \"LABEL\": \"abukuma\"}, {\"X\": -0.41995176672935486, \"Y\": -2.4295923709869385, \"LABEL\": \"pencilling\"}, {\"X\": 3.1094343662261963, \"Y\": -5.622354030609131, \"LABEL\": \"264\"}, {\"X\": 2.504326581954956, \"Y\": 2.0905590057373047, \"LABEL\": \"kamila\"}, {\"X\": 2.721346616744995, \"Y\": -1.1884266138076782, \"LABEL\": \"avidity\"}, {\"X\": -3.3693079948425293, \"Y\": 3.6464855670928955, \"LABEL\": \"reiki\"}, {\"X\": -0.6915136575698853, \"Y\": 0.31359708309173584, \"LABEL\": \"chor\\u0105\\u017cy\"}, {\"X\": 2.71543025970459, \"Y\": 5.6813788414001465, \"LABEL\": \"ajuonuma\"}, {\"X\": 6.7858781814575195, \"Y\": -1.7728632688522339, \"LABEL\": \"webisodes\"}, {\"X\": 5.258972644805908, \"Y\": -2.425100326538086, \"LABEL\": \"torturous\"}, {\"X\": -1.6661289930343628, \"Y\": -1.8150676488876343, \"LABEL\": \"sloe\"}, {\"X\": 0.12431690096855164, \"Y\": 0.44366908073425293, \"LABEL\": \"candesartan\"}, {\"X\": 4.909939765930176, \"Y\": -0.6915772557258606, \"LABEL\": \"clatters\"}, {\"X\": -5.589606761932373, \"Y\": -0.7920774221420288, \"LABEL\": \"rooting\"}, {\"X\": -0.09530546516180038, \"Y\": 5.172979354858398, \"LABEL\": \"serkis\"}, {\"X\": 3.324399948120117, \"Y\": -7.463619709014893, \"LABEL\": \"44-point\"}, {\"X\": -0.26336196064949036, \"Y\": 3.4820518493652344, \"LABEL\": \"cherchesov\"}, {\"X\": 0.5440764427185059, \"Y\": -4.685610771179199, \"LABEL\": \"escheated\"}, {\"X\": -1.4864734411239624, \"Y\": -1.5802057981491089, \"LABEL\": \"bunol\"}, {\"X\": -0.8392516374588013, \"Y\": -0.14768391847610474, \"LABEL\": \"liezi\"}, {\"X\": 6.286784648895264, \"Y\": -2.830918788909912, \"LABEL\": \"kickstarter\"}, {\"X\": 0.36384400725364685, \"Y\": 1.554878830909729, \"LABEL\": \"ostman\"}, {\"X\": -5.085896968841553, \"Y\": -1.1009342670440674, \"LABEL\": \"reassures\"}, {\"X\": -6.134974956512451, \"Y\": 0.647824764251709, \"LABEL\": \"tribal\"}, {\"X\": 1.264809012413025, \"Y\": -4.586625099182129, \"LABEL\": \"g/ml\"}, {\"X\": -4.246692180633545, \"Y\": -2.170362949371338, \"LABEL\": \"parters\"}, {\"X\": 0.6492833495140076, \"Y\": -7.058145999908447, \"LABEL\": \"1.3300\"}, {\"X\": 1.4552477598190308, \"Y\": -1.4935919046401978, \"LABEL\": \"three-digit\"}, {\"X\": 4.505843162536621, \"Y\": 4.7638726234436035, \"LABEL\": \"auth\"}, {\"X\": -0.37553170323371887, \"Y\": -3.352853298187256, \"LABEL\": \"herradura\"}, {\"X\": 3.017249584197998, \"Y\": -0.1693984866142273, \"LABEL\": \"checkups\"}, {\"X\": -6.201612949371338, \"Y\": 1.6991413831710815, \"LABEL\": \"researching\"}, {\"X\": -3.7746057510375977, \"Y\": 1.18880295753479, \"LABEL\": \"bouma\"}, {\"X\": 1.164233684539795, \"Y\": -2.289808750152588, \"LABEL\": \"itv4\"}, {\"X\": 0.0036464505828917027, \"Y\": 3.370640754699707, \"LABEL\": \"grassl\"}, {\"X\": 4.042679309844971, \"Y\": -3.13041353225708, \"LABEL\": \"smidge\"}, {\"X\": -0.5132850408554077, \"Y\": 2.1022183895111084, \"LABEL\": \"stankevicius\"}, {\"X\": -2.284701347351074, \"Y\": 1.9775488376617432, \"LABEL\": \"tri-motor\"}, {\"X\": 0.11401417851448059, \"Y\": -1.8262522220611572, \"LABEL\": \"chiasm\"}, {\"X\": -1.572561264038086, \"Y\": 4.183897495269775, \"LABEL\": \"zeise\"}, {\"X\": -4.265983581542969, \"Y\": -4.049999713897705, \"LABEL\": \"hindrances\"}, {\"X\": 4.486840724945068, \"Y\": -2.5730481147766113, \"LABEL\": \"bellybutton\"}, {\"X\": 0.15340356528759003, \"Y\": -3.042842149734497, \"LABEL\": \"non-timber\"}, {\"X\": -2.1166532039642334, \"Y\": -1.51158607006073, \"LABEL\": \"dalong\"}, {\"X\": -1.5134034156799316, \"Y\": -3.0816662311553955, \"LABEL\": \"namsos\"}, {\"X\": -0.8932472467422485, \"Y\": 0.9856873154640198, \"LABEL\": \"dusseldorp\"}, {\"X\": 2.5988943576812744, \"Y\": -1.5243507623672485, \"LABEL\": \"penndot\"}, {\"X\": -0.9376810193061829, \"Y\": 2.69065523147583, \"LABEL\": \"stohl\"}, {\"X\": 0.48264849185943604, \"Y\": 1.9528493881225586, \"LABEL\": \"cooly\"}, {\"X\": -0.2216450721025467, \"Y\": -2.137394428253174, \"LABEL\": \"boomtowns\"}, {\"X\": 3.027496337890625, \"Y\": 2.08317494392395, \"LABEL\": \"howes\"}, {\"X\": 2.770573377609253, \"Y\": -0.6317859888076782, \"LABEL\": \"ante\"}, {\"X\": 4.030344486236572, \"Y\": 1.161239743232727, \"LABEL\": \"mikva\"}, {\"X\": 2.6297004222869873, \"Y\": -5.0181379318237305, \"LABEL\": \"1,682\"}, {\"X\": 0.8068556785583496, \"Y\": 1.2467707395553589, \"LABEL\": \"jiming\"}, {\"X\": 1.106695294380188, \"Y\": 2.1049458980560303, \"LABEL\": \"arimura\"}, {\"X\": -5.114989757537842, \"Y\": -2.785109758377075, \"LABEL\": \"outlooks\"}, {\"X\": -0.3089248239994049, \"Y\": -1.2863410711288452, \"LABEL\": \"nilradical\"}, {\"X\": 0.09980495274066925, \"Y\": 3.1322410106658936, \"LABEL\": \"ierodiaconou\"}, {\"X\": 5.848381519317627, \"Y\": 5.104788780212402, \"LABEL\": \"kwapis\"}, {\"X\": -0.302593469619751, \"Y\": 2.570828437805176, \"LABEL\": \"meillon\"}, {\"X\": 4.465209484100342, \"Y\": 4.377076625823975, \"LABEL\": \"merrifield\"}, {\"X\": 5.825064182281494, \"Y\": 5.090928554534912, \"LABEL\": \"vandermark\"}, {\"X\": 0.7168691158294678, \"Y\": 4.196808815002441, \"LABEL\": \"zelia\"}, {\"X\": 2.039263963699341, \"Y\": 4.907598495483398, \"LABEL\": \"harri\"}, {\"X\": -2.089001178741455, \"Y\": -3.786590576171875, \"LABEL\": \"gishu\"}, {\"X\": 2.5284886360168457, \"Y\": 3.9823665618896484, \"LABEL\": \"nobuaki\"}, {\"X\": 2.2055652141571045, \"Y\": 2.9951462745666504, \"LABEL\": \"pfaltzgraff\"}, {\"X\": 1.1397440433502197, \"Y\": 5.676534175872803, \"LABEL\": \"somsak\"}, {\"X\": 0.9328576922416687, \"Y\": 2.959993839263916, \"LABEL\": \"texiero\"}, {\"X\": 1.023233413696289, \"Y\": 2.449716567993164, \"LABEL\": \"bignon\"}, {\"X\": -4.910726070404053, \"Y\": 1.2872512340545654, \"LABEL\": \"semifinalist\"}, {\"X\": -3.889275074005127, \"Y\": -0.7582499980926514, \"LABEL\": \"superbugs\"}, {\"X\": 2.0327646732330322, \"Y\": 2.1185171604156494, \"LABEL\": \"mutsuko\"}, {\"X\": -1.7709965705871582, \"Y\": -0.2730307877063751, \"LABEL\": \"dingyuan\"}, {\"X\": 0.9386283159255981, \"Y\": 3.1383864879608154, \"LABEL\": \"ardern\"}, {\"X\": 2.8571813106536865, \"Y\": -2.196781873703003, \"LABEL\": \"pre-determined\"}, {\"X\": -1.648820161819458, \"Y\": 3.6768252849578857, \"LABEL\": \"bikash\"}, {\"X\": 0.18295402824878693, \"Y\": -6.229718208312988, \"LABEL\": \"easy-to-use\"}, {\"X\": -0.47915369272232056, \"Y\": -1.2388533353805542, \"LABEL\": \"cave-like\"}, {\"X\": -1.5974961519241333, \"Y\": -3.431577682495117, \"LABEL\": \"mitcham\"}, {\"X\": 1.4571980237960815, \"Y\": -0.40712544322013855, \"LABEL\": \"unmaintained\"}, {\"X\": -0.808030366897583, \"Y\": -4.07732629776001, \"LABEL\": \"shoreward\"}, {\"X\": -4.306983947753906, \"Y\": -3.141064405441284, \"LABEL\": \"monotonously\"}, {\"X\": 3.2702953815460205, \"Y\": 3.7243480682373047, \"LABEL\": \"kalaile\"}, {\"X\": 0.17931286990642548, \"Y\": -1.4836384057998657, \"LABEL\": \"premillennial\"}, {\"X\": 2.5309178829193115, \"Y\": -0.5111627578735352, \"LABEL\": \"marbles\"}, {\"X\": 2.0505564212799072, \"Y\": 0.010850331746041775, \"LABEL\": \"vinum\"}, {\"X\": -0.16687695682048798, \"Y\": 2.0796847343444824, \"LABEL\": \"lugt\"}, {\"X\": -0.44799259305000305, \"Y\": -1.9058966636657715, \"LABEL\": \"farside\"}, {\"X\": 6.007603168487549, \"Y\": -3.676948070526123, \"LABEL\": \"pamarot\"}, {\"X\": -1.2987737655639648, \"Y\": 2.3567683696746826, \"LABEL\": \"poniatowska\"}, {\"X\": -0.6927797794342041, \"Y\": -0.8762091994285583, \"LABEL\": \"stardrive\"}, {\"X\": 3.9591338634490967, \"Y\": 1.5040847063064575, \"LABEL\": \"fresheners\"}, {\"X\": -1.8443305492401123, \"Y\": -2.668548345565796, \"LABEL\": \"kauniainen\"}, {\"X\": 4.949864387512207, \"Y\": -0.20266582071781158, \"LABEL\": \"defers\"}, {\"X\": 1.8882755041122437, \"Y\": 2.116459608078003, \"LABEL\": \"noyori\"}, {\"X\": -2.0497796535491943, \"Y\": 0.4224169850349426, \"LABEL\": \"magnete\"}, {\"X\": 2.2334859371185303, \"Y\": -5.455985069274902, \"LABEL\": \"78.9\"}, {\"X\": 0.14793553948402405, \"Y\": 1.8811672925949097, \"LABEL\": \"agne\"}, {\"X\": 0.47576653957366943, \"Y\": 3.835096597671509, \"LABEL\": \"galwey\"}, {\"X\": 1.0425070524215698, \"Y\": -4.252124309539795, \"LABEL\": \"41.93\"}, {\"X\": 3.0131642818450928, \"Y\": 2.970839738845825, \"LABEL\": \"kring\"}, {\"X\": 2.5990302562713623, \"Y\": 4.188086986541748, \"LABEL\": \"aamoth\"}, {\"X\": 2.3963427543640137, \"Y\": -1.744647741317749, \"LABEL\": \"anglo-chinese\"}, {\"X\": 4.179649829864502, \"Y\": 4.119491100311279, \"LABEL\": \"stepherson\"}, {\"X\": 4.0117926597595215, \"Y\": 2.7017595767974854, \"LABEL\": \"radosh\"}, {\"X\": 7.402660369873047, \"Y\": 2.5984511375427246, \"LABEL\": \"6-foot-8\"}, {\"X\": 1.9331005811691284, \"Y\": -2.264892339706421, \"LABEL\": \"07-08\"}, {\"X\": 0.7503552436828613, \"Y\": -2.0694847106933594, \"LABEL\": \"ceg\"}, {\"X\": 0.5261183381080627, \"Y\": 0.9118633270263672, \"LABEL\": \"qezel\"}, {\"X\": -1.6624281406402588, \"Y\": -0.31740760803222656, \"LABEL\": \"narwa\"}, {\"X\": -2.1412923336029053, \"Y\": -2.07502818107605, \"LABEL\": \"namma\"}, {\"X\": 0.8799474835395813, \"Y\": 2.865025043487549, \"LABEL\": \"schissler\"}, {\"X\": 2.8282039165496826, \"Y\": -0.26172181963920593, \"LABEL\": \"norad\"}, {\"X\": 3.2279317378997803, \"Y\": 1.7990776300430298, \"LABEL\": \"ramnaresh\"}, {\"X\": 1.5036970376968384, \"Y\": -5.484650611877441, \"LABEL\": \"14.60\"}, {\"X\": 6.661594390869141, \"Y\": 0.5886408686637878, \"LABEL\": \"rhizomorphs\"}, {\"X\": -1.7650123834609985, \"Y\": -1.609515905380249, \"LABEL\": \"hormiga\"}, {\"X\": 2.512727975845337, \"Y\": -0.711495578289032, \"LABEL\": \"teem\"}, {\"X\": -2.557863712310791, \"Y\": 1.6498621702194214, \"LABEL\": \"kushans\"}, {\"X\": -6.624603271484375, \"Y\": 0.19522948563098907, \"LABEL\": \"raises\"}, {\"X\": 1.2709170579910278, \"Y\": 3.4114081859588623, \"LABEL\": \"derisi\"}, {\"X\": 1.923162579536438, \"Y\": 4.23362922668457, \"LABEL\": \"milmore\"}, {\"X\": 2.634093761444092, \"Y\": -3.441298484802246, \"LABEL\": \"gondwanaland\"}, {\"X\": -0.44234558939933777, \"Y\": 5.880458831787109, \"LABEL\": \"salles\"}, {\"X\": 0.6634779572486877, \"Y\": 6.505969524383545, \"LABEL\": \"lessing\"}, {\"X\": 5.032402992248535, \"Y\": -1.7715402841567993, \"LABEL\": \"interdependent\"}, {\"X\": 3.2576773166656494, \"Y\": -0.5783759355545044, \"LABEL\": \"utopia\"}, {\"X\": 2.0506224632263184, \"Y\": 3.2959558963775635, \"LABEL\": \"plitt\"}, {\"X\": -2.4105091094970703, \"Y\": 1.8192697763442993, \"LABEL\": \"187th\"}, {\"X\": -2.9599146842956543, \"Y\": -2.356595039367676, \"LABEL\": \"lawrenceburg\"}, {\"X\": 4.765929222106934, \"Y\": -4.8030524253845215, \"LABEL\": \".475\"}, {\"X\": 3.5915098190307617, \"Y\": -1.8713772296905518, \"LABEL\": \"homemaking\"}, {\"X\": -0.1450655460357666, \"Y\": -0.7660230994224548, \"LABEL\": \"ascidian\"}, {\"X\": 1.3989816904067993, \"Y\": -5.080520153045654, \"LABEL\": \"19.42\"}, {\"X\": -1.5843003988265991, \"Y\": 4.279051780700684, \"LABEL\": \"pankow\"}, {\"X\": -0.7287145256996155, \"Y\": -3.547367572784424, \"LABEL\": \"nydalen\"}, {\"X\": 2.8666863441467285, \"Y\": -0.2602540850639343, \"LABEL\": \"roseville\"}, {\"X\": -3.261658191680908, \"Y\": 3.974595785140991, \"LABEL\": \"hysterectomies\"}, {\"X\": 2.979837656021118, \"Y\": 2.1588730812072754, \"LABEL\": \"hitz\"}, {\"X\": 1.2401959896087646, \"Y\": -4.300473690032959, \"LABEL\": \"62.08\"}, {\"X\": 2.401427745819092, \"Y\": -4.482678413391113, \"LABEL\": \"305,000\"}, {\"X\": 5.593363285064697, \"Y\": -2.516845703125, \"LABEL\": \"throught\"}, {\"X\": 0.8758257627487183, \"Y\": 2.467820405960083, \"LABEL\": \"emmanual\"}, {\"X\": -2.1278018951416016, \"Y\": -2.5598959922790527, \"LABEL\": \"bahraich\"}, {\"X\": -1.2154110670089722, \"Y\": 0.007728284224867821, \"LABEL\": \"\\u0441\\u0441\\u0441\\u0440\"}, {\"X\": 1.324009895324707, \"Y\": -4.8058013916015625, \"LABEL\": \"6,995\"}, {\"X\": 2.0759084224700928, \"Y\": -4.295571804046631, \"LABEL\": \"799,000\"}, {\"X\": -1.3088327646255493, \"Y\": -1.7831000089645386, \"LABEL\": \"sgv\"}, {\"X\": 0.4509640336036682, \"Y\": 0.1757926642894745, \"LABEL\": \"shin-chan\"}, {\"X\": 1.2501575946807861, \"Y\": -4.246234893798828, \"LABEL\": \"68.83\"}, {\"X\": -1.8491039276123047, \"Y\": 5.53540563583374, \"LABEL\": \"henchoz\"}, {\"X\": 1.0738856792449951, \"Y\": -4.9921135902404785, \"LABEL\": \"djnpb\"}, {\"X\": 0.7416700124740601, \"Y\": 1.3045936822891235, \"LABEL\": \"timour\"}, {\"X\": -1.159955382347107, \"Y\": -2.2770814895629883, \"LABEL\": \"hrad\\u010dany\"}, {\"X\": 1.8790570497512817, \"Y\": -3.6160545349121094, \"LABEL\": \"fvd\"}, {\"X\": -1.230932354927063, \"Y\": -0.0427897647023201, \"LABEL\": \"scirpus\"}, {\"X\": 0.6051204204559326, \"Y\": 5.398555755615234, \"LABEL\": \"syler\"}, {\"X\": 3.001858711242676, \"Y\": -1.2590783834457397, \"LABEL\": \"supplants\"}, {\"X\": -2.4560632705688477, \"Y\": 3.437851905822754, \"LABEL\": \"antrel\"}, {\"X\": -0.4768800437450409, \"Y\": -3.0673325061798096, \"LABEL\": \"irlanda\"}, {\"X\": -4.2897233963012695, \"Y\": -0.5050011873245239, \"LABEL\": \"21st-century\"}, {\"X\": -6.246222972869873, \"Y\": 2.1362171173095703, \"LABEL\": \"sciences\"}, {\"X\": 1.502062201499939, \"Y\": -5.359248638153076, \"LABEL\": \"9.29\"}, {\"X\": 2.7288930416107178, \"Y\": -4.995479583740234, \"LABEL\": \"36,563\"}, {\"X\": 1.975622534751892, \"Y\": 4.0446858406066895, \"LABEL\": \"tinetti\"}, {\"X\": -1.5157309770584106, \"Y\": -2.3913207054138184, \"LABEL\": \"charcas\"}, {\"X\": 2.8057448863983154, \"Y\": -6.930405616760254, \"LABEL\": \"142-0\"}, {\"X\": 5.3770432472229, \"Y\": 2.347860097885132, \"LABEL\": \"jansher\"}, {\"X\": 0.31767991185188293, \"Y\": -0.3021169900894165, \"LABEL\": \"homotopic\"}, {\"X\": 0.9328848123550415, \"Y\": 0.2770574390888214, \"LABEL\": \"sarcosine\"}, {\"X\": 4.055144786834717, \"Y\": -3.4810867309570312, \"LABEL\": \"filers\"}, {\"X\": 1.6088413000106812, \"Y\": -1.0806667804718018, \"LABEL\": \"7:4\"}, {\"X\": 5.295670509338379, \"Y\": 0.47773119807243347, \"LABEL\": \"camano\"}, {\"X\": -0.14832808077335358, \"Y\": -5.5578179359436035, \"LABEL\": \"cfr\"}, {\"X\": 0.4335172772407532, \"Y\": 1.5135447978973389, \"LABEL\": \"gitler\"}, {\"X\": 1.0401942729949951, \"Y\": 4.702306747436523, \"LABEL\": \"whitbeck\"}, {\"X\": 1.2615262269973755, \"Y\": 0.34508246183395386, \"LABEL\": \"rodovia\"}, {\"X\": 3.794196844100952, \"Y\": -3.0988988876342773, \"LABEL\": \"spiffier\"}, {\"X\": -1.9308222532272339, \"Y\": 4.0204572677612305, \"LABEL\": \"arona\"}, {\"X\": 2.3752787113189697, \"Y\": 2.674659490585327, \"LABEL\": \"fauzan\"}, {\"X\": 3.595292806625366, \"Y\": 2.662632465362549, \"LABEL\": \"depaulo\"}, {\"X\": 1.4563257694244385, \"Y\": -0.47698718309402466, \"LABEL\": \"steganography\"}, {\"X\": 0.629610002040863, \"Y\": -4.473034381866455, \"LABEL\": \"kitzsteinhorn\"}, {\"X\": 1.0189512968063354, \"Y\": -4.337838172912598, \"LABEL\": \"chippac\"}, {\"X\": -1.8551113605499268, \"Y\": -4.701650619506836, \"LABEL\": \"greenock\"}, {\"X\": 4.536681652069092, \"Y\": -2.2562968730926514, \"LABEL\": \"pooped\"}, {\"X\": -0.9183726906776428, \"Y\": -6.887787342071533, \"LABEL\": \"10:58\"}, {\"X\": -1.6016112565994263, \"Y\": -1.05020010471344, \"LABEL\": \"parashah\"}, {\"X\": -0.00901112798601389, \"Y\": 1.8545567989349365, \"LABEL\": \"sparda\"}, {\"X\": -6.965553283691406, \"Y\": 0.14643794298171997, \"LABEL\": \"wide\"}, {\"X\": -1.708258867263794, \"Y\": -2.3682854175567627, \"LABEL\": \"hawthornden\"}, {\"X\": -1.7813329696655273, \"Y\": 5.178121566772461, \"LABEL\": \"georgio\"}, {\"X\": 3.5091919898986816, \"Y\": 3.8859195709228516, \"LABEL\": \"gomery\"}, {\"X\": 0.9955394864082336, \"Y\": 1.166245460510254, \"LABEL\": \"ariana\"}, {\"X\": 0.3140842914581299, \"Y\": -0.08194920420646667, \"LABEL\": \"20-ish\"}, {\"X\": -0.91415935754776, \"Y\": -6.757782936096191, \"LABEL\": \"0035\"}, {\"X\": 0.04337085783481598, \"Y\": -3.4266357421875, \"LABEL\": \"booksmith\"}, {\"X\": -0.48430851101875305, \"Y\": -2.0649337768554688, \"LABEL\": \"glaucous\"}, {\"X\": 0.25402170419692993, \"Y\": -1.5934669971466064, \"LABEL\": \"bryozoans\"}, {\"X\": -0.8936382532119751, \"Y\": -0.08279524743556976, \"LABEL\": \"brocchinia\"}, {\"X\": 0.9734310507774353, \"Y\": 0.10025542229413986, \"LABEL\": \"chello\"}, {\"X\": 2.015815496444702, \"Y\": -4.989287376403809, \"LABEL\": \"133.4\"}, {\"X\": 0.6088980436325073, \"Y\": 1.1383061408996582, \"LABEL\": \"gristedes\"}, {\"X\": 0.7695484161376953, \"Y\": -3.1594817638397217, \"LABEL\": \"115.99\"}, {\"X\": 0.7896000742912292, \"Y\": -0.8297614455223083, \"LABEL\": \"burnable\"}, {\"X\": 1.8463404178619385, \"Y\": -4.388657093048096, \"LABEL\": \"313.5\"}, {\"X\": 3.3952887058258057, \"Y\": -0.4847152829170227, \"LABEL\": \"phughes\"}, {\"X\": 2.722517490386963, \"Y\": -6.727994441986084, \"LABEL\": \"78-77\"}, {\"X\": 1.7589372396469116, \"Y\": 0.3755834102630615, \"LABEL\": \"polychoral\"}, {\"X\": -1.1595796346664429, \"Y\": 2.1046717166900635, \"LABEL\": \"naseerullah\"}, {\"X\": -6.304244518280029, \"Y\": 1.3536558151245117, \"LABEL\": \"debated\"}, {\"X\": -7.402403354644775, \"Y\": 1.3374603986740112, \"LABEL\": \"candidate\"}, {\"X\": -0.16991259157657623, \"Y\": -0.09999248385429382, \"LABEL\": \"tandoor\"}, {\"X\": -4.497568607330322, \"Y\": -3.2242043018341064, \"LABEL\": \"shallowly\"}, {\"X\": -2.2557857036590576, \"Y\": -1.8547484874725342, \"LABEL\": \"mittelsachsen\"}, {\"X\": 3.899888515472412, \"Y\": 3.4239261150360107, \"LABEL\": \"madejski\"}, {\"X\": -2.7247941493988037, \"Y\": 1.9143424034118652, \"LABEL\": \"breakthough\"}, {\"X\": 5.369792461395264, \"Y\": -4.4130024909973145, \"LABEL\": \"satmex\"}, {\"X\": -4.437332630157471, \"Y\": 0.5690012574195862, \"LABEL\": \"administering\"}, {\"X\": 1.9392154216766357, \"Y\": 0.6660285592079163, \"LABEL\": \"four-hand\"}, {\"X\": 5.493865966796875, \"Y\": -4.497071266174316, \"LABEL\": \"petroleo\"}, {\"X\": 2.999803066253662, \"Y\": -3.996654510498047, \"LABEL\": \"airlifted\"}, {\"X\": -0.7657148241996765, \"Y\": 2.153794527053833, \"LABEL\": \"hillyard\"}, {\"X\": -6.586854934692383, \"Y\": 0.18239647150039673, \"LABEL\": \"greatest\"}, {\"X\": -4.191340446472168, \"Y\": 0.11426876485347748, \"LABEL\": \"athenians\"}, {\"X\": 0.05738334730267525, \"Y\": -2.7530503273010254, \"LABEL\": \"tramps\"}, {\"X\": -1.1214964389801025, \"Y\": -3.4629642963409424, \"LABEL\": \"maresfield\"}, {\"X\": 1.9800541400909424, \"Y\": -5.058652400970459, \"LABEL\": \"195.5\"}, {\"X\": -4.808558464050293, \"Y\": 3.497864246368408, \"LABEL\": \"greco\"}, {\"X\": -2.8104424476623535, \"Y\": -3.7003188133239746, \"LABEL\": \"sardasht\"}, {\"X\": -0.11373542994260788, \"Y\": 1.5662137269973755, \"LABEL\": \"shennib\"}, {\"X\": 2.12085223197937, \"Y\": -1.271883249282837, \"LABEL\": \"subspace\"}, {\"X\": -0.008420388214290142, \"Y\": -0.39281463623046875, \"LABEL\": \"bursae\"}, {\"X\": 2.540670156478882, \"Y\": -1.99428391456604, \"LABEL\": \"halmstads\"}, {\"X\": 0.1611412912607193, \"Y\": 0.12523116171360016, \"LABEL\": \"super-regional\"}, {\"X\": 5.70344352722168, \"Y\": 3.5489628314971924, \"LABEL\": \"warlow\"}, {\"X\": 2.4916868209838867, \"Y\": -0.37622973322868347, \"LABEL\": \"bmd\"}, {\"X\": -4.433347702026367, \"Y\": -1.932485580444336, \"LABEL\": \"sectorally\"}, {\"X\": -1.9635237455368042, \"Y\": -3.332237958908081, \"LABEL\": \"ormesby\"}, {\"X\": -2.779273271560669, \"Y\": -5.4903244972229, \"LABEL\": \"preprinted\"}, {\"X\": -0.1466137170791626, \"Y\": 0.03378412872552872, \"LABEL\": \"murri\"}, {\"X\": -3.2070553302764893, \"Y\": 2.965989351272583, \"LABEL\": \"deere\"}, {\"X\": -0.227098286151886, \"Y\": 0.04270951449871063, \"LABEL\": \"oreophryne\"}, {\"X\": -3.185490131378174, \"Y\": 1.085041880607605, \"LABEL\": \"campas\"}, {\"X\": 1.5313199758529663, \"Y\": 3.3188304901123047, \"LABEL\": \"rapee\"}, {\"X\": -3.7439587116241455, \"Y\": 2.360015392303467, \"LABEL\": \"merisant\"}, {\"X\": 3.152996063232422, \"Y\": 0.1009153202176094, \"LABEL\": \"obstetrics\"}, {\"X\": -0.21674640476703644, \"Y\": 3.500581741333008, \"LABEL\": \"oikawa\"}, {\"X\": 0.689202606678009, \"Y\": 0.02143505960702896, \"LABEL\": \"silambam\"}, {\"X\": -4.080693244934082, \"Y\": -1.7217508554458618, \"LABEL\": \"fallouts\"}, {\"X\": -0.5267958045005798, \"Y\": -3.9074084758758545, \"LABEL\": \"lavelanet\"}, {\"X\": -1.8119994401931763, \"Y\": -1.8124334812164307, \"LABEL\": \"tonal\\u00e1\"}, {\"X\": 3.675616979598999, \"Y\": 1.909192681312561, \"LABEL\": \"ratcliffe\"}, {\"X\": 2.5962395668029785, \"Y\": 3.1405744552612305, \"LABEL\": \"mustier\"}, {\"X\": -0.5151135921478271, \"Y\": -1.5447670221328735, \"LABEL\": \"11th-century\"}, {\"X\": -1.2895190715789795, \"Y\": -0.18251439929008484, \"LABEL\": \"mutine\"}, {\"X\": -4.952302932739258, \"Y\": 2.7134757041931152, \"LABEL\": \"catapult\"}, {\"X\": 0.015475026331841946, \"Y\": 0.844860315322876, \"LABEL\": \"szegedi\"}, {\"X\": 0.45344260334968567, \"Y\": 3.6560282707214355, \"LABEL\": \"tearle\"}, {\"X\": -1.0746796131134033, \"Y\": -0.34831181168556213, \"LABEL\": \"abhisheka\"}, {\"X\": 0.008003615774214268, \"Y\": 2.4392781257629395, \"LABEL\": \"sohrab\"}, {\"X\": -1.6368249654769897, \"Y\": -0.06420421600341797, \"LABEL\": \"danshaku\"}, {\"X\": -0.7147657871246338, \"Y\": 1.1523480415344238, \"LABEL\": \"hookey\"}, {\"X\": -6.799302577972412, \"Y\": 0.8673799633979797, \"LABEL\": \"feb.\"}, {\"X\": -1.1862636804580688, \"Y\": -0.10938185453414917, \"LABEL\": \"spiralis\"}, {\"X\": -0.7484938502311707, \"Y\": 1.479034662246704, \"LABEL\": \"measurer\"}, {\"X\": -0.2692379653453827, \"Y\": -0.44383180141448975, \"LABEL\": \"yankunytjatjara\"}, {\"X\": 2.0818018913269043, \"Y\": -2.5906014442443848, \"LABEL\": \"multiband\"}, {\"X\": 0.9063144326210022, \"Y\": 3.7021689414978027, \"LABEL\": \"smeshko\"}, {\"X\": 1.1063454151153564, \"Y\": -4.557582378387451, \"LABEL\": \"77.74\"}, {\"X\": 0.6187258362770081, \"Y\": 0.6236720681190491, \"LABEL\": \"dexedrine\"}, {\"X\": -6.644833564758301, \"Y\": 0.2811097800731659, \"LABEL\": \"admit\"}, {\"X\": 1.2738916873931885, \"Y\": 2.5161585807800293, \"LABEL\": \"nourbakhsh\"}, {\"X\": 4.320197105407715, \"Y\": -1.7213518619537354, \"LABEL\": \"propositional\"}, {\"X\": 1.407273769378662, \"Y\": 2.089465379714966, \"LABEL\": \"rivadineira\"}, {\"X\": 4.104052543640137, \"Y\": 0.19634440541267395, \"LABEL\": \"ruby\"}, {\"X\": -3.9036972522735596, \"Y\": 4.005024433135986, \"LABEL\": \"wingman\"}, {\"X\": 0.4795721471309662, \"Y\": -1.478142499923706, \"LABEL\": \"gaas\"}, {\"X\": 1.8908461332321167, \"Y\": 2.6921656131744385, \"LABEL\": \"bisaccia\"}, {\"X\": 1.6493163108825684, \"Y\": 2.6233103275299072, \"LABEL\": \"kornstein\"}, {\"X\": 0.7634507417678833, \"Y\": 1.2970441579818726, \"LABEL\": \"teferi\"}, {\"X\": 1.480955958366394, \"Y\": 3.6889257431030273, \"LABEL\": \"reuel\"}, {\"X\": -4.216372013092041, \"Y\": -1.1415857076644897, \"LABEL\": \"non-rigid\"}, {\"X\": 0.5418301820755005, \"Y\": 2.5422370433807373, \"LABEL\": \"delcourt\"}, {\"X\": -0.8094885945320129, \"Y\": -2.1631176471710205, \"LABEL\": \"nyayo\"}, {\"X\": 0.5199074149131775, \"Y\": 0.3563807010650635, \"LABEL\": \"poltergeist\"}, {\"X\": 0.756041944026947, \"Y\": -1.6039185523986816, \"LABEL\": \"free-space\"}, {\"X\": -2.46695876121521, \"Y\": 4.173104286193848, \"LABEL\": \"ciri\"}, {\"X\": -2.8090217113494873, \"Y\": -3.4413363933563232, \"LABEL\": \"berks\"}, {\"X\": 3.124002456665039, \"Y\": 2.1774179935455322, \"LABEL\": \"xuejun\"}, {\"X\": 2.14005708694458, \"Y\": -4.358668804168701, \"LABEL\": \"748,000\"}, {\"X\": 2.8728716373443604, \"Y\": 3.3098714351654053, \"LABEL\": \"chaeruddin\"}, {\"X\": 1.1894021034240723, \"Y\": 3.970659017562866, \"LABEL\": \"miriori\"}, {\"X\": 1.8622978925704956, \"Y\": 4.91149377822876, \"LABEL\": \"sturua\"}, {\"X\": -0.3835364282131195, \"Y\": -0.4144679307937622, \"LABEL\": \"flat-footed\"}, {\"X\": 2.0507402420043945, \"Y\": 4.231440544128418, \"LABEL\": \"gambale\"}, {\"X\": -2.267732620239258, \"Y\": 0.4741020202636719, \"LABEL\": \"misterios\"}, {\"X\": 0.1599375158548355, \"Y\": -0.3641418218612671, \"LABEL\": \"murut\"}, {\"X\": 0.23106667399406433, \"Y\": -1.9474050998687744, \"LABEL\": \"accretions\"}, {\"X\": 0.7432717680931091, \"Y\": -0.20102562010288239, \"LABEL\": \"ephorus\"}, {\"X\": 0.6001558899879456, \"Y\": 1.8884234428405762, \"LABEL\": \"implicity\"}, {\"X\": -5.993593692779541, \"Y\": 0.7426393628120422, \"LABEL\": \"attacker\"}, {\"X\": -6.660768508911133, \"Y\": -0.7148597836494446, \"LABEL\": \"bond\"}, {\"X\": 1.0917718410491943, \"Y\": 4.160521984100342, \"LABEL\": \"jajang\"}, {\"X\": -3.181213140487671, \"Y\": 1.0420238971710205, \"LABEL\": \"spreckels\"}, {\"X\": 2.5083348751068115, \"Y\": 6.594765663146973, \"LABEL\": \"hein\"}, {\"X\": -1.1481573581695557, \"Y\": 1.2666404247283936, \"LABEL\": \"cattles\"}, {\"X\": 1.248016595840454, \"Y\": -1.0288934707641602, \"LABEL\": \"read-out\"}, {\"X\": -0.7165589928627014, \"Y\": -3.4320366382598877, \"LABEL\": \"semantan\"}, {\"X\": -1.430783987045288, \"Y\": 4.950942039489746, \"LABEL\": \"castelli\"}, {\"X\": 1.1394834518432617, \"Y\": -6.159359931945801, \"LABEL\": \"2-5\"}, {\"X\": -1.048640489578247, \"Y\": 3.095489501953125, \"LABEL\": \"beslagic\"}, {\"X\": 0.8115217089653015, \"Y\": 3.1820497512817383, \"LABEL\": \"reichhold\"}, {\"X\": -2.153787851333618, \"Y\": 0.4496886432170868, \"LABEL\": \"v\\u00e9hicule\"}, {\"X\": -0.9594224095344543, \"Y\": -2.296113967895508, \"LABEL\": \"nibong\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='LABEL'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a21c2f",
   "metadata": {},
   "source": [
    "### Other relationships\n",
    "\n",
    "Beyond cosine similarity, there are other word relationships to explore via vector space math. For example, one way of modeling something like a _concept_ is to think about what other concepts comprise it. In other words: what plus what creates a new concept? Could we identify concepts by adding together vectors to create a new vector? Which words would this new vector be closest to in the vector space? Using the `.similar_by_vector()` method, we can find out.\n",
    "\n",
    "```{margin} What this loop does\n",
    "For each concept in our `concepts` dictionary:\n",
    "\n",
    "1. Get its associated pair of words\n",
    "2. Query the model for those words' vectors and add them together to create a new vector\n",
    "3. Find the most similar words to this new vector\n",
    "4. Use a dataframe to display the results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432fc44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'sand' + 'ocean' (for 'beach')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sand</td>\n",
       "      <td>0.845458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocean</td>\n",
       "      <td>0.845268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.687682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaches</td>\n",
       "      <td>0.667521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waters</td>\n",
       "      <td>0.664894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coastal</td>\n",
       "      <td>0.632485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>water</td>\n",
       "      <td>0.618701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coast</td>\n",
       "      <td>0.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dunes</td>\n",
       "      <td>0.599333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>surface</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0     sand  0.845458\n",
       "1    ocean  0.845268\n",
       "2      sea  0.687682\n",
       "3  beaches  0.667521\n",
       "4   waters  0.664894\n",
       "5  coastal  0.632485\n",
       "6    water  0.618701\n",
       "7    coast  0.604373\n",
       "8    dunes  0.599333\n",
       "9  surface  0.597545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'vacation' + 'room' (for 'hotel')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vacation</td>\n",
       "      <td>0.823460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>room</td>\n",
       "      <td>0.810719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rooms</td>\n",
       "      <td>0.704233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>0.658199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel</td>\n",
       "      <td>0.647865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dining</td>\n",
       "      <td>0.634925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stay</td>\n",
       "      <td>0.617807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apartment</td>\n",
       "      <td>0.616495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>staying</td>\n",
       "      <td>0.615182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>home</td>\n",
       "      <td>0.606009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0   vacation  0.823460\n",
       "1       room  0.810719\n",
       "2      rooms  0.704233\n",
       "3    bedroom  0.658199\n",
       "4      hotel  0.647865\n",
       "5     dining  0.634925\n",
       "6       stay  0.617807\n",
       "7  apartment  0.616495\n",
       "8    staying  0.615182\n",
       "9       home  0.606009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'air' + 'car' (for 'airplane')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air</td>\n",
       "      <td>0.827957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>0.810086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>0.719382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cars</td>\n",
       "      <td>0.671697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.645963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>0.637166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>passenger</td>\n",
       "      <td>0.625993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aircraft</td>\n",
       "      <td>0.624820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jet</td>\n",
       "      <td>0.618584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.610345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0        air  0.827957\n",
       "1        car  0.810086\n",
       "2    vehicle  0.719382\n",
       "3       cars  0.671697\n",
       "4      truck  0.645963\n",
       "5   vehicles  0.637166\n",
       "6  passenger  0.625993\n",
       "7   aircraft  0.624820\n",
       "8        jet  0.618584\n",
       "9   airplane  0.610345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concepts = {'beach': ('sand', 'ocean'), 'hotel': ('vacation', 'room'), 'airplane': ('air', 'car')}\n",
    "\n",
    "for concept in concepts:\n",
    "    pair = concepts[concept]\n",
    "    generated_concept = model[pair[0]] + model[pair[1]]\n",
    "    similarities = model.similar_by_vector(generated_concept)\n",
    "    print(f\"Most similar tokens to '{pair[0]}' + '{pair[1]}' (for '{concept}')\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b64296",
   "metadata": {},
   "source": [
    "Not bad! Our target concept isn't the most similar word for either of these examples, but it's in the top 10.\n",
    "\n",
    "Most famously, word embeddings enable quasi-logical reasoning. Though, as we mentioned earlier, relationships between antonyms and synoyms do not necessarily map to a vector space, certain analogies do – at least under the right circumstances, and with particular training data. The logic here is that we identify a relationship between two words and we subtract one of those words' vectors from the other. To that new vector we add in a vector for a target word, which forms the analogy. Querying for the word closest to this modified vector should produce a similar relation between the result and the target word as that between the original pair.\n",
    "\n",
    "Here, we ask: \"strong is to stronger what clear is to X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae0fa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easier</td>\n",
       "      <td>0.633451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>should</td>\n",
       "      <td>0.630116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clearer</td>\n",
       "      <td>0.621850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better</td>\n",
       "      <td>0.602637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>must</td>\n",
       "      <td>0.601793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>need</td>\n",
       "      <td>0.595918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meant</td>\n",
       "      <td>0.594797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>harder</td>\n",
       "      <td>0.591297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anything</td>\n",
       "      <td>0.589579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.589187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WORD     SCORE\n",
       "0    easier  0.633451\n",
       "1    should  0.630116\n",
       "2   clearer  0.621850\n",
       "3    better  0.602637\n",
       "4      must  0.601793\n",
       "5      need  0.595918\n",
       "6     meant  0.594797\n",
       "7    harder  0.591297\n",
       "8  anything  0.589579\n",
       "9   nothing  0.589187"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'clearer'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['stronger', 'clear'], negative=['strong'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'clearer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecf5b1",
   "metadata": {},
   "source": [
    "And here, we ask: \"Paris is to France what Berlin is to X\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f1b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>0.835242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german</td>\n",
       "      <td>0.684480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austria</td>\n",
       "      <td>0.612803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poland</td>\n",
       "      <td>0.581331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germans</td>\n",
       "      <td>0.574868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>munich</td>\n",
       "      <td>0.543591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>belgium</td>\n",
       "      <td>0.532413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>britain</td>\n",
       "      <td>0.529541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>europe</td>\n",
       "      <td>0.524402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>czech</td>\n",
       "      <td>0.515241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0  germany  0.835242\n",
       "1   german  0.684480\n",
       "2  austria  0.612803\n",
       "3   poland  0.581331\n",
       "4  germans  0.574868\n",
       "5   munich  0.543591\n",
       "6  belgium  0.532413\n",
       "7  britain  0.529541\n",
       "8   europe  0.524402\n",
       "9    czech  0.515241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'Germany'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['france', 'berlin'], negative=['paris'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'Germany'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702496a",
   "metadata": {},
   "source": [
    "Both of the above produce compelling results, though your mileage may vary. Consider the following: \"arm is to hand what leg is to X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b08a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>final</td>\n",
       "      <td>0.543408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>0.540411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legs</td>\n",
       "      <td>0.527352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>back</td>\n",
       "      <td>0.523477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saturday</td>\n",
       "      <td>0.522487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>round</td>\n",
       "      <td>0.516250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>0.516066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>second</td>\n",
       "      <td>0.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>place</td>\n",
       "      <td>0.509784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>side</td>\n",
       "      <td>0.508683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WORD     SCORE\n",
       "0     final  0.543408\n",
       "1     table  0.540411\n",
       "2      legs  0.527352\n",
       "3      back  0.523477\n",
       "4  saturday  0.522487\n",
       "5     round  0.516250\n",
       "6      draw  0.516066\n",
       "7    second  0.510900\n",
       "8     place  0.509784\n",
       "9      side  0.508683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'foot'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['hand', 'leg'], negative=['arm'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'foot'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e207db9",
   "metadata": {},
   "source": [
    "Importantly, these results are always going to be specific to the data on which a model was trained. Claims made on the basis of word embeddings that aspire to general linguistic truths would be treading on shaky ground here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5ded1",
   "metadata": {},
   "source": [
    "Document similarity\n",
    "------------------------\n",
    "\n",
    "While the above word relationships are relatively abstract (and any such findings therefrom should be couched accordingly), we can ground them with a concrete task. In this final section, we use GloVe embeddings to encode our corpus documents. This involves associating a word vector for each token in an obituary. Of course, GloVe has not been trained on the obituaries, so there may be important differences in token behavior between that model and the corpus; but we can assume that the general nature of GloVe will give us a decent sense of the overall feature space of the corpus. The result will be an enriched representation of each document, the nuances of which may better help us identify things like similarities between obituaries in our corpus.\n",
    "\n",
    "The other consideration for using GloVe with our specific corpus concerns the out-of-vocabulary words we've already discussed. Before we can encode our documents, we need to filter out tokens for which GloVe has no representation. We can do so by referencing the `in_glove` set we produced above.\n",
    "\n",
    "```{margin} What this loop does\n",
    "For every obituary:\n",
    "\n",
    "1. Create a new list to hold the tokens we want to keep\n",
    "2. Go through each of the tokens and check whether they are in GloVe\n",
    "3. Append in-vocabulary tokens to the new list\n",
    "4. Once we've checked all tokens, append them to a new corpus list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c21df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = []\n",
    "for doc in corpus:\n",
    "    keep = []\n",
    "    for token in doc:\n",
    "        if token in in_glove:\n",
    "            keep.append(token)\n",
    "    pruned.append(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a3902",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Time to encode. This is an easy operation. All we need to do is run the list of document's tokens directly into the model object and `gensim` will encode each accordingly. The result will be an `(n, 200)` array, where `n` is the number of tokens we passed to the model; each one will have 200 dimensions.\n",
    "\n",
    "But if we kept this array as is, we'd run into trouble. Matrix operations often require identically shaped representations, so documents with different lengths would be incomparable. To get around this, we take the mean of all the vectors in a document. The result is a 200-dimension vector that stands as a general representation of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58fee46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = [np.mean(model[doc], axis=0) for doc in pruned]\n",
    "doc_embeddings = np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81aa64e",
   "metadata": {},
   "source": [
    "Let's quickly check our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7212620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an encoded document: (485, 200) \n",
      "Shape of an encoded document after taking its mean embedding: (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of an encoded document: {model[pruned[0]].shape}\",\n",
    "    f\"\\nShape of an encoded document after taking its mean embedding: {doc_embeddings[0].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48ab3e",
   "metadata": {},
   "source": [
    "From here, we can treat these embeddings almost as if they represented words. Let's plot our obituaries accordingly. Take a look around at this and see what you can find. As a starting point, you might focus on that cluster of nodes right in the middle of the graph, toward the top. All the obituaries there are for sports players – they're even broken out by sport (baseball players are on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d708fb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-cdd6e7dec25d49bf9b50c841d7b94ffe\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cdd6e7dec25d49bf9b50c841d7b94ffe\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cdd6e7dec25d49bf9b50c841d7b94ffe\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a28cc8f5664461a6d14d99852e051b08\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"LABEL\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-a28cc8f5664461a6d14d99852e051b08\": [{\"X\": 8.091551780700684, \"Y\": 2.0906689167022705, \"LABEL\": \"Ada Lovelace\"}, {\"X\": 0.27661922574043274, \"Y\": -15.781915664672852, \"LABEL\": \"Robert E Lee\"}, {\"X\": 5.676476955413818, \"Y\": -19.14383888244629, \"LABEL\": \"Andrew Johnson\"}, {\"X\": -0.12391627579927444, \"Y\": -14.628294944763184, \"LABEL\": \"Bedford Forrest\"}, {\"X\": 7.088719844818115, \"Y\": -10.943242073059082, \"LABEL\": \"Lucretia Mott\"}, {\"X\": 7.151451110839844, \"Y\": 0.22176532447338104, \"LABEL\": \"Charles Darwin\"}, {\"X\": 1.7045074701309204, \"Y\": -15.467674255371094, \"LABEL\": \"Ulysses Grant\"}, {\"X\": 1.9255167245864868, \"Y\": 22.318931579589844, \"LABEL\": \"Mary Ewing Outerbridge\"}, {\"X\": -1.086108922958374, \"Y\": 0.7886382937431335, \"LABEL\": \"Emma Lazarus\"}, {\"X\": -7.712327480316162, \"Y\": 1.9806593656539917, \"LABEL\": \"Louisa M Alcott\"}, {\"X\": 0.3493926525115967, \"Y\": 3.807469129562378, \"LABEL\": \"P T Barnum\"}, {\"X\": -4.703158855438232, \"Y\": 0.5863965749740601, \"LABEL\": \"R L Stevenson\"}, {\"X\": 5.521393299102783, \"Y\": -13.030205726623535, \"LABEL\": \"Fred Douglass\"}, {\"X\": -6.474916934967041, \"Y\": 1.4430011510849, \"LABEL\": \"Harriet Beecher Stowe\"}, {\"X\": -6.187773704528809, \"Y\": -3.947223663330078, \"LABEL\": \"Stephen Crane\"}, {\"X\": 3.1740682125091553, \"Y\": 0.10201379656791687, \"LABEL\": \"Nietzsche\"}, {\"X\": 2.068394184112549, \"Y\": -15.72886848449707, \"LABEL\": \"William McKinley\"}, {\"X\": -3.9055821895599365, \"Y\": -18.564924240112305, \"LABEL\": \"Queen Victoria\"}, {\"X\": 9.108197212219238, \"Y\": -17.378419876098633, \"LABEL\": \"Benjamin Harrison\"}, {\"X\": 6.131286144256592, \"Y\": -11.840657234191895, \"LABEL\": \"Elizabeth Cady Stanton\"}, {\"X\": -0.09505367279052734, \"Y\": 9.596481323242188, \"LABEL\": \"James M N Whistler\"}, {\"X\": 5.002827167510986, \"Y\": 7.627674579620361, \"LABEL\": \"Emily Warren Roebling\"}, {\"X\": 6.0222063064575195, \"Y\": -11.809542655944824, \"LABEL\": \"Susan B Anthony\"}, {\"X\": -5.094084739685059, \"Y\": -22.840654373168945, \"LABEL\": \"Qiu Jin\"}, {\"X\": 7.328059196472168, \"Y\": -17.089235305786133, \"LABEL\": \"Cleveland\"}, {\"X\": -8.329190254211426, \"Y\": -3.2094368934631348, \"LABEL\": \"Sarah Orne Jewett\"}, {\"X\": -0.16614337265491486, \"Y\": -14.082984924316406, \"LABEL\": \"Geronimo\"}, {\"X\": 12.177238464355469, \"Y\": -6.174693584442139, \"LABEL\": \"William James\"}, {\"X\": 1.544617772102356, \"Y\": 3.0097622871398926, \"LABEL\": \"Florence Nightingale\"}, {\"X\": 2.5238845348358154, \"Y\": -2.197495222091675, \"LABEL\": \"Tolstoy\"}, {\"X\": 1.8686614036560059, \"Y\": -7.12151575088501, \"LABEL\": \"Joseph Pulitzer\"}, {\"X\": -3.684795379638672, \"Y\": -13.992591857910156, \"LABEL\": \"John P Holland\"}, {\"X\": -2.057128429412842, \"Y\": -13.71431827545166, \"LABEL\": \"Alfred Thayer Mahan\"}, {\"X\": 8.100855827331543, \"Y\": -1.8766006231307983, \"LABEL\": \"John Muir\"}, {\"X\": 9.796393394470215, \"Y\": -5.93954610824585, \"LABEL\": \"F W Taylor\"}, {\"X\": 6.883401393890381, \"Y\": -9.799163818359375, \"LABEL\": \"B T Washington\"}, {\"X\": 6.070070266723633, \"Y\": -6.789181709289551, \"LABEL\": \"J J Hill\"}, {\"X\": -4.98611307144165, \"Y\": 5.544983386993408, \"LABEL\": \"Jack London\"}, {\"X\": 9.348206520080566, \"Y\": -0.3093770444393158, \"LABEL\": \"Martian Theory\"}, {\"X\": 0.9835492372512817, \"Y\": 8.568599700927734, \"LABEL\": \"Hilaire G E Degas\"}, {\"X\": 5.058760166168213, \"Y\": -5.55410099029541, \"LABEL\": \"C J Walker\"}, {\"X\": 7.083810806274414, \"Y\": -7.065182209014893, \"LABEL\": \"Carnegie Started\"}, {\"X\": 6.753143310546875, \"Y\": -11.97126293182373, \"LABEL\": \"Anna H Shaw\"}, {\"X\": -11.278009414672852, \"Y\": 10.254430770874023, \"LABEL\": \"Marlene Dietrich\"}, {\"X\": -4.040230751037598, \"Y\": 6.009982585906982, \"LABEL\": \"Nellie Bly\"}, {\"X\": 7.2527313232421875, \"Y\": 5.735374450683594, \"LABEL\": \"Alexander Graham Bell\"}, {\"X\": 7.394962787628174, \"Y\": -18.833269119262695, \"LABEL\": \"Warren Harding\"}, {\"X\": 0.3803764283657074, \"Y\": 4.592177391052246, \"LABEL\": \"Harry Houdini\"}, {\"X\": 7.597263336181641, \"Y\": -11.927424430847168, \"LABEL\": \"Victoria Martin\"}, {\"X\": 9.641090393066406, \"Y\": -12.305803298950195, \"LABEL\": \"Mabel Craty\"}, {\"X\": 11.67678451538086, \"Y\": 0.7729182243347168, \"LABEL\": \"Marie Curie\"}, {\"X\": -0.6706202626228333, \"Y\": -19.157577514648438, \"LABEL\": \"Balfour\"}, {\"X\": 11.928217887878418, \"Y\": -3.9716310501098633, \"LABEL\": \"Elmer Sperry\"}, {\"X\": 5.692505836486816, \"Y\": -18.688413619995117, \"LABEL\": \"William Howard Taft\"}, {\"X\": -3.857030153274536, \"Y\": -0.20321665704250336, \"LABEL\": \"Conan Doyle\"}, {\"X\": 2.690385580062866, \"Y\": -9.949936866760254, \"LABEL\": \"Ida B Wells\"}, {\"X\": 7.520230770111084, \"Y\": -3.9424619674682617, \"LABEL\": \"Melvil Dewey\"}, {\"X\": -4.427181720733643, \"Y\": 8.317904472351074, \"LABEL\": \"Thomas Edison\"}, {\"X\": 4.436167240142822, \"Y\": 20.830867767333984, \"LABEL\": \"Knute Rocke\"}, {\"X\": -6.756148338317871, \"Y\": 17.858734130859375, \"LABEL\": \"John Philip Sousa\"}, {\"X\": -9.516679763793945, \"Y\": 15.955499649047852, \"LABEL\": \"Florenz Ziegfeld\"}, {\"X\": 5.917566299438477, \"Y\": -18.310640335083008, \"LABEL\": \"Calvin Coolidge\"}, {\"X\": -2.748025417327881, \"Y\": 1.688985824584961, \"LABEL\": \"Ring Lardner\"}, {\"X\": 2.5936787128448486, \"Y\": 10.610109329223633, \"LABEL\": \"Louis C Tiffany\"}, {\"X\": 7.165395736694336, \"Y\": 5.762889862060547, \"LABEL\": \"T A Watson\"}, {\"X\": 3.2825615406036377, \"Y\": -14.545103073120117, \"LABEL\": \"Justice Holmes\"}, {\"X\": 4.036138534545898, \"Y\": -12.795279502868652, \"LABEL\": \"Jane Addams\"}, {\"X\": -1.245707392692566, \"Y\": 4.458401203155518, \"LABEL\": \"Will Rogers\"}, {\"X\": 2.270707607269287, \"Y\": -7.003915309906006, \"LABEL\": \"Adolph S Ochs\"}, {\"X\": -8.896123886108398, \"Y\": 2.9925708770751953, \"LABEL\": \"Anne Macy\"}, {\"X\": 5.590899467468262, \"Y\": 21.519197463989258, \"LABEL\": \"John W Heisman\"}, {\"X\": 2.01053786277771, \"Y\": -2.3436379432678223, \"LABEL\": \"Maxim Gorky\"}, {\"X\": -9.908794403076172, \"Y\": 22.84100914001465, \"LABEL\": \"Maurice Ravel\"}, {\"X\": -4.8431925773620605, \"Y\": -0.5525018572807312, \"LABEL\": \"Edith Wharton\"}, {\"X\": 6.505295276641846, \"Y\": -5.8620147705078125, \"LABEL\": \"John Rockefeller\"}, {\"X\": 3.9247500896453857, \"Y\": -14.95589828491211, \"LABEL\": \"Clarence Darrow\"}, {\"X\": 12.804132461547852, \"Y\": -3.616757392883301, \"LABEL\": \"George E Hale\"}, {\"X\": -3.914403200149536, \"Y\": 13.519213676452637, \"LABEL\": \"Constantin Stanislavsky\"}, {\"X\": -6.564340591430664, \"Y\": 3.656446695327759, \"LABEL\": \"W B Yeats\"}, {\"X\": 3.7699551582336426, \"Y\": -2.7242953777313232, \"LABEL\": \"Pope Pius XI\"}, {\"X\": 2.4019153118133545, \"Y\": 4.9478254318237305, \"LABEL\": \"Howard Carter\"}, {\"X\": -5.265549182891846, \"Y\": 1.0430127382278442, \"LABEL\": \"Scott Fitzgerald\"}, {\"X\": 5.508907794952393, \"Y\": -8.690286636352539, \"LABEL\": \"Marcus Garvey\"}, {\"X\": 8.533053398132324, \"Y\": 6.58959436416626, \"LABEL\": \"Frank Conrad\"}, {\"X\": 5.797258377075195, \"Y\": 17.059532165527344, \"LABEL\": \"Lou Gehrig\"}, {\"X\": -6.856973171234131, \"Y\": -0.04005557671189308, \"LABEL\": \"James Joyce\"}, {\"X\": -9.25129508972168, \"Y\": 0.140128031373024, \"LABEL\": \"Virginia Woolf\"}, {\"X\": -11.549149513244629, \"Y\": 15.181727409362793, \"LABEL\": \"George M Cohan\"}, {\"X\": 9.84423828125, \"Y\": -3.4867053031921387, \"LABEL\": \"J H Kellogg\"}, {\"X\": 9.245756149291992, \"Y\": -2.9804117679595947, \"LABEL\": \"George Washington Carver\"}, {\"X\": 8.371291160583496, \"Y\": -18.89356231689453, \"LABEL\": \"Alfred E Smith\"}, {\"X\": 1.4978272914886475, \"Y\": -4.835948467254639, \"LABEL\": \"Ida M Tarbell\"}, {\"X\": -1.6803885698318481, \"Y\": 5.1880316734313965, \"LABEL\": \"Ernie Pyle\"}, {\"X\": 4.058639049530029, \"Y\": -20.418596267700195, \"LABEL\": \"Harry S Truman\"}, {\"X\": -0.8401336669921875, \"Y\": -15.223628044128418, \"LABEL\": \"George Patton\"}, {\"X\": 9.216495513916016, \"Y\": -18.456850051879883, \"LABEL\": \"FDR\"}, {\"X\": -12.015007019042969, \"Y\": 18.891151428222656, \"LABEL\": \"Jerome Kern\"}, {\"X\": -1.1993987560272217, \"Y\": -23.22262191772461, \"LABEL\": \"Adolf Hitler\"}, {\"X\": -10.432635307312012, \"Y\": 23.799230575561523, \"LABEL\": \"Bela Bartok\"}, {\"X\": -5.089295864105225, \"Y\": -0.15323784947395325, \"LABEL\": \"Gertrude Stein\"}, {\"X\": 0.20181503891944885, \"Y\": -18.508689880371094, \"LABEL\": \"Lord Keynes\"}, {\"X\": 14.339727401733398, \"Y\": -3.8698630332946777, \"LABEL\": \"C E M Clung\"}, {\"X\": -7.172032356262207, \"Y\": -2.4086458683013916, \"LABEL\": \"Willa Cather\"}, {\"X\": 0.21853679418563843, \"Y\": 5.627852916717529, \"LABEL\": \"Al Capone\"}, {\"X\": 6.648294448852539, \"Y\": -17.572429656982422, \"LABEL\": \"Fiorello La Guardia\"}, {\"X\": 13.307640075683594, \"Y\": 0.42554211616516113, \"LABEL\": \"Max Planck\"}, {\"X\": 8.608011245727539, \"Y\": -5.832132339477539, \"LABEL\": \"Henry Ford\"}, {\"X\": -1.0562225580215454, \"Y\": -15.967155456542969, \"LABEL\": \"John Pershing\"}, {\"X\": -4.043996334075928, \"Y\": 12.996066093444824, \"LABEL\": \"Sergei Eisenstein\"}, {\"X\": 2.2623484134674072, \"Y\": -24.15312385559082, \"LABEL\": \"Mohandas K Gandhi\"}, {\"X\": 4.894998073577881, \"Y\": 19.319122314453125, \"LABEL\": \"Babe Ruth\"}, {\"X\": -2.856966018676758, \"Y\": 0.6255747675895691, \"LABEL\": \"Mitchell\"}, {\"X\": 13.911664962768555, \"Y\": -1.560059905052185, \"LABEL\": \"A J Dempster\"}, {\"X\": -7.5595574378967285, \"Y\": 1.3543107509613037, \"LABEL\": \"Edna St V Millay\"}, {\"X\": 3.163919687271118, \"Y\": -19.970783233642578, \"LABEL\": \"Henry L Stimson\"}, {\"X\": -13.955941200256348, \"Y\": 14.72899055480957, \"LABEL\": \"Fanny Brice\"}, {\"X\": 10.77446460723877, \"Y\": 3.5342905521392822, \"LABEL\": \"Henrietta Lacks\"}, {\"X\": 1.9521914720535278, \"Y\": -18.063495635986328, \"LABEL\": \"Eva Peron\"}, {\"X\": 10.432589530944824, \"Y\": -9.631891250610352, \"LABEL\": \"John Dewey\"}, {\"X\": 3.5099353790283203, \"Y\": -26.283109664916992, \"LABEL\": \"Chaim Weizmann\"}, {\"X\": 8.09255599975586, \"Y\": -8.190781593322754, \"LABEL\": \"Charles Spaulding\"}, {\"X\": 4.831263542175293, \"Y\": -18.89034080505371, \"LABEL\": \"Fred Vinson\"}, {\"X\": -7.062807083129883, \"Y\": -2.807157278060913, \"LABEL\": \"Marjorie Rawlings\"}, {\"X\": -1.870943307876587, \"Y\": -24.25212287902832, \"LABEL\": \"Joseph Stalin\"}, {\"X\": 3.240004301071167, \"Y\": 20.131383895874023, \"LABEL\": \"Jim Thorpe\"}, {\"X\": -5.3576507568359375, \"Y\": 5.728669166564941, \"LABEL\": \"Eugene O Neill\"}, {\"X\": 1.3012663125991821, \"Y\": -6.720988750457764, \"LABEL\": \"Anne O Hare McCormick\"}, {\"X\": -1.6752671003341675, \"Y\": 7.95874547958374, \"LABEL\": \"Frida Kahlo\"}, {\"X\": 0.5131005644798279, \"Y\": -22.073766708374023, \"LABEL\": \"Getulio Vargas\"}, {\"X\": 13.51837158203125, \"Y\": -1.1113442182540894, \"LABEL\": \"Enrico Fermi\"}, {\"X\": 0.08283113688230515, \"Y\": 8.975573539733887, \"LABEL\": \"Henri Matisse\"}, {\"X\": 8.589686393737793, \"Y\": -2.9787580966949463, \"LABEL\": \"Liberty H Bailey\"}, {\"X\": -12.400946617126465, \"Y\": 13.532814979553223, \"LABEL\": \"Lionel Barrymore\"}, {\"X\": 1.1347702741622925, \"Y\": -0.9631224274635315, \"LABEL\": \"Thomas Mann\"}, {\"X\": 5.868532180786133, \"Y\": -0.1329895257949829, \"LABEL\": \"Albert Einstein\"}, {\"X\": 0.644413948059082, \"Y\": 20.133750915527344, \"LABEL\": \"Margaret Abbott\"}, {\"X\": 8.071982383728027, \"Y\": -13.857742309570312, \"LABEL\": \"Walter White\"}, {\"X\": 5.629862308502197, \"Y\": 17.75945472717285, \"LABEL\": \"Cy Young\"}, {\"X\": -3.1962456703186035, \"Y\": -6.119327068328857, \"LABEL\": \"Dale Carnegie\"}, {\"X\": 2.187002182006836, \"Y\": 20.86017417907715, \"LABEL\": \"Babe Zaharias\"}, {\"X\": 7.334896564483643, \"Y\": -5.641847133636475, \"LABEL\": \"Charles Merrill\"}, {\"X\": 8.088459014892578, \"Y\": -6.528273105621338, \"LABEL\": \"Thomas J Watson Sr\"}, {\"X\": 8.822431564331055, \"Y\": -7.036258697509766, \"LABEL\": \"Gerard Swope\"}, {\"X\": 0.019837092608213425, \"Y\": 12.256941795349121, \"LABEL\": \"Christian Dior\"}, {\"X\": -14.10526180267334, \"Y\": 19.550914764404297, \"LABEL\": \"W C Handy\"}, {\"X\": -14.630707740783691, \"Y\": 19.120119094848633, \"LABEL\": \"Billie Holiday\"}, {\"X\": 4.07633638381958, \"Y\": 8.701501846313477, \"LABEL\": \"Frank Lloyd Wright\"}, {\"X\": -12.73944091796875, \"Y\": 13.955618858337402, \"LABEL\": \"Ethel Barrymore\"}, {\"X\": -7.092732906341553, \"Y\": 12.458260536193848, \"LABEL\": \"Cecil De Mille\"}, {\"X\": 14.156725883483887, \"Y\": -3.8438351154327393, \"LABEL\": \"Ross G Harrison\"}, {\"X\": 2.8466544151306152, \"Y\": -20.701236724853516, \"LABEL\": \"John Dulles\"}, {\"X\": 1.4341976642608643, \"Y\": -2.3023252487182617, \"LABEL\": \"Boris Pasternak\"}, {\"X\": 14.588375091552734, \"Y\": -2.4826066493988037, \"LABEL\": \"Beno Gutenberg\"}, {\"X\": -1.3730815649032593, \"Y\": 2.8357672691345215, \"LABEL\": \"Emily Post\"}, {\"X\": -4.77087926864624, \"Y\": -4.387993335723877, \"LABEL\": \"Richard Wright\"}, {\"X\": 2.366114377975464, \"Y\": -21.633243560791016, \"LABEL\": \"Hammarskjold\"}, {\"X\": -4.490510940551758, \"Y\": 1.1779651641845703, \"LABEL\": \"Ernest Hemingway\"}, {\"X\": -2.0805320739746094, \"Y\": 6.881127834320068, \"LABEL\": \"Primitive Artist\"}, {\"X\": 9.335868835449219, \"Y\": -11.504140853881836, \"LABEL\": \"Emily Balch\"}, {\"X\": 7.491336345672607, \"Y\": -20.341108322143555, \"LABEL\": \"Sam Rayburn\"}, {\"X\": 5.287370204925537, \"Y\": 1.026353359222412, \"LABEL\": \"Carl G Jung\"}, {\"X\": -7.579326629638672, \"Y\": 7.755613803863525, \"LABEL\": \"Marilyn Monroe\"}, {\"X\": 6.434357643127441, \"Y\": -15.811779022216797, \"LABEL\": \"Eleanor Roosevelt\"}, {\"X\": -6.365342140197754, \"Y\": -0.5313624739646912, \"LABEL\": \"William Faulkner\"}, {\"X\": -8.25678825378418, \"Y\": 0.12019874900579453, \"LABEL\": \"Sylvia Plath\"}, {\"X\": 5.7489495277404785, \"Y\": -20.73656463623047, \"LABEL\": \"John F Kennedy\"}, {\"X\": -5.720600128173828, \"Y\": -1.2441251277923584, \"LABEL\": \"Robert Frost\"}, {\"X\": 9.770573616027832, \"Y\": -10.765989303588867, \"LABEL\": \"W E B DuBois\"}, {\"X\": 9.999427795410156, \"Y\": -15.389067649841309, \"LABEL\": \"Herbert Hoover\"}, {\"X\": -0.9084845781326294, \"Y\": -16.072019577026367, \"LABEL\": \"Douglas MacArthur\"}, {\"X\": -3.2682387828826904, \"Y\": 3.656870126724243, \"LABEL\": \"Sean O Casey\"}, {\"X\": 8.007101058959961, \"Y\": -0.9683744311332703, \"LABEL\": \"Rachel Carson\"}, {\"X\": -12.125691413879395, \"Y\": 17.64192771911621, \"LABEL\": \"Cole Porter\"}, {\"X\": -4.58758020401001, \"Y\": -2.1346490383148193, \"LABEL\": \"Nella Larsen\"}, {\"X\": 6.842874526977539, \"Y\": -19.00663185119629, \"LABEL\": \"Adlai Ewing Stevenson\"}, {\"X\": -9.394267082214355, \"Y\": 13.346661567687988, \"LABEL\": \"David O Selznick\"}, {\"X\": -1.381027340888977, \"Y\": -19.34161376953125, \"LABEL\": \"Churchill\"}, {\"X\": 5.711763858795166, \"Y\": 19.77413558959961, \"LABEL\": \"Branch Rickey\"}, {\"X\": 3.8823516368865967, \"Y\": -0.213487908244133, \"LABEL\": \"Martin Buber\"}, {\"X\": -0.4909175634384155, \"Y\": -7.9278740882873535, \"LABEL\": \"Edward R Murrow\"}, {\"X\": 4.8164801597595215, \"Y\": -0.28993940353393555, \"LABEL\": \"Albert Schweitzer\"}, {\"X\": -5.047088146209717, \"Y\": 2.665815830230713, \"LABEL\": \"Shirley Jackson\"}, {\"X\": 5.2891435623168945, \"Y\": -10.936084747314453, \"LABEL\": \"Margaret Sanger\"}, {\"X\": -2.9033801555633545, \"Y\": -14.646029472351074, \"LABEL\": \"Chester Nimitz\"}, {\"X\": -9.930489540100098, \"Y\": 9.449071884155273, \"LABEL\": \"Buster Keaton\"}, {\"X\": -0.7975120544433594, \"Y\": 3.8963050842285156, \"LABEL\": \"Lenny Bruce\"}, {\"X\": -6.360071659088135, \"Y\": 11.415752410888672, \"LABEL\": \"Walt Disney\"}, {\"X\": 8.573323249816895, \"Y\": -6.428279399871826, \"LABEL\": \"Alfred P Sloan Jr\"}, {\"X\": 11.55565071105957, \"Y\": 3.184868335723877, \"LABEL\": \"Gregory Pincus\"}, {\"X\": 0.00041557318763807416, \"Y\": -4.988834857940674, \"LABEL\": \"Henry R Luce\"}, {\"X\": -8.7759370803833, \"Y\": -1.413293719291687, \"LABEL\": \"Langston Hughes\"}, {\"X\": 12.157036781311035, \"Y\": -0.9133463501930237, \"LABEL\": \"J Robert Oppenheimer\"}, {\"X\": 7.771406650543213, \"Y\": -18.096389770507812, \"LABEL\": \"Robert Francis Kennedy\"}, {\"X\": -4.025519847869873, \"Y\": 3.2836079597473145, \"LABEL\": \"Helen Keller\"}, {\"X\": 0.4102725684642792, \"Y\": -4.575801372528076, \"LABEL\": \"Upton Sinclair\"}, {\"X\": 2.8380794525146484, \"Y\": -11.659584045410156, \"LABEL\": \"Martin Luther King Jr\"}, {\"X\": -4.41843318939209, \"Y\": -10.777113914489746, \"LABEL\": \"Yuri Gagarin\"}, {\"X\": 3.9844374656677246, \"Y\": 8.830159187316895, \"LABEL\": \"Mies van der Rohe\"}, {\"X\": 4.2705183029174805, \"Y\": -20.593523025512695, \"LABEL\": \"David Eisenhower\"}, {\"X\": -14.627070426940918, \"Y\": 21.661876678466797, \"LABEL\": \"Coleman Hawkins\"}, {\"X\": -12.288959503173828, \"Y\": 9.538145065307617, \"LABEL\": \"Madhubala\"}, {\"X\": -10.91363525390625, \"Y\": 15.947610855102539, \"LABEL\": \"Judy Garland\"}, {\"X\": 1.7566102743148804, \"Y\": 20.89740753173828, \"LABEL\": \"Maureen Connolly\"}, {\"X\": -2.7319345474243164, \"Y\": -22.500757217407227, \"LABEL\": \"Ho Chi Minh\"}, {\"X\": -0.2954440712928772, \"Y\": 20.148393630981445, \"LABEL\": \"Sonja Henie\"}, {\"X\": 6.556248188018799, \"Y\": -20.099365234375, \"LABEL\": \"Everett Dirksen\"}, {\"X\": 11.221527099609375, \"Y\": -14.67345142364502, \"LABEL\": \"Walter Reuther\"}, {\"X\": -1.1447980403900146, \"Y\": -22.354820251464844, \"LABEL\": \"Edouard Daladier\"}, {\"X\": 0.42066845297813416, \"Y\": -0.9524916410446167, \"LABEL\": \"Erich Maria Remarque\"}, {\"X\": -1.1164467334747314, \"Y\": -22.03478240966797, \"LABEL\": \"De Gaulle Rallied\"}, {\"X\": -0.10751062631607056, \"Y\": 12.785686492919922, \"LABEL\": \"Coco Chanel\"}, {\"X\": -2.0791187286376953, \"Y\": -16.304954528808594, \"LABEL\": \"Florence Blanchfield\"}, {\"X\": -2.1270408630371094, \"Y\": -24.744159698486328, \"LABEL\": \"Khrushchev\"}, {\"X\": -2.000955820083618, \"Y\": 10.29051685333252, \"LABEL\": \"Diane Arbus\"}, {\"X\": 3.6846258640289307, \"Y\": -13.232927322387695, \"LABEL\": \"Ralph Bunche\"}, {\"X\": 2.091111421585083, \"Y\": 20.403209686279297, \"LABEL\": \"Bobby Jones\"}, {\"X\": -14.313529014587402, \"Y\": 19.865781784057617, \"LABEL\": \"Louis Armstrong\"}, {\"X\": 3.22304368019104, \"Y\": -20.77686309814453, \"LABEL\": \"Dean Acheson\"}, {\"X\": -10.097384452819824, \"Y\": 22.32509994506836, \"LABEL\": \"Igor Stravinsky\"}, {\"X\": 4.117844581604004, \"Y\": -17.112918853759766, \"LABEL\": \"Hugo Black\"}, {\"X\": -15.796982765197754, \"Y\": 18.43817710876465, \"LABEL\": \"Mahalia Jackson\"}, {\"X\": 5.305239677429199, \"Y\": 19.629106521606445, \"LABEL\": \"Jackie Robinson\"}, {\"X\": -4.282553672790527, \"Y\": -18.213993072509766, \"LABEL\": \"The Duke of Windsor\"}, {\"X\": 8.927469253540039, \"Y\": -15.705222129821777, \"LABEL\": \"J Edgar Hoover\"}, {\"X\": 10.598589897155762, \"Y\": -15.200116157531738, \"LABEL\": \"Lyndon Johnson\"}, {\"X\": -10.891209602355957, \"Y\": 22.281869888305664, \"LABEL\": \"Otto Klemperer\"}, {\"X\": -1.146892786026001, \"Y\": -11.699542045593262, \"LABEL\": \"Eddie Rickenbacker\"}, {\"X\": 6.063342094421387, \"Y\": -16.626441955566406, \"LABEL\": \"Jeanette Rankin\"}, {\"X\": -0.6464793682098389, \"Y\": 8.681639671325684, \"LABEL\": \"Pablo Picasso\"}, {\"X\": -1.6775704622268677, \"Y\": -11.294557571411133, \"LABEL\": \"Roberto Clemente\"}, {\"X\": -6.072815418243408, \"Y\": 2.3014135360717773, \"LABEL\": \"Nancy Mitford\"}, {\"X\": 4.281641006469727, \"Y\": -17.33233642578125, \"LABEL\": \"Earl Warren\"}, {\"X\": -3.6575300693511963, \"Y\": 4.310171604156494, \"LABEL\": \"Sylvia Plath\"}, {\"X\": -7.029117584228516, \"Y\": 14.658928871154785, \"LABEL\": \"Ed Sullivan\"}, {\"X\": -11.440699577331543, \"Y\": 13.920867919921875, \"LABEL\": \"Katharine Cornell\"}, {\"X\": 0.06418497860431671, \"Y\": -11.084087371826172, \"LABEL\": \"Charles Lindbergh\"}, {\"X\": -3.7349493503570557, \"Y\": -20.69317626953125, \"LABEL\": \"Haile Selassie\"}, {\"X\": 2.433624267578125, \"Y\": -11.153874397277832, \"LABEL\": \"Elijah Muhammad\"}, {\"X\": -0.5462758541107178, \"Y\": -22.259645462036133, \"LABEL\": \"Franco\"}, {\"X\": -1.7076590061187744, \"Y\": 10.271132469177246, \"LABEL\": \"Walker Evans\"}, {\"X\": -3.3342642784118652, \"Y\": -23.15538787841797, \"LABEL\": \"Chiang Kai shek\"}, {\"X\": 6.070744037628174, \"Y\": -5.3253607749938965, \"LABEL\": \"J Paul Getty\"}, {\"X\": -4.137241840362549, \"Y\": -23.191923141479492, \"LABEL\": \"Mao Tse Tung\"}, {\"X\": -0.27759745717048645, \"Y\": 8.607646942138672, \"LABEL\": \"Max Ernst\"}, {\"X\": 9.683806419372559, \"Y\": -19.59040641784668, \"LABEL\": \"Richard Daley\"}, {\"X\": 11.677897453308105, \"Y\": 0.38013437390327454, \"LABEL\": \"Jacques Monod\"}, {\"X\": -6.574921607971191, \"Y\": 12.041075706481934, \"LABEL\": \"Adolph Zukor\"}, {\"X\": -9.727242469787598, \"Y\": 9.819466590881348, \"LABEL\": \"Charles Chaplin\"}, {\"X\": -11.043335914611816, \"Y\": 12.999029159545898, \"LABEL\": \"Joan Crawford\"}, {\"X\": -0.9071201086044312, \"Y\": -19.694175720214844, \"LABEL\": \"Dash Ended\"}, {\"X\": -10.317038536071777, \"Y\": 19.612215042114258, \"LABEL\": \"Maria Callas\"}, {\"X\": 2.7228610515594482, \"Y\": -26.44182777404785, \"LABEL\": \"Golda Meir\"}, {\"X\": 6.646701335906982, \"Y\": 1.0209088325500488, \"LABEL\": \"Margaret Mead\"}, {\"X\": 2.1077523231506348, \"Y\": -12.60781192779541, \"LABEL\": \"Pope Paul VI\"}, {\"X\": 12.33845043182373, \"Y\": -11.70703125, \"LABEL\": \"Bruce Catton\"}, {\"X\": -11.74393081665039, \"Y\": 21.50006103515625, \"LABEL\": \"Arthur Fiedler\"}, {\"X\": -8.150505065917969, \"Y\": 11.311758041381836, \"LABEL\": \"John Wayne\"}, {\"X\": 6.948338985443115, \"Y\": -14.58864688873291, \"LABEL\": \"A Philip Randolph\"}, {\"X\": -13.960192680358887, \"Y\": 21.73772621154785, \"LABEL\": \"Stan Kenton\"}, {\"X\": -12.107344627380371, \"Y\": 18.3758602142334, \"LABEL\": \"Richard Rodgers\"}, {\"X\": 2.986626386642456, \"Y\": 19.712684631347656, \"LABEL\": \"Jesse Owens\"}, {\"X\": -8.183670997619629, \"Y\": 9.536137580871582, \"LABEL\": \"Alfred Hitchcock\"}, {\"X\": 6.70174503326416, \"Y\": 1.8510829210281372, \"LABEL\": \"Jean Piaget\"}, {\"X\": 2.353790044784546, \"Y\": -0.572279155254364, \"LABEL\": \"Jean Paul Sartre\"}, {\"X\": 1.7528080940246582, \"Y\": 17.703086853027344, \"LABEL\": \"Joe Louis\"}, {\"X\": 5.644028186798096, \"Y\": -7.230025291442871, \"LABEL\": \"Robert Moses\"}, {\"X\": 2.0614261627197266, \"Y\": -26.772367477416992, \"LABEL\": \"Anwar el Sadat\"}, {\"X\": -11.400519371032715, \"Y\": 11.441492080688477, \"LABEL\": \"Ingrid Bergman\"}, {\"X\": 5.484755992889404, \"Y\": 1.6772611141204834, \"LABEL\": \"Anna Freud\"}, {\"X\": -1.912028193473816, \"Y\": -25.10733413696289, \"LABEL\": \"Leonid Brezhnev\"}, {\"X\": -10.517951011657715, \"Y\": 21.389081954956055, \"LABEL\": \"Arthur Rubinstein\"}, {\"X\": -14.269951820373535, \"Y\": 21.065935134887695, \"LABEL\": \"Thelonious Monk\"}, {\"X\": -8.867549896240234, \"Y\": 14.433915138244629, \"LABEL\": \"Lee Strasberg\"}, {\"X\": 5.190177917480469, \"Y\": 19.076826095581055, \"LABEL\": \"Satchel Paige\"}, {\"X\": 1.5962300300598145, \"Y\": 17.764183044433594, \"LABEL\": \"Jack Dempsey\"}, {\"X\": -16.065866470336914, \"Y\": 22.428876876831055, \"LABEL\": \"Earl Hines\"}, {\"X\": -16.141265869140625, \"Y\": 20.509902954101562, \"LABEL\": \"Muddy Waters\"}, {\"X\": -5.086297035217285, \"Y\": 1.3645154237747192, \"LABEL\": \"Truman Capote\"}, {\"X\": -3.7623701095581055, \"Y\": 2.300950288772583, \"LABEL\": \"Lillian Hellman\"}, {\"X\": -0.18157348036766052, \"Y\": 18.524791717529297, \"LABEL\": \"Johnny Weissmuller\"}, {\"X\": 1.7264039516448975, \"Y\": 10.500162124633789, \"LABEL\": \"Ansel Adams\"}, {\"X\": -11.954716682434082, \"Y\": 16.602327346801758, \"LABEL\": \"Ethel Merman\"}, {\"X\": 2.271125316619873, \"Y\": -24.124380111694336, \"LABEL\": \"Indira Gandhi\"}, {\"X\": 4.975656032562256, \"Y\": 5.127691268920898, \"LABEL\": \"Ray A Kroc\"}, {\"X\": -12.228028297424316, \"Y\": 12.913139343261719, \"LABEL\": \"Richard Burton\"}, {\"X\": -15.569748878479004, \"Y\": 21.67116355895996, \"LABEL\": \"Count Basie\"}, {\"X\": -2.1185460090637207, \"Y\": 3.2466747760772705, \"LABEL\": \"E B White\"}, {\"X\": -8.22596549987793, \"Y\": 12.748188018798828, \"LABEL\": \"Orson Welles\"}, {\"X\": 5.446950435638428, \"Y\": 18.269994735717773, \"LABEL\": \"Roger Maris\"}, {\"X\": -10.144200325012207, \"Y\": 11.936922073364258, \"LABEL\": \"James Cagney\"}, {\"X\": 0.42267510294914246, \"Y\": 10.323667526245117, \"LABEL\": \"Georgia O Keeffe\"}, {\"X\": -15.13720417022705, \"Y\": 21.26741600036621, \"LABEL\": \"Benny Goodman\"}, {\"X\": -1.9517865180969238, \"Y\": -1.7426565885543823, \"LABEL\": \"Jorge Luis Borges\"}, {\"X\": -3.0005218982696533, \"Y\": -2.1604509353637695, \"LABEL\": \"Bernard Malamud\"}, {\"X\": -7.233739376068115, \"Y\": 16.33815574645996, \"LABEL\": \"Kate Smith\"}, {\"X\": -3.284060001373291, \"Y\": -8.799633979797363, \"LABEL\": \"The Challenger\"}, {\"X\": -0.16135701537132263, \"Y\": -1.3806411027908325, \"LABEL\": \"Primo Levi\"}, {\"X\": 0.160842165350914, \"Y\": -5.715259552001953, \"LABEL\": \"Clare Boothe Luce\"}, {\"X\": -9.35444450378418, \"Y\": 11.49026870727539, \"LABEL\": \"John Huston\"}, {\"X\": -12.722125053405762, \"Y\": 11.221147537231445, \"LABEL\": \"Rita Hayworth\"}, {\"X\": 2.0823161602020264, \"Y\": -9.595553398132324, \"LABEL\": \"James Baldwin\"}, {\"X\": 8.856524467468262, \"Y\": -19.307838439941406, \"LABEL\": \"Alf Landon\"}, {\"X\": -11.902486801147461, \"Y\": 22.863727569580078, \"LABEL\": \"Andres Segovie\"}, {\"X\": -8.181717872619629, \"Y\": 13.401019096374512, \"LABEL\": \"John Houseman\"}, {\"X\": -5.737271308898926, \"Y\": 0.15060238540172577, \"LABEL\": \"Louis L Amour\"}, {\"X\": 11.293539047241211, \"Y\": -1.3875865936279297, \"LABEL\": \"William B Shockley\"}, {\"X\": -10.462252616882324, \"Y\": 14.882281303405762, \"LABEL\": \"Lucille Ball\"}, {\"X\": -5.80594539642334, \"Y\": -1.162826418876648, \"LABEL\": \"Robert Penn Warren\"}, {\"X\": 11.61480712890625, \"Y\": -16.435943603515625, \"LABEL\": \"Ferdinand Marcos\"}, {\"X\": -2.4243974685668945, \"Y\": -25.397769927978516, \"LABEL\": \"Andrei Sakharov\"}, {\"X\": -1.439214825630188, \"Y\": -25.602018356323242, \"LABEL\": \"Andrei A Gromyko\"}, {\"X\": 1.8964375257492065, \"Y\": -8.184626579284668, \"LABEL\": \"I F Stone\"}, {\"X\": -10.617868423461914, \"Y\": 21.194807052612305, \"LABEL\": \"Vladimir Horowitz\"}, {\"X\": -4.264729976654053, \"Y\": -20.33122444152832, \"LABEL\": \"Hirohito\"}, {\"X\": 7.009855270385742, \"Y\": 9.991463661193848, \"LABEL\": \"August A Busch Jr\"}, {\"X\": 7.647704601287842, \"Y\": -21.37030029296875, \"LABEL\": \"Claude Pepper\"}, {\"X\": -6.073394298553467, \"Y\": 4.2050347328186035, \"LABEL\": \"Samuel Beckett\"}, {\"X\": -10.959029197692871, \"Y\": 10.45793628692627, \"LABEL\": \"Greta Garbo\"}, {\"X\": -10.609621047973633, \"Y\": 12.900979042053223, \"LABEL\": \"Sammy Davis Jr\"}, {\"X\": -11.28962516784668, \"Y\": 21.364112854003906, \"LABEL\": \"Leonard Bernstein\"}, {\"X\": 0.227839857339859, \"Y\": 11.358028411865234, \"LABEL\": \"Erte\"}, {\"X\": 6.354766845703125, \"Y\": -14.028027534484863, \"LABEL\": \"Ralph David Abernathy\"}, {\"X\": -13.484834671020508, \"Y\": 13.493277549743652, \"LABEL\": \"Rex Harrison\"}, {\"X\": -9.161225318908691, \"Y\": 11.457062721252441, \"LABEL\": \"Frank Capra\"}, {\"X\": -9.09293270111084, \"Y\": 5.257132053375244, \"LABEL\": \"Dr Seuss\"}, {\"X\": -14.794368743896484, \"Y\": 22.301076889038086, \"LABEL\": \"Miles Davis\"}, {\"X\": -8.754213333129883, \"Y\": 18.975425720214844, \"LABEL\": \"Martha Graham\"}, {\"X\": 6.248449802398682, \"Y\": 19.179615020751953, \"LABEL\": \"Leo Durocher\"}, {\"X\": -14.048850059509277, \"Y\": 12.563765525817871, \"LABEL\": \"Peggy Ashcroft\"}, {\"X\": -5.369603633880615, \"Y\": -2.4714550971984863, \"LABEL\": \"Alex Haley\"}, {\"X\": -9.515295028686523, \"Y\": 21.74912452697754, \"LABEL\": \"John Cage\"}, {\"X\": 2.635098695755005, \"Y\": -27.009138107299805, \"LABEL\": \"Menachem Begin\"}, {\"X\": -12.868858337402344, \"Y\": 14.436298370361328, \"LABEL\": \"Shirley Booth\"}, {\"X\": -3.589008331298828, \"Y\": -3.1664249897003174, \"LABEL\": \"Isaac Asimov\"}, {\"X\": -1.5208920240402222, \"Y\": -4.282694339752197, \"LABEL\": \"William Shawn\"}, {\"X\": 3.540773630142212, \"Y\": -9.904070854187012, \"LABEL\": \"Marsha P Johnson\"}, {\"X\": 4.08953332901001, \"Y\": -16.77513313293457, \"LABEL\": \"Thurgood Marshall\"}, {\"X\": -8.321367263793945, \"Y\": 9.756512641906738, \"LABEL\": \"Federico Fellini\"}, {\"X\": 7.209724426269531, \"Y\": -23.14278221130371, \"LABEL\": \"Cesar Chavez\"}, {\"X\": -12.737433433532715, \"Y\": 23.424379348754883, \"LABEL\": \"Carlos Montoya\"}, {\"X\": -15.065054893493652, \"Y\": 22.669597625732422, \"LABEL\": \"Dizzy Gillespie\"}, {\"X\": 2.0340092182159424, \"Y\": 21.639955520629883, \"LABEL\": \"Arthur Ashe\"}, {\"X\": -3.833820104598999, \"Y\": -0.9150063991546631, \"LABEL\": \"William Golding\"}, {\"X\": 13.081901550292969, \"Y\": 3.823532819747925, \"LABEL\": \"Albert Sabin\"}, {\"X\": 7.929497241973877, \"Y\": -20.055503845214844, \"LABEL\": \"Thomas P O Neill Jr\"}, {\"X\": 6.86980676651001, \"Y\": -19.333242416381836, \"LABEL\": \"Richard Nixon\"}, {\"X\": 5.999512195587158, \"Y\": 1.4719454050064087, \"LABEL\": \"Erik Erikson\"}, {\"X\": 12.014708518981934, \"Y\": -0.6923053860664368, \"LABEL\": \"Linus C Pauling\"}, {\"X\": -11.61903190612793, \"Y\": 14.309277534484863, \"LABEL\": \"Jessica Tandy\"}, {\"X\": 12.510062217712402, \"Y\": -8.423639297485352, \"LABEL\": \"Jan Tinbergen\"}, {\"X\": -2.5444443225860596, \"Y\": -0.5598430037498474, \"LABEL\": \"Jacqueline Kennedy\"}, {\"X\": 13.037522315979004, \"Y\": 3.7997031211853027, \"LABEL\": \"Jonas Salk\"}, {\"X\": -2.3941287994384766, \"Y\": 10.565855026245117, \"LABEL\": \"Alfred Eisenstaedt\"}, {\"X\": -12.642889022827148, \"Y\": 15.47425365447998, \"LABEL\": \"Ginger Rogers\"}, {\"X\": -10.982821464538574, \"Y\": 14.190242767333984, \"LABEL\": \"George Abbott\"}, {\"X\": 2.525446653366089, \"Y\": -27.234472274780273, \"LABEL\": \"Yitzhak Rabin\"}, {\"X\": 9.54769515991211, \"Y\": -0.1324111521244049, \"LABEL\": \"Carl Sagan\"}, {\"X\": 0.5164222717285156, \"Y\": 1.8136403560638428, \"LABEL\": \"Timothy Leary\"}, {\"X\": -13.102167129516602, \"Y\": 16.364717483520508, \"LABEL\": \"Gene Kelly\"}, {\"X\": -7.367973327636719, \"Y\": -0.7886866331100464, \"LABEL\": \"Allen Ginsberg\"}, {\"X\": -4.148149490356445, \"Y\": -23.655927658081055, \"LABEL\": \"Deng Xiaoping\"}, {\"X\": -9.627241134643555, \"Y\": 12.181411743164062, \"LABEL\": \"James Stewart\"}, {\"X\": -10.782670021057129, \"Y\": 7.878483772277832, \"LABEL\": \"Bob Kane\"}, {\"X\": 3.504016399383545, \"Y\": 2.526505470275879, \"LABEL\": \"Benjamin Spock\"}, {\"X\": 1.4857224225997925, \"Y\": 20.424386978149414, \"LABEL\": \"Helen Moody\"}, {\"X\": -13.051321029663086, \"Y\": 12.466283798217773, \"LABEL\": \"Maureen O Sullivan\"}, {\"X\": 11.939977645874023, \"Y\": -8.256277084350586, \"LABEL\": \"Theodore Schultz\"}, {\"X\": -4.298580169677734, \"Y\": -10.627726554870605, \"LABEL\": \"Alan B Shepard Jr\"}, {\"X\": -8.180453300476074, \"Y\": 19.80317497253418, \"LABEL\": \"Galina Ulanova\"}, {\"X\": 8.874265670776367, \"Y\": -20.762174606323242, \"LABEL\": \"Bella Abzug\"}, {\"X\": -0.5873187780380249, \"Y\": -7.927563667297363, \"LABEL\": \"Fred W Friendly\"}, {\"X\": -13.889960289001465, \"Y\": 18.14456558227539, \"LABEL\": \"Frank Sinatra\"}, {\"X\": 1.1848206520080566, \"Y\": -26.802824020385742, \"LABEL\": \"Hassan II\"}, {\"X\": -1.3036211729049683, \"Y\": 0.27954360842704773, \"LABEL\": \"Iris Murdoch\"}, {\"X\": 1.455483317375183, \"Y\": -27.17296028137207, \"LABEL\": \"King Hussein\"}, {\"X\": -0.09497282654047012, \"Y\": -20.308443069458008, \"LABEL\": \"Pierre Trudeau\"}, {\"X\": 8.329566955566406, \"Y\": -17.360031127929688, \"LABEL\": \"Elliot Richardson\"}, {\"X\": -9.796686172485352, \"Y\": 6.504615306854248, \"LABEL\": \"Charles M Schulz\"}, {\"X\": 8.127979278564453, \"Y\": 1.5808155536651611, \"LABEL\": \"Karen Sparck Jones\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_data = prepare_vis_data(doc_embeddings, manifest['NAME'])\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='LABEL'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee6c3b",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "The document embeddings seem to be partitioned into different clusters. We'll end by using a hierarchical clusterer to see if we can further specifiy these clusters. This involves loading the `AgglomerativeClustering` object from `scikit-learn` and fitting it to our document embeddings. Hierarchical clustering requires us to predefine the number of clusters we'd like to generate. In this case, we'll go with 18.\n",
    "\n",
    "```{margin} Why this number of clusters?\n",
    "We grid searched different numbers and measured the results with a [silhouette coefficient].\n",
    "\n",
    "[silhouette coefficient]: https://towardsdatascience.com/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8689e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 18\n",
    "agg = AgglomerativeClustering(n_clusters=n_clusters).fit(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1aa71f",
   "metadata": {},
   "source": [
    "Now we can assign the clusterer's predicted labels to the dataframe that contains our visualization data and re-plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "902d168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d07bb242ec8543ea953f6a5fac74ca80\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d07bb242ec8543ea953f6a5fac74ca80\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d07bb242ec8543ea953f6a5fac74ca80\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-09f454d9c77ade2db5f71a47dd67df03\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"color\": {\"field\": \"CLUSTER\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"LABEL\", \"type\": \"nominal\"}, {\"field\": \"CLUSTER\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector003\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-09f454d9c77ade2db5f71a47dd67df03\": [{\"X\": 8.091551780700684, \"Y\": 2.0906689167022705, \"LABEL\": \"Ada Lovelace\", \"CLUSTER\": 2}, {\"X\": 0.27661922574043274, \"Y\": -15.781915664672852, \"LABEL\": \"Robert E Lee\", \"CLUSTER\": 13}, {\"X\": 5.676476955413818, \"Y\": -19.14383888244629, \"LABEL\": \"Andrew Johnson\", \"CLUSTER\": 8}, {\"X\": -0.12391627579927444, \"Y\": -14.628294944763184, \"LABEL\": \"Bedford Forrest\", \"CLUSTER\": 13}, {\"X\": 7.088719844818115, \"Y\": -10.943242073059082, \"LABEL\": \"Lucretia Mott\", \"CLUSTER\": 5}, {\"X\": 7.151451110839844, \"Y\": 0.22176532447338104, \"LABEL\": \"Charles Darwin\", \"CLUSTER\": 2}, {\"X\": 1.7045074701309204, \"Y\": -15.467674255371094, \"LABEL\": \"Ulysses Grant\", \"CLUSTER\": 13}, {\"X\": 1.9255167245864868, \"Y\": 22.318931579589844, \"LABEL\": \"Mary Ewing Outerbridge\", \"CLUSTER\": 15}, {\"X\": -1.086108922958374, \"Y\": 0.7886382937431335, \"LABEL\": \"Emma Lazarus\", \"CLUSTER\": 9}, {\"X\": -7.712327480316162, \"Y\": 1.9806593656539917, \"LABEL\": \"Louisa M Alcott\", \"CLUSTER\": 9}, {\"X\": 0.3493926525115967, \"Y\": 3.807469129562378, \"LABEL\": \"P T Barnum\", \"CLUSTER\": 9}, {\"X\": -4.703158855438232, \"Y\": 0.5863965749740601, \"LABEL\": \"R L Stevenson\", \"CLUSTER\": 9}, {\"X\": 5.521393299102783, \"Y\": -13.030205726623535, \"LABEL\": \"Fred Douglass\", \"CLUSTER\": 5}, {\"X\": -6.474916934967041, \"Y\": 1.4430011510849, \"LABEL\": \"Harriet Beecher Stowe\", \"CLUSTER\": 9}, {\"X\": -6.187773704528809, \"Y\": -3.947223663330078, \"LABEL\": \"Stephen Crane\", \"CLUSTER\": 9}, {\"X\": 3.1740682125091553, \"Y\": 0.10201379656791687, \"LABEL\": \"Nietzsche\", \"CLUSTER\": 2}, {\"X\": 2.068394184112549, \"Y\": -15.72886848449707, \"LABEL\": \"William McKinley\", \"CLUSTER\": 13}, {\"X\": -3.9055821895599365, \"Y\": -18.564924240112305, \"LABEL\": \"Queen Victoria\", \"CLUSTER\": 13}, {\"X\": 9.108197212219238, \"Y\": -17.378419876098633, \"LABEL\": \"Benjamin Harrison\", \"CLUSTER\": 8}, {\"X\": 6.131286144256592, \"Y\": -11.840657234191895, \"LABEL\": \"Elizabeth Cady Stanton\", \"CLUSTER\": 5}, {\"X\": -0.09505367279052734, \"Y\": 9.596481323242188, \"LABEL\": \"James M N Whistler\", \"CLUSTER\": 10}, {\"X\": 5.002827167510986, \"Y\": 7.627674579620361, \"LABEL\": \"Emily Warren Roebling\", \"CLUSTER\": 18}, {\"X\": 6.0222063064575195, \"Y\": -11.809542655944824, \"LABEL\": \"Susan B Anthony\", \"CLUSTER\": 5}, {\"X\": -5.094084739685059, \"Y\": -22.840654373168945, \"LABEL\": \"Qiu Jin\", \"CLUSTER\": 4}, {\"X\": 7.328059196472168, \"Y\": -17.089235305786133, \"LABEL\": \"Cleveland\", \"CLUSTER\": 8}, {\"X\": -8.329190254211426, \"Y\": -3.2094368934631348, \"LABEL\": \"Sarah Orne Jewett\", \"CLUSTER\": 9}, {\"X\": -0.16614337265491486, \"Y\": -14.082984924316406, \"LABEL\": \"Geronimo\", \"CLUSTER\": 13}, {\"X\": 12.177238464355469, \"Y\": -6.174693584442139, \"LABEL\": \"William James\", \"CLUSTER\": 5}, {\"X\": 1.544617772102356, \"Y\": 3.0097622871398926, \"LABEL\": \"Florence Nightingale\", \"CLUSTER\": 9}, {\"X\": 2.5238845348358154, \"Y\": -2.197495222091675, \"LABEL\": \"Tolstoy\", \"CLUSTER\": 2}, {\"X\": 1.8686614036560059, \"Y\": -7.12151575088501, \"LABEL\": \"Joseph Pulitzer\", \"CLUSTER\": 5}, {\"X\": -3.684795379638672, \"Y\": -13.992591857910156, \"LABEL\": \"John P Holland\", \"CLUSTER\": 13}, {\"X\": -2.057128429412842, \"Y\": -13.71431827545166, \"LABEL\": \"Alfred Thayer Mahan\", \"CLUSTER\": 13}, {\"X\": 8.100855827331543, \"Y\": -1.8766006231307983, \"LABEL\": \"John Muir\", \"CLUSTER\": 5}, {\"X\": 9.796393394470215, \"Y\": -5.93954610824585, \"LABEL\": \"F W Taylor\", \"CLUSTER\": 18}, {\"X\": 6.883401393890381, \"Y\": -9.799163818359375, \"LABEL\": \"B T Washington\", \"CLUSTER\": 5}, {\"X\": 6.070070266723633, \"Y\": -6.789181709289551, \"LABEL\": \"J J Hill\", \"CLUSTER\": 18}, {\"X\": -4.98611307144165, \"Y\": 5.544983386993408, \"LABEL\": \"Jack London\", \"CLUSTER\": 9}, {\"X\": 9.348206520080566, \"Y\": -0.3093770444393158, \"LABEL\": \"Martian Theory\", \"CLUSTER\": 6}, {\"X\": 0.9835492372512817, \"Y\": 8.568599700927734, \"LABEL\": \"Hilaire G E Degas\", \"CLUSTER\": 10}, {\"X\": 5.058760166168213, \"Y\": -5.55410099029541, \"LABEL\": \"C J Walker\", \"CLUSTER\": 18}, {\"X\": 7.083810806274414, \"Y\": -7.065182209014893, \"LABEL\": \"Carnegie Started\", \"CLUSTER\": 18}, {\"X\": 6.753143310546875, \"Y\": -11.97126293182373, \"LABEL\": \"Anna H Shaw\", \"CLUSTER\": 5}, {\"X\": -11.278009414672852, \"Y\": 10.254430770874023, \"LABEL\": \"Marlene Dietrich\", \"CLUSTER\": 14}, {\"X\": -4.040230751037598, \"Y\": 6.009982585906982, \"LABEL\": \"Nellie Bly\", \"CLUSTER\": 9}, {\"X\": 7.2527313232421875, \"Y\": 5.735374450683594, \"LABEL\": \"Alexander Graham Bell\", \"CLUSTER\": 3}, {\"X\": 7.394962787628174, \"Y\": -18.833269119262695, \"LABEL\": \"Warren Harding\", \"CLUSTER\": 8}, {\"X\": 0.3803764283657074, \"Y\": 4.592177391052246, \"LABEL\": \"Harry Houdini\", \"CLUSTER\": 9}, {\"X\": 7.597263336181641, \"Y\": -11.927424430847168, \"LABEL\": \"Victoria Martin\", \"CLUSTER\": 5}, {\"X\": 9.641090393066406, \"Y\": -12.305803298950195, \"LABEL\": \"Mabel Craty\", \"CLUSTER\": 5}, {\"X\": 11.67678451538086, \"Y\": 0.7729182243347168, \"LABEL\": \"Marie Curie\", \"CLUSTER\": 6}, {\"X\": -0.6706202626228333, \"Y\": -19.157577514648438, \"LABEL\": \"Balfour\", \"CLUSTER\": 4}, {\"X\": 11.928217887878418, \"Y\": -3.9716310501098633, \"LABEL\": \"Elmer Sperry\", \"CLUSTER\": 6}, {\"X\": 5.692505836486816, \"Y\": -18.688413619995117, \"LABEL\": \"William Howard Taft\", \"CLUSTER\": 8}, {\"X\": -3.857030153274536, \"Y\": -0.20321665704250336, \"LABEL\": \"Conan Doyle\", \"CLUSTER\": 9}, {\"X\": 2.690385580062866, \"Y\": -9.949936866760254, \"LABEL\": \"Ida B Wells\", \"CLUSTER\": 5}, {\"X\": 7.520230770111084, \"Y\": -3.9424619674682617, \"LABEL\": \"Melvil Dewey\", \"CLUSTER\": 5}, {\"X\": -4.427181720733643, \"Y\": 8.317904472351074, \"LABEL\": \"Thomas Edison\", \"CLUSTER\": 3}, {\"X\": 4.436167240142822, \"Y\": 20.830867767333984, \"LABEL\": \"Knute Rocke\", \"CLUSTER\": 1}, {\"X\": -6.756148338317871, \"Y\": 17.858734130859375, \"LABEL\": \"John Philip Sousa\", \"CLUSTER\": 9}, {\"X\": -9.516679763793945, \"Y\": 15.955499649047852, \"LABEL\": \"Florenz Ziegfeld\", \"CLUSTER\": 14}, {\"X\": 5.917566299438477, \"Y\": -18.310640335083008, \"LABEL\": \"Calvin Coolidge\", \"CLUSTER\": 8}, {\"X\": -2.748025417327881, \"Y\": 1.688985824584961, \"LABEL\": \"Ring Lardner\", \"CLUSTER\": 9}, {\"X\": 2.5936787128448486, \"Y\": 10.610109329223633, \"LABEL\": \"Louis C Tiffany\", \"CLUSTER\": 10}, {\"X\": 7.165395736694336, \"Y\": 5.762889862060547, \"LABEL\": \"T A Watson\", \"CLUSTER\": 3}, {\"X\": 3.2825615406036377, \"Y\": -14.545103073120117, \"LABEL\": \"Justice Holmes\", \"CLUSTER\": 8}, {\"X\": 4.036138534545898, \"Y\": -12.795279502868652, \"LABEL\": \"Jane Addams\", \"CLUSTER\": 5}, {\"X\": -1.245707392692566, \"Y\": 4.458401203155518, \"LABEL\": \"Will Rogers\", \"CLUSTER\": 9}, {\"X\": 2.270707607269287, \"Y\": -7.003915309906006, \"LABEL\": \"Adolph S Ochs\", \"CLUSTER\": 5}, {\"X\": -8.896123886108398, \"Y\": 2.9925708770751953, \"LABEL\": \"Anne Macy\", \"CLUSTER\": 9}, {\"X\": 5.590899467468262, \"Y\": 21.519197463989258, \"LABEL\": \"John W Heisman\", \"CLUSTER\": 1}, {\"X\": 2.01053786277771, \"Y\": -2.3436379432678223, \"LABEL\": \"Maxim Gorky\", \"CLUSTER\": 2}, {\"X\": -9.908794403076172, \"Y\": 22.84100914001465, \"LABEL\": \"Maurice Ravel\", \"CLUSTER\": 7}, {\"X\": -4.8431925773620605, \"Y\": -0.5525018572807312, \"LABEL\": \"Edith Wharton\", \"CLUSTER\": 9}, {\"X\": 6.505295276641846, \"Y\": -5.8620147705078125, \"LABEL\": \"John Rockefeller\", \"CLUSTER\": 18}, {\"X\": 3.9247500896453857, \"Y\": -14.95589828491211, \"LABEL\": \"Clarence Darrow\", \"CLUSTER\": 8}, {\"X\": 12.804132461547852, \"Y\": -3.616757392883301, \"LABEL\": \"George E Hale\", \"CLUSTER\": 6}, {\"X\": -3.914403200149536, \"Y\": 13.519213676452637, \"LABEL\": \"Constantin Stanislavsky\", \"CLUSTER\": 7}, {\"X\": -6.564340591430664, \"Y\": 3.656446695327759, \"LABEL\": \"W B Yeats\", \"CLUSTER\": 9}, {\"X\": 3.7699551582336426, \"Y\": -2.7242953777313232, \"LABEL\": \"Pope Pius XI\", \"CLUSTER\": 2}, {\"X\": 2.4019153118133545, \"Y\": 4.9478254318237305, \"LABEL\": \"Howard Carter\", \"CLUSTER\": 3}, {\"X\": -5.265549182891846, \"Y\": 1.0430127382278442, \"LABEL\": \"Scott Fitzgerald\", \"CLUSTER\": 9}, {\"X\": 5.508907794952393, \"Y\": -8.690286636352539, \"LABEL\": \"Marcus Garvey\", \"CLUSTER\": 18}, {\"X\": 8.533053398132324, \"Y\": 6.58959436416626, \"LABEL\": \"Frank Conrad\", \"CLUSTER\": 3}, {\"X\": 5.797258377075195, \"Y\": 17.059532165527344, \"LABEL\": \"Lou Gehrig\", \"CLUSTER\": 12}, {\"X\": -6.856973171234131, \"Y\": -0.04005557671189308, \"LABEL\": \"James Joyce\", \"CLUSTER\": 9}, {\"X\": -9.25129508972168, \"Y\": 0.140128031373024, \"LABEL\": \"Virginia Woolf\", \"CLUSTER\": 9}, {\"X\": -11.549149513244629, \"Y\": 15.181727409362793, \"LABEL\": \"George M Cohan\", \"CLUSTER\": 14}, {\"X\": 9.84423828125, \"Y\": -3.4867053031921387, \"LABEL\": \"J H Kellogg\", \"CLUSTER\": 17}, {\"X\": 9.245756149291992, \"Y\": -2.9804117679595947, \"LABEL\": \"George Washington Carver\", \"CLUSTER\": 17}, {\"X\": 8.371291160583496, \"Y\": -18.89356231689453, \"LABEL\": \"Alfred E Smith\", \"CLUSTER\": 8}, {\"X\": 1.4978272914886475, \"Y\": -4.835948467254639, \"LABEL\": \"Ida M Tarbell\", \"CLUSTER\": 9}, {\"X\": -1.6803885698318481, \"Y\": 5.1880316734313965, \"LABEL\": \"Ernie Pyle\", \"CLUSTER\": 13}, {\"X\": 4.058639049530029, \"Y\": -20.418596267700195, \"LABEL\": \"Harry S Truman\", \"CLUSTER\": 4}, {\"X\": -0.8401336669921875, \"Y\": -15.223628044128418, \"LABEL\": \"George Patton\", \"CLUSTER\": 13}, {\"X\": 9.216495513916016, \"Y\": -18.456850051879883, \"LABEL\": \"FDR\", \"CLUSTER\": 8}, {\"X\": -12.015007019042969, \"Y\": 18.891151428222656, \"LABEL\": \"Jerome Kern\", \"CLUSTER\": 11}, {\"X\": -1.1993987560272217, \"Y\": -23.22262191772461, \"LABEL\": \"Adolf Hitler\", \"CLUSTER\": 4}, {\"X\": -10.432635307312012, \"Y\": 23.799230575561523, \"LABEL\": \"Bela Bartok\", \"CLUSTER\": 7}, {\"X\": -5.089295864105225, \"Y\": -0.15323784947395325, \"LABEL\": \"Gertrude Stein\", \"CLUSTER\": 9}, {\"X\": 0.20181503891944885, \"Y\": -18.508689880371094, \"LABEL\": \"Lord Keynes\", \"CLUSTER\": 4}, {\"X\": 14.339727401733398, \"Y\": -3.8698630332946777, \"LABEL\": \"C E M Clung\", \"CLUSTER\": 6}, {\"X\": -7.172032356262207, \"Y\": -2.4086458683013916, \"LABEL\": \"Willa Cather\", \"CLUSTER\": 9}, {\"X\": 0.21853679418563843, \"Y\": 5.627852916717529, \"LABEL\": \"Al Capone\", \"CLUSTER\": 13}, {\"X\": 6.648294448852539, \"Y\": -17.572429656982422, \"LABEL\": \"Fiorello La Guardia\", \"CLUSTER\": 8}, {\"X\": 13.307640075683594, \"Y\": 0.42554211616516113, \"LABEL\": \"Max Planck\", \"CLUSTER\": 6}, {\"X\": 8.608011245727539, \"Y\": -5.832132339477539, \"LABEL\": \"Henry Ford\", \"CLUSTER\": 18}, {\"X\": -1.0562225580215454, \"Y\": -15.967155456542969, \"LABEL\": \"John Pershing\", \"CLUSTER\": 13}, {\"X\": -4.043996334075928, \"Y\": 12.996066093444824, \"LABEL\": \"Sergei Eisenstein\", \"CLUSTER\": 7}, {\"X\": 2.2623484134674072, \"Y\": -24.15312385559082, \"LABEL\": \"Mohandas K Gandhi\", \"CLUSTER\": 4}, {\"X\": 4.894998073577881, \"Y\": 19.319122314453125, \"LABEL\": \"Babe Ruth\", \"CLUSTER\": 1}, {\"X\": -2.856966018676758, \"Y\": 0.6255747675895691, \"LABEL\": \"Mitchell\", \"CLUSTER\": 9}, {\"X\": 13.911664962768555, \"Y\": -1.560059905052185, \"LABEL\": \"A J Dempster\", \"CLUSTER\": 6}, {\"X\": -7.5595574378967285, \"Y\": 1.3543107509613037, \"LABEL\": \"Edna St V Millay\", \"CLUSTER\": 9}, {\"X\": 3.163919687271118, \"Y\": -19.970783233642578, \"LABEL\": \"Henry L Stimson\", \"CLUSTER\": 4}, {\"X\": -13.955941200256348, \"Y\": 14.72899055480957, \"LABEL\": \"Fanny Brice\", \"CLUSTER\": 14}, {\"X\": 10.77446460723877, \"Y\": 3.5342905521392822, \"LABEL\": \"Henrietta Lacks\", \"CLUSTER\": 17}, {\"X\": 1.9521914720535278, \"Y\": -18.063495635986328, \"LABEL\": \"Eva Peron\", \"CLUSTER\": 4}, {\"X\": 10.432589530944824, \"Y\": -9.631891250610352, \"LABEL\": \"John Dewey\", \"CLUSTER\": 5}, {\"X\": 3.5099353790283203, \"Y\": -26.283109664916992, \"LABEL\": \"Chaim Weizmann\", \"CLUSTER\": 16}, {\"X\": 8.09255599975586, \"Y\": -8.190781593322754, \"LABEL\": \"Charles Spaulding\", \"CLUSTER\": 5}, {\"X\": 4.831263542175293, \"Y\": -18.89034080505371, \"LABEL\": \"Fred Vinson\", \"CLUSTER\": 8}, {\"X\": -7.062807083129883, \"Y\": -2.807157278060913, \"LABEL\": \"Marjorie Rawlings\", \"CLUSTER\": 9}, {\"X\": -1.870943307876587, \"Y\": -24.25212287902832, \"LABEL\": \"Joseph Stalin\", \"CLUSTER\": 4}, {\"X\": 3.240004301071167, \"Y\": 20.131383895874023, \"LABEL\": \"Jim Thorpe\", \"CLUSTER\": 15}, {\"X\": -5.3576507568359375, \"Y\": 5.728669166564941, \"LABEL\": \"Eugene O Neill\", \"CLUSTER\": 9}, {\"X\": 1.3012663125991821, \"Y\": -6.720988750457764, \"LABEL\": \"Anne O Hare McCormick\", \"CLUSTER\": 5}, {\"X\": -1.6752671003341675, \"Y\": 7.95874547958374, \"LABEL\": \"Frida Kahlo\", \"CLUSTER\": 10}, {\"X\": 0.5131005644798279, \"Y\": -22.073766708374023, \"LABEL\": \"Getulio Vargas\", \"CLUSTER\": 4}, {\"X\": 13.51837158203125, \"Y\": -1.1113442182540894, \"LABEL\": \"Enrico Fermi\", \"CLUSTER\": 6}, {\"X\": 0.08283113688230515, \"Y\": 8.975573539733887, \"LABEL\": \"Henri Matisse\", \"CLUSTER\": 10}, {\"X\": 8.589686393737793, \"Y\": -2.9787580966949463, \"LABEL\": \"Liberty H Bailey\", \"CLUSTER\": 5}, {\"X\": -12.400946617126465, \"Y\": 13.532814979553223, \"LABEL\": \"Lionel Barrymore\", \"CLUSTER\": 14}, {\"X\": 1.1347702741622925, \"Y\": -0.9631224274635315, \"LABEL\": \"Thomas Mann\", \"CLUSTER\": 2}, {\"X\": 5.868532180786133, \"Y\": -0.1329895257949829, \"LABEL\": \"Albert Einstein\", \"CLUSTER\": 2}, {\"X\": 0.644413948059082, \"Y\": 20.133750915527344, \"LABEL\": \"Margaret Abbott\", \"CLUSTER\": 15}, {\"X\": 8.071982383728027, \"Y\": -13.857742309570312, \"LABEL\": \"Walter White\", \"CLUSTER\": 5}, {\"X\": 5.629862308502197, \"Y\": 17.75945472717285, \"LABEL\": \"Cy Young\", \"CLUSTER\": 1}, {\"X\": -3.1962456703186035, \"Y\": -6.119327068328857, \"LABEL\": \"Dale Carnegie\", \"CLUSTER\": 5}, {\"X\": 2.187002182006836, \"Y\": 20.86017417907715, \"LABEL\": \"Babe Zaharias\", \"CLUSTER\": 15}, {\"X\": 7.334896564483643, \"Y\": -5.641847133636475, \"LABEL\": \"Charles Merrill\", \"CLUSTER\": 18}, {\"X\": 8.088459014892578, \"Y\": -6.528273105621338, \"LABEL\": \"Thomas J Watson Sr\", \"CLUSTER\": 18}, {\"X\": 8.822431564331055, \"Y\": -7.036258697509766, \"LABEL\": \"Gerard Swope\", \"CLUSTER\": 18}, {\"X\": 0.019837092608213425, \"Y\": 12.256941795349121, \"LABEL\": \"Christian Dior\", \"CLUSTER\": 10}, {\"X\": -14.10526180267334, \"Y\": 19.550914764404297, \"LABEL\": \"W C Handy\", \"CLUSTER\": 11}, {\"X\": -14.630707740783691, \"Y\": 19.120119094848633, \"LABEL\": \"Billie Holiday\", \"CLUSTER\": 11}, {\"X\": 4.07633638381958, \"Y\": 8.701501846313477, \"LABEL\": \"Frank Lloyd Wright\", \"CLUSTER\": 10}, {\"X\": -12.73944091796875, \"Y\": 13.955618858337402, \"LABEL\": \"Ethel Barrymore\", \"CLUSTER\": 14}, {\"X\": -7.092732906341553, \"Y\": 12.458260536193848, \"LABEL\": \"Cecil De Mille\", \"CLUSTER\": 14}, {\"X\": 14.156725883483887, \"Y\": -3.8438351154327393, \"LABEL\": \"Ross G Harrison\", \"CLUSTER\": 6}, {\"X\": 2.8466544151306152, \"Y\": -20.701236724853516, \"LABEL\": \"John Dulles\", \"CLUSTER\": 4}, {\"X\": 1.4341976642608643, \"Y\": -2.3023252487182617, \"LABEL\": \"Boris Pasternak\", \"CLUSTER\": 2}, {\"X\": 14.588375091552734, \"Y\": -2.4826066493988037, \"LABEL\": \"Beno Gutenberg\", \"CLUSTER\": 6}, {\"X\": -1.3730815649032593, \"Y\": 2.8357672691345215, \"LABEL\": \"Emily Post\", \"CLUSTER\": 9}, {\"X\": -4.77087926864624, \"Y\": -4.387993335723877, \"LABEL\": \"Richard Wright\", \"CLUSTER\": 9}, {\"X\": 2.366114377975464, \"Y\": -21.633243560791016, \"LABEL\": \"Hammarskjold\", \"CLUSTER\": 4}, {\"X\": -4.490510940551758, \"Y\": 1.1779651641845703, \"LABEL\": \"Ernest Hemingway\", \"CLUSTER\": 9}, {\"X\": -2.0805320739746094, \"Y\": 6.881127834320068, \"LABEL\": \"Primitive Artist\", \"CLUSTER\": 10}, {\"X\": 9.335868835449219, \"Y\": -11.504140853881836, \"LABEL\": \"Emily Balch\", \"CLUSTER\": 5}, {\"X\": 7.491336345672607, \"Y\": -20.341108322143555, \"LABEL\": \"Sam Rayburn\", \"CLUSTER\": 8}, {\"X\": 5.287370204925537, \"Y\": 1.026353359222412, \"LABEL\": \"Carl G Jung\", \"CLUSTER\": 2}, {\"X\": -7.579326629638672, \"Y\": 7.755613803863525, \"LABEL\": \"Marilyn Monroe\", \"CLUSTER\": 14}, {\"X\": 6.434357643127441, \"Y\": -15.811779022216797, \"LABEL\": \"Eleanor Roosevelt\", \"CLUSTER\": 5}, {\"X\": -6.365342140197754, \"Y\": -0.5313624739646912, \"LABEL\": \"William Faulkner\", \"CLUSTER\": 9}, {\"X\": -8.25678825378418, \"Y\": 0.12019874900579453, \"LABEL\": \"Sylvia Plath\", \"CLUSTER\": 9}, {\"X\": 5.7489495277404785, \"Y\": -20.73656463623047, \"LABEL\": \"John F Kennedy\", \"CLUSTER\": 8}, {\"X\": -5.720600128173828, \"Y\": -1.2441251277923584, \"LABEL\": \"Robert Frost\", \"CLUSTER\": 9}, {\"X\": 9.770573616027832, \"Y\": -10.765989303588867, \"LABEL\": \"W E B DuBois\", \"CLUSTER\": 5}, {\"X\": 9.999427795410156, \"Y\": -15.389067649841309, \"LABEL\": \"Herbert Hoover\", \"CLUSTER\": 5}, {\"X\": -0.9084845781326294, \"Y\": -16.072019577026367, \"LABEL\": \"Douglas MacArthur\", \"CLUSTER\": 13}, {\"X\": -3.2682387828826904, \"Y\": 3.656870126724243, \"LABEL\": \"Sean O Casey\", \"CLUSTER\": 9}, {\"X\": 8.007101058959961, \"Y\": -0.9683744311332703, \"LABEL\": \"Rachel Carson\", \"CLUSTER\": 17}, {\"X\": -12.125691413879395, \"Y\": 17.64192771911621, \"LABEL\": \"Cole Porter\", \"CLUSTER\": 11}, {\"X\": -4.58758020401001, \"Y\": -2.1346490383148193, \"LABEL\": \"Nella Larsen\", \"CLUSTER\": 9}, {\"X\": 6.842874526977539, \"Y\": -19.00663185119629, \"LABEL\": \"Adlai Ewing Stevenson\", \"CLUSTER\": 8}, {\"X\": -9.394267082214355, \"Y\": 13.346661567687988, \"LABEL\": \"David O Selznick\", \"CLUSTER\": 14}, {\"X\": -1.381027340888977, \"Y\": -19.34161376953125, \"LABEL\": \"Churchill\", \"CLUSTER\": 4}, {\"X\": 5.711763858795166, \"Y\": 19.77413558959961, \"LABEL\": \"Branch Rickey\", \"CLUSTER\": 1}, {\"X\": 3.8823516368865967, \"Y\": -0.213487908244133, \"LABEL\": \"Martin Buber\", \"CLUSTER\": 2}, {\"X\": -0.4909175634384155, \"Y\": -7.9278740882873535, \"LABEL\": \"Edward R Murrow\", \"CLUSTER\": 5}, {\"X\": 4.8164801597595215, \"Y\": -0.28993940353393555, \"LABEL\": \"Albert Schweitzer\", \"CLUSTER\": 2}, {\"X\": -5.047088146209717, \"Y\": 2.665815830230713, \"LABEL\": \"Shirley Jackson\", \"CLUSTER\": 9}, {\"X\": 5.2891435623168945, \"Y\": -10.936084747314453, \"LABEL\": \"Margaret Sanger\", \"CLUSTER\": 5}, {\"X\": -2.9033801555633545, \"Y\": -14.646029472351074, \"LABEL\": \"Chester Nimitz\", \"CLUSTER\": 13}, {\"X\": -9.930489540100098, \"Y\": 9.449071884155273, \"LABEL\": \"Buster Keaton\", \"CLUSTER\": 14}, {\"X\": -0.7975120544433594, \"Y\": 3.8963050842285156, \"LABEL\": \"Lenny Bruce\", \"CLUSTER\": 9}, {\"X\": -6.360071659088135, \"Y\": 11.415752410888672, \"LABEL\": \"Walt Disney\", \"CLUSTER\": 14}, {\"X\": 8.573323249816895, \"Y\": -6.428279399871826, \"LABEL\": \"Alfred P Sloan Jr\", \"CLUSTER\": 18}, {\"X\": 11.55565071105957, \"Y\": 3.184868335723877, \"LABEL\": \"Gregory Pincus\", \"CLUSTER\": 17}, {\"X\": 0.00041557318763807416, \"Y\": -4.988834857940674, \"LABEL\": \"Henry R Luce\", \"CLUSTER\": 9}, {\"X\": -8.7759370803833, \"Y\": -1.413293719291687, \"LABEL\": \"Langston Hughes\", \"CLUSTER\": 9}, {\"X\": 12.157036781311035, \"Y\": -0.9133463501930237, \"LABEL\": \"J Robert Oppenheimer\", \"CLUSTER\": 6}, {\"X\": 7.771406650543213, \"Y\": -18.096389770507812, \"LABEL\": \"Robert Francis Kennedy\", \"CLUSTER\": 8}, {\"X\": -4.025519847869873, \"Y\": 3.2836079597473145, \"LABEL\": \"Helen Keller\", \"CLUSTER\": 9}, {\"X\": 0.4102725684642792, \"Y\": -4.575801372528076, \"LABEL\": \"Upton Sinclair\", \"CLUSTER\": 9}, {\"X\": 2.8380794525146484, \"Y\": -11.659584045410156, \"LABEL\": \"Martin Luther King Jr\", \"CLUSTER\": 5}, {\"X\": -4.41843318939209, \"Y\": -10.777113914489746, \"LABEL\": \"Yuri Gagarin\", \"CLUSTER\": 3}, {\"X\": 3.9844374656677246, \"Y\": 8.830159187316895, \"LABEL\": \"Mies van der Rohe\", \"CLUSTER\": 10}, {\"X\": 4.2705183029174805, \"Y\": -20.593523025512695, \"LABEL\": \"David Eisenhower\", \"CLUSTER\": 4}, {\"X\": -14.627070426940918, \"Y\": 21.661876678466797, \"LABEL\": \"Coleman Hawkins\", \"CLUSTER\": 11}, {\"X\": -12.288959503173828, \"Y\": 9.538145065307617, \"LABEL\": \"Madhubala\", \"CLUSTER\": 14}, {\"X\": -10.91363525390625, \"Y\": 15.947610855102539, \"LABEL\": \"Judy Garland\", \"CLUSTER\": 11}, {\"X\": 1.7566102743148804, \"Y\": 20.89740753173828, \"LABEL\": \"Maureen Connolly\", \"CLUSTER\": 15}, {\"X\": -2.7319345474243164, \"Y\": -22.500757217407227, \"LABEL\": \"Ho Chi Minh\", \"CLUSTER\": 4}, {\"X\": -0.2954440712928772, \"Y\": 20.148393630981445, \"LABEL\": \"Sonja Henie\", \"CLUSTER\": 15}, {\"X\": 6.556248188018799, \"Y\": -20.099365234375, \"LABEL\": \"Everett Dirksen\", \"CLUSTER\": 8}, {\"X\": 11.221527099609375, \"Y\": -14.67345142364502, \"LABEL\": \"Walter Reuther\", \"CLUSTER\": 5}, {\"X\": -1.1447980403900146, \"Y\": -22.354820251464844, \"LABEL\": \"Edouard Daladier\", \"CLUSTER\": 4}, {\"X\": 0.42066845297813416, \"Y\": -0.9524916410446167, \"LABEL\": \"Erich Maria Remarque\", \"CLUSTER\": 2}, {\"X\": -1.1164467334747314, \"Y\": -22.03478240966797, \"LABEL\": \"De Gaulle Rallied\", \"CLUSTER\": 4}, {\"X\": -0.10751062631607056, \"Y\": 12.785686492919922, \"LABEL\": \"Coco Chanel\", \"CLUSTER\": 10}, {\"X\": -2.0791187286376953, \"Y\": -16.304954528808594, \"LABEL\": \"Florence Blanchfield\", \"CLUSTER\": 13}, {\"X\": -2.1270408630371094, \"Y\": -24.744159698486328, \"LABEL\": \"Khrushchev\", \"CLUSTER\": 4}, {\"X\": -2.000955820083618, \"Y\": 10.29051685333252, \"LABEL\": \"Diane Arbus\", \"CLUSTER\": 10}, {\"X\": 3.6846258640289307, \"Y\": -13.232927322387695, \"LABEL\": \"Ralph Bunche\", \"CLUSTER\": 5}, {\"X\": 2.091111421585083, \"Y\": 20.403209686279297, \"LABEL\": \"Bobby Jones\", \"CLUSTER\": 15}, {\"X\": -14.313529014587402, \"Y\": 19.865781784057617, \"LABEL\": \"Louis Armstrong\", \"CLUSTER\": 11}, {\"X\": 3.22304368019104, \"Y\": -20.77686309814453, \"LABEL\": \"Dean Acheson\", \"CLUSTER\": 4}, {\"X\": -10.097384452819824, \"Y\": 22.32509994506836, \"LABEL\": \"Igor Stravinsky\", \"CLUSTER\": 7}, {\"X\": 4.117844581604004, \"Y\": -17.112918853759766, \"LABEL\": \"Hugo Black\", \"CLUSTER\": 8}, {\"X\": -15.796982765197754, \"Y\": 18.43817710876465, \"LABEL\": \"Mahalia Jackson\", \"CLUSTER\": 11}, {\"X\": 5.305239677429199, \"Y\": 19.629106521606445, \"LABEL\": \"Jackie Robinson\", \"CLUSTER\": 1}, {\"X\": -4.282553672790527, \"Y\": -18.213993072509766, \"LABEL\": \"The Duke of Windsor\", \"CLUSTER\": 13}, {\"X\": 8.927469253540039, \"Y\": -15.705222129821777, \"LABEL\": \"J Edgar Hoover\", \"CLUSTER\": 5}, {\"X\": 10.598589897155762, \"Y\": -15.200116157531738, \"LABEL\": \"Lyndon Johnson\", \"CLUSTER\": 5}, {\"X\": -10.891209602355957, \"Y\": 22.281869888305664, \"LABEL\": \"Otto Klemperer\", \"CLUSTER\": 7}, {\"X\": -1.146892786026001, \"Y\": -11.699542045593262, \"LABEL\": \"Eddie Rickenbacker\", \"CLUSTER\": 18}, {\"X\": 6.063342094421387, \"Y\": -16.626441955566406, \"LABEL\": \"Jeanette Rankin\", \"CLUSTER\": 8}, {\"X\": -0.6464793682098389, \"Y\": 8.681639671325684, \"LABEL\": \"Pablo Picasso\", \"CLUSTER\": 10}, {\"X\": -1.6775704622268677, \"Y\": -11.294557571411133, \"LABEL\": \"Roberto Clemente\", \"CLUSTER\": 4}, {\"X\": -6.072815418243408, \"Y\": 2.3014135360717773, \"LABEL\": \"Nancy Mitford\", \"CLUSTER\": 9}, {\"X\": 4.281641006469727, \"Y\": -17.33233642578125, \"LABEL\": \"Earl Warren\", \"CLUSTER\": 8}, {\"X\": -3.6575300693511963, \"Y\": 4.310171604156494, \"LABEL\": \"Sylvia Plath\", \"CLUSTER\": 9}, {\"X\": -7.029117584228516, \"Y\": 14.658928871154785, \"LABEL\": \"Ed Sullivan\", \"CLUSTER\": 9}, {\"X\": -11.440699577331543, \"Y\": 13.920867919921875, \"LABEL\": \"Katharine Cornell\", \"CLUSTER\": 14}, {\"X\": 0.06418497860431671, \"Y\": -11.084087371826172, \"LABEL\": \"Charles Lindbergh\", \"CLUSTER\": 5}, {\"X\": -3.7349493503570557, \"Y\": -20.69317626953125, \"LABEL\": \"Haile Selassie\", \"CLUSTER\": 4}, {\"X\": 2.433624267578125, \"Y\": -11.153874397277832, \"LABEL\": \"Elijah Muhammad\", \"CLUSTER\": 5}, {\"X\": -0.5462758541107178, \"Y\": -22.259645462036133, \"LABEL\": \"Franco\", \"CLUSTER\": 4}, {\"X\": -1.7076590061187744, \"Y\": 10.271132469177246, \"LABEL\": \"Walker Evans\", \"CLUSTER\": 10}, {\"X\": -3.3342642784118652, \"Y\": -23.15538787841797, \"LABEL\": \"Chiang Kai shek\", \"CLUSTER\": 4}, {\"X\": 6.070744037628174, \"Y\": -5.3253607749938965, \"LABEL\": \"J Paul Getty\", \"CLUSTER\": 18}, {\"X\": -4.137241840362549, \"Y\": -23.191923141479492, \"LABEL\": \"Mao Tse Tung\", \"CLUSTER\": 4}, {\"X\": -0.27759745717048645, \"Y\": 8.607646942138672, \"LABEL\": \"Max Ernst\", \"CLUSTER\": 10}, {\"X\": 9.683806419372559, \"Y\": -19.59040641784668, \"LABEL\": \"Richard Daley\", \"CLUSTER\": 8}, {\"X\": 11.677897453308105, \"Y\": 0.38013437390327454, \"LABEL\": \"Jacques Monod\", \"CLUSTER\": 6}, {\"X\": -6.574921607971191, \"Y\": 12.041075706481934, \"LABEL\": \"Adolph Zukor\", \"CLUSTER\": 14}, {\"X\": -9.727242469787598, \"Y\": 9.819466590881348, \"LABEL\": \"Charles Chaplin\", \"CLUSTER\": 14}, {\"X\": -11.043335914611816, \"Y\": 12.999029159545898, \"LABEL\": \"Joan Crawford\", \"CLUSTER\": 14}, {\"X\": -0.9071201086044312, \"Y\": -19.694175720214844, \"LABEL\": \"Dash Ended\", \"CLUSTER\": 4}, {\"X\": -10.317038536071777, \"Y\": 19.612215042114258, \"LABEL\": \"Maria Callas\", \"CLUSTER\": 7}, {\"X\": 2.7228610515594482, \"Y\": -26.44182777404785, \"LABEL\": \"Golda Meir\", \"CLUSTER\": 16}, {\"X\": 6.646701335906982, \"Y\": 1.0209088325500488, \"LABEL\": \"Margaret Mead\", \"CLUSTER\": 2}, {\"X\": 2.1077523231506348, \"Y\": -12.60781192779541, \"LABEL\": \"Pope Paul VI\", \"CLUSTER\": 2}, {\"X\": 12.33845043182373, \"Y\": -11.70703125, \"LABEL\": \"Bruce Catton\", \"CLUSTER\": 5}, {\"X\": -11.74393081665039, \"Y\": 21.50006103515625, \"LABEL\": \"Arthur Fiedler\", \"CLUSTER\": 7}, {\"X\": -8.150505065917969, \"Y\": 11.311758041381836, \"LABEL\": \"John Wayne\", \"CLUSTER\": 14}, {\"X\": 6.948338985443115, \"Y\": -14.58864688873291, \"LABEL\": \"A Philip Randolph\", \"CLUSTER\": 5}, {\"X\": -13.960192680358887, \"Y\": 21.73772621154785, \"LABEL\": \"Stan Kenton\", \"CLUSTER\": 11}, {\"X\": -12.107344627380371, \"Y\": 18.3758602142334, \"LABEL\": \"Richard Rodgers\", \"CLUSTER\": 11}, {\"X\": 2.986626386642456, \"Y\": 19.712684631347656, \"LABEL\": \"Jesse Owens\", \"CLUSTER\": 15}, {\"X\": -8.183670997619629, \"Y\": 9.536137580871582, \"LABEL\": \"Alfred Hitchcock\", \"CLUSTER\": 14}, {\"X\": 6.70174503326416, \"Y\": 1.8510829210281372, \"LABEL\": \"Jean Piaget\", \"CLUSTER\": 2}, {\"X\": 2.353790044784546, \"Y\": -0.572279155254364, \"LABEL\": \"Jean Paul Sartre\", \"CLUSTER\": 2}, {\"X\": 1.7528080940246582, \"Y\": 17.703086853027344, \"LABEL\": \"Joe Louis\", \"CLUSTER\": 15}, {\"X\": 5.644028186798096, \"Y\": -7.230025291442871, \"LABEL\": \"Robert Moses\", \"CLUSTER\": 18}, {\"X\": 2.0614261627197266, \"Y\": -26.772367477416992, \"LABEL\": \"Anwar el Sadat\", \"CLUSTER\": 16}, {\"X\": -11.400519371032715, \"Y\": 11.441492080688477, \"LABEL\": \"Ingrid Bergman\", \"CLUSTER\": 14}, {\"X\": 5.484755992889404, \"Y\": 1.6772611141204834, \"LABEL\": \"Anna Freud\", \"CLUSTER\": 2}, {\"X\": -1.912028193473816, \"Y\": -25.10733413696289, \"LABEL\": \"Leonid Brezhnev\", \"CLUSTER\": 4}, {\"X\": -10.517951011657715, \"Y\": 21.389081954956055, \"LABEL\": \"Arthur Rubinstein\", \"CLUSTER\": 7}, {\"X\": -14.269951820373535, \"Y\": 21.065935134887695, \"LABEL\": \"Thelonious Monk\", \"CLUSTER\": 11}, {\"X\": -8.867549896240234, \"Y\": 14.433915138244629, \"LABEL\": \"Lee Strasberg\", \"CLUSTER\": 14}, {\"X\": 5.190177917480469, \"Y\": 19.076826095581055, \"LABEL\": \"Satchel Paige\", \"CLUSTER\": 1}, {\"X\": 1.5962300300598145, \"Y\": 17.764183044433594, \"LABEL\": \"Jack Dempsey\", \"CLUSTER\": 15}, {\"X\": -16.065866470336914, \"Y\": 22.428876876831055, \"LABEL\": \"Earl Hines\", \"CLUSTER\": 11}, {\"X\": -16.141265869140625, \"Y\": 20.509902954101562, \"LABEL\": \"Muddy Waters\", \"CLUSTER\": 11}, {\"X\": -5.086297035217285, \"Y\": 1.3645154237747192, \"LABEL\": \"Truman Capote\", \"CLUSTER\": 9}, {\"X\": -3.7623701095581055, \"Y\": 2.300950288772583, \"LABEL\": \"Lillian Hellman\", \"CLUSTER\": 9}, {\"X\": -0.18157348036766052, \"Y\": 18.524791717529297, \"LABEL\": \"Johnny Weissmuller\", \"CLUSTER\": 15}, {\"X\": 1.7264039516448975, \"Y\": 10.500162124633789, \"LABEL\": \"Ansel Adams\", \"CLUSTER\": 10}, {\"X\": -11.954716682434082, \"Y\": 16.602327346801758, \"LABEL\": \"Ethel Merman\", \"CLUSTER\": 11}, {\"X\": 2.271125316619873, \"Y\": -24.124380111694336, \"LABEL\": \"Indira Gandhi\", \"CLUSTER\": 4}, {\"X\": 4.975656032562256, \"Y\": 5.127691268920898, \"LABEL\": \"Ray A Kroc\", \"CLUSTER\": 18}, {\"X\": -12.228028297424316, \"Y\": 12.913139343261719, \"LABEL\": \"Richard Burton\", \"CLUSTER\": 14}, {\"X\": -15.569748878479004, \"Y\": 21.67116355895996, \"LABEL\": \"Count Basie\", \"CLUSTER\": 11}, {\"X\": -2.1185460090637207, \"Y\": 3.2466747760772705, \"LABEL\": \"E B White\", \"CLUSTER\": 9}, {\"X\": -8.22596549987793, \"Y\": 12.748188018798828, \"LABEL\": \"Orson Welles\", \"CLUSTER\": 14}, {\"X\": 5.446950435638428, \"Y\": 18.269994735717773, \"LABEL\": \"Roger Maris\", \"CLUSTER\": 1}, {\"X\": -10.144200325012207, \"Y\": 11.936922073364258, \"LABEL\": \"James Cagney\", \"CLUSTER\": 14}, {\"X\": 0.42267510294914246, \"Y\": 10.323667526245117, \"LABEL\": \"Georgia O Keeffe\", \"CLUSTER\": 10}, {\"X\": -15.13720417022705, \"Y\": 21.26741600036621, \"LABEL\": \"Benny Goodman\", \"CLUSTER\": 11}, {\"X\": -1.9517865180969238, \"Y\": -1.7426565885543823, \"LABEL\": \"Jorge Luis Borges\", \"CLUSTER\": 9}, {\"X\": -3.0005218982696533, \"Y\": -2.1604509353637695, \"LABEL\": \"Bernard Malamud\", \"CLUSTER\": 9}, {\"X\": -7.233739376068115, \"Y\": 16.33815574645996, \"LABEL\": \"Kate Smith\", \"CLUSTER\": 9}, {\"X\": -3.284060001373291, \"Y\": -8.799633979797363, \"LABEL\": \"The Challenger\", \"CLUSTER\": 5}, {\"X\": -0.16135701537132263, \"Y\": -1.3806411027908325, \"LABEL\": \"Primo Levi\", \"CLUSTER\": 2}, {\"X\": 0.160842165350914, \"Y\": -5.715259552001953, \"LABEL\": \"Clare Boothe Luce\", \"CLUSTER\": 9}, {\"X\": -9.35444450378418, \"Y\": 11.49026870727539, \"LABEL\": \"John Huston\", \"CLUSTER\": 14}, {\"X\": -12.722125053405762, \"Y\": 11.221147537231445, \"LABEL\": \"Rita Hayworth\", \"CLUSTER\": 14}, {\"X\": 2.0823161602020264, \"Y\": -9.595553398132324, \"LABEL\": \"James Baldwin\", \"CLUSTER\": 5}, {\"X\": 8.856524467468262, \"Y\": -19.307838439941406, \"LABEL\": \"Alf Landon\", \"CLUSTER\": 8}, {\"X\": -11.902486801147461, \"Y\": 22.863727569580078, \"LABEL\": \"Andres Segovie\", \"CLUSTER\": 7}, {\"X\": -8.181717872619629, \"Y\": 13.401019096374512, \"LABEL\": \"John Houseman\", \"CLUSTER\": 14}, {\"X\": -5.737271308898926, \"Y\": 0.15060238540172577, \"LABEL\": \"Louis L Amour\", \"CLUSTER\": 9}, {\"X\": 11.293539047241211, \"Y\": -1.3875865936279297, \"LABEL\": \"William B Shockley\", \"CLUSTER\": 6}, {\"X\": -10.462252616882324, \"Y\": 14.882281303405762, \"LABEL\": \"Lucille Ball\", \"CLUSTER\": 14}, {\"X\": -5.80594539642334, \"Y\": -1.162826418876648, \"LABEL\": \"Robert Penn Warren\", \"CLUSTER\": 9}, {\"X\": 11.61480712890625, \"Y\": -16.435943603515625, \"LABEL\": \"Ferdinand Marcos\", \"CLUSTER\": 4}, {\"X\": -2.4243974685668945, \"Y\": -25.397769927978516, \"LABEL\": \"Andrei Sakharov\", \"CLUSTER\": 4}, {\"X\": -1.439214825630188, \"Y\": -25.602018356323242, \"LABEL\": \"Andrei A Gromyko\", \"CLUSTER\": 4}, {\"X\": 1.8964375257492065, \"Y\": -8.184626579284668, \"LABEL\": \"I F Stone\", \"CLUSTER\": 5}, {\"X\": -10.617868423461914, \"Y\": 21.194807052612305, \"LABEL\": \"Vladimir Horowitz\", \"CLUSTER\": 7}, {\"X\": -4.264729976654053, \"Y\": -20.33122444152832, \"LABEL\": \"Hirohito\", \"CLUSTER\": 4}, {\"X\": 7.009855270385742, \"Y\": 9.991463661193848, \"LABEL\": \"August A Busch Jr\", \"CLUSTER\": 18}, {\"X\": 7.647704601287842, \"Y\": -21.37030029296875, \"LABEL\": \"Claude Pepper\", \"CLUSTER\": 8}, {\"X\": -6.073394298553467, \"Y\": 4.2050347328186035, \"LABEL\": \"Samuel Beckett\", \"CLUSTER\": 9}, {\"X\": -10.959029197692871, \"Y\": 10.45793628692627, \"LABEL\": \"Greta Garbo\", \"CLUSTER\": 14}, {\"X\": -10.609621047973633, \"Y\": 12.900979042053223, \"LABEL\": \"Sammy Davis Jr\", \"CLUSTER\": 14}, {\"X\": -11.28962516784668, \"Y\": 21.364112854003906, \"LABEL\": \"Leonard Bernstein\", \"CLUSTER\": 7}, {\"X\": 0.227839857339859, \"Y\": 11.358028411865234, \"LABEL\": \"Erte\", \"CLUSTER\": 10}, {\"X\": 6.354766845703125, \"Y\": -14.028027534484863, \"LABEL\": \"Ralph David Abernathy\", \"CLUSTER\": 5}, {\"X\": -13.484834671020508, \"Y\": 13.493277549743652, \"LABEL\": \"Rex Harrison\", \"CLUSTER\": 14}, {\"X\": -9.161225318908691, \"Y\": 11.457062721252441, \"LABEL\": \"Frank Capra\", \"CLUSTER\": 14}, {\"X\": -9.09293270111084, \"Y\": 5.257132053375244, \"LABEL\": \"Dr Seuss\", \"CLUSTER\": 9}, {\"X\": -14.794368743896484, \"Y\": 22.301076889038086, \"LABEL\": \"Miles Davis\", \"CLUSTER\": 11}, {\"X\": -8.754213333129883, \"Y\": 18.975425720214844, \"LABEL\": \"Martha Graham\", \"CLUSTER\": 7}, {\"X\": 6.248449802398682, \"Y\": 19.179615020751953, \"LABEL\": \"Leo Durocher\", \"CLUSTER\": 1}, {\"X\": -14.048850059509277, \"Y\": 12.563765525817871, \"LABEL\": \"Peggy Ashcroft\", \"CLUSTER\": 14}, {\"X\": -5.369603633880615, \"Y\": -2.4714550971984863, \"LABEL\": \"Alex Haley\", \"CLUSTER\": 9}, {\"X\": -9.515295028686523, \"Y\": 21.74912452697754, \"LABEL\": \"John Cage\", \"CLUSTER\": 7}, {\"X\": 2.635098695755005, \"Y\": -27.009138107299805, \"LABEL\": \"Menachem Begin\", \"CLUSTER\": 16}, {\"X\": -12.868858337402344, \"Y\": 14.436298370361328, \"LABEL\": \"Shirley Booth\", \"CLUSTER\": 14}, {\"X\": -3.589008331298828, \"Y\": -3.1664249897003174, \"LABEL\": \"Isaac Asimov\", \"CLUSTER\": 9}, {\"X\": -1.5208920240402222, \"Y\": -4.282694339752197, \"LABEL\": \"William Shawn\", \"CLUSTER\": 9}, {\"X\": 3.540773630142212, \"Y\": -9.904070854187012, \"LABEL\": \"Marsha P Johnson\", \"CLUSTER\": 5}, {\"X\": 4.08953332901001, \"Y\": -16.77513313293457, \"LABEL\": \"Thurgood Marshall\", \"CLUSTER\": 8}, {\"X\": -8.321367263793945, \"Y\": 9.756512641906738, \"LABEL\": \"Federico Fellini\", \"CLUSTER\": 14}, {\"X\": 7.209724426269531, \"Y\": -23.14278221130371, \"LABEL\": \"Cesar Chavez\", \"CLUSTER\": 4}, {\"X\": -12.737433433532715, \"Y\": 23.424379348754883, \"LABEL\": \"Carlos Montoya\", \"CLUSTER\": 7}, {\"X\": -15.065054893493652, \"Y\": 22.669597625732422, \"LABEL\": \"Dizzy Gillespie\", \"CLUSTER\": 11}, {\"X\": 2.0340092182159424, \"Y\": 21.639955520629883, \"LABEL\": \"Arthur Ashe\", \"CLUSTER\": 15}, {\"X\": -3.833820104598999, \"Y\": -0.9150063991546631, \"LABEL\": \"William Golding\", \"CLUSTER\": 9}, {\"X\": 13.081901550292969, \"Y\": 3.823532819747925, \"LABEL\": \"Albert Sabin\", \"CLUSTER\": 17}, {\"X\": 7.929497241973877, \"Y\": -20.055503845214844, \"LABEL\": \"Thomas P O Neill Jr\", \"CLUSTER\": 8}, {\"X\": 6.86980676651001, \"Y\": -19.333242416381836, \"LABEL\": \"Richard Nixon\", \"CLUSTER\": 8}, {\"X\": 5.999512195587158, \"Y\": 1.4719454050064087, \"LABEL\": \"Erik Erikson\", \"CLUSTER\": 2}, {\"X\": 12.014708518981934, \"Y\": -0.6923053860664368, \"LABEL\": \"Linus C Pauling\", \"CLUSTER\": 6}, {\"X\": -11.61903190612793, \"Y\": 14.309277534484863, \"LABEL\": \"Jessica Tandy\", \"CLUSTER\": 14}, {\"X\": 12.510062217712402, \"Y\": -8.423639297485352, \"LABEL\": \"Jan Tinbergen\", \"CLUSTER\": 5}, {\"X\": -2.5444443225860596, \"Y\": -0.5598430037498474, \"LABEL\": \"Jacqueline Kennedy\", \"CLUSTER\": 9}, {\"X\": 13.037522315979004, \"Y\": 3.7997031211853027, \"LABEL\": \"Jonas Salk\", \"CLUSTER\": 17}, {\"X\": -2.3941287994384766, \"Y\": 10.565855026245117, \"LABEL\": \"Alfred Eisenstaedt\", \"CLUSTER\": 10}, {\"X\": -12.642889022827148, \"Y\": 15.47425365447998, \"LABEL\": \"Ginger Rogers\", \"CLUSTER\": 14}, {\"X\": -10.982821464538574, \"Y\": 14.190242767333984, \"LABEL\": \"George Abbott\", \"CLUSTER\": 14}, {\"X\": 2.525446653366089, \"Y\": -27.234472274780273, \"LABEL\": \"Yitzhak Rabin\", \"CLUSTER\": 16}, {\"X\": 9.54769515991211, \"Y\": -0.1324111521244049, \"LABEL\": \"Carl Sagan\", \"CLUSTER\": 6}, {\"X\": 0.5164222717285156, \"Y\": 1.8136403560638428, \"LABEL\": \"Timothy Leary\", \"CLUSTER\": 9}, {\"X\": -13.102167129516602, \"Y\": 16.364717483520508, \"LABEL\": \"Gene Kelly\", \"CLUSTER\": 11}, {\"X\": -7.367973327636719, \"Y\": -0.7886866331100464, \"LABEL\": \"Allen Ginsberg\", \"CLUSTER\": 9}, {\"X\": -4.148149490356445, \"Y\": -23.655927658081055, \"LABEL\": \"Deng Xiaoping\", \"CLUSTER\": 4}, {\"X\": -9.627241134643555, \"Y\": 12.181411743164062, \"LABEL\": \"James Stewart\", \"CLUSTER\": 14}, {\"X\": -10.782670021057129, \"Y\": 7.878483772277832, \"LABEL\": \"Bob Kane\", \"CLUSTER\": 9}, {\"X\": 3.504016399383545, \"Y\": 2.526505470275879, \"LABEL\": \"Benjamin Spock\", \"CLUSTER\": 9}, {\"X\": 1.4857224225997925, \"Y\": 20.424386978149414, \"LABEL\": \"Helen Moody\", \"CLUSTER\": 15}, {\"X\": -13.051321029663086, \"Y\": 12.466283798217773, \"LABEL\": \"Maureen O Sullivan\", \"CLUSTER\": 14}, {\"X\": 11.939977645874023, \"Y\": -8.256277084350586, \"LABEL\": \"Theodore Schultz\", \"CLUSTER\": 5}, {\"X\": -4.298580169677734, \"Y\": -10.627726554870605, \"LABEL\": \"Alan B Shepard Jr\", \"CLUSTER\": 3}, {\"X\": -8.180453300476074, \"Y\": 19.80317497253418, \"LABEL\": \"Galina Ulanova\", \"CLUSTER\": 7}, {\"X\": 8.874265670776367, \"Y\": -20.762174606323242, \"LABEL\": \"Bella Abzug\", \"CLUSTER\": 8}, {\"X\": -0.5873187780380249, \"Y\": -7.927563667297363, \"LABEL\": \"Fred W Friendly\", \"CLUSTER\": 5}, {\"X\": -13.889960289001465, \"Y\": 18.14456558227539, \"LABEL\": \"Frank Sinatra\", \"CLUSTER\": 11}, {\"X\": 1.1848206520080566, \"Y\": -26.802824020385742, \"LABEL\": \"Hassan II\", \"CLUSTER\": 16}, {\"X\": -1.3036211729049683, \"Y\": 0.27954360842704773, \"LABEL\": \"Iris Murdoch\", \"CLUSTER\": 9}, {\"X\": 1.455483317375183, \"Y\": -27.17296028137207, \"LABEL\": \"King Hussein\", \"CLUSTER\": 16}, {\"X\": -0.09497282654047012, \"Y\": -20.308443069458008, \"LABEL\": \"Pierre Trudeau\", \"CLUSTER\": 4}, {\"X\": 8.329566955566406, \"Y\": -17.360031127929688, \"LABEL\": \"Elliot Richardson\", \"CLUSTER\": 8}, {\"X\": -9.796686172485352, \"Y\": 6.504615306854248, \"LABEL\": \"Charles M Schulz\", \"CLUSTER\": 9}, {\"X\": 8.127979278564453, \"Y\": 1.5808155536651611, \"LABEL\": \"Karen Sparck Jones\", \"CLUSTER\": 2}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_data['CLUSTER'] = agg.labels_ + 1\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip=['LABEL', 'CLUSTER'],\n",
    "    color='CLUSTER:N'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742d46b",
   "metadata": {},
   "source": [
    "Once again, take a look around and see what you can find. These clusters seem to be both detailed and nicely partitioned, bracketing off, for example, classical musicians and composers (cluster 7) from jazz and popular musicians (cluster 11).\n",
    "\n",
    "```{margin} What this loop does\n",
    "For each cluster:\n",
    "\n",
    "1. Subset the visualization data with that cluster and get all the people\n",
    "2. Convert the column into string and then split that string into chunks with `textwrap`\n",
    "3. Print to screen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2e8c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:  7\n",
      "-----------\n",
      "Maurice Ravel, Constantin Stanislavsky, Bela Bartok, Sergei Eisenstein, Igor\n",
      "Stravinsky, Otto Klemperer, Maria Callas, Arthur Fiedler, Arthur Rubinstein,\n",
      "Andres Segovie, Vladimir Horowitz, Leonard Bernstein, Martha Graham, John Cage,\n",
      "Carlos Montoya, Galina Ulanova\n",
      "\n",
      "\n",
      "Cluster: 11\n",
      "-----------\n",
      "Jerome Kern, W C Handy, Billie Holiday, Cole Porter, Coleman Hawkins, Judy\n",
      "Garland, Louis Armstrong, Mahalia Jackson, Stan Kenton, Richard Rodgers,\n",
      "Thelonious Monk, Earl Hines, Muddy Waters, Ethel Merman, Count Basie, Benny\n",
      "Goodman, Miles Davis, Dizzy Gillespie, Gene Kelly, Frank Sinatra\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for k in [7, 11]:\n",
    "    people = vis_data[vis_data['CLUSTER']==k]['LABEL']\n",
    "    people = ', '.join(person for person in people)\n",
    "    people = textwrap.wrap(people, 80)\n",
    "    print(f\"Cluster: {k:>2}\\n-----------\")\n",
    "    for entry in people:\n",
    "        print(entry)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60c62d",
   "metadata": {},
   "source": [
    "Consider further cluster 6, which seems to be about famous scientists.\n",
    "\n",
    "```{margin} What's going on with \"Martian Theory\"?\n",
    "It appears this is actually Percival Lowell, an astronomer (he did, however, advance the idea that Mars is inhabited). Apparently our metadata is a little messy!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c608fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martian Theory\n",
      "Marie Curie\n",
      "Elmer Sperry\n",
      "George E Hale\n",
      "C E M Clung\n",
      "Max Planck\n",
      "A J Dempster\n",
      "Enrico Fermi\n",
      "Ross G Harrison\n",
      "Beno Gutenberg\n",
      "J Robert Oppenheimer\n",
      "Jacques Monod\n",
      "William B Shockley\n",
      "Linus C Pauling\n",
      "Carl Sagan\n"
     ]
    }
   ],
   "source": [
    "for person in vis_data[vis_data['CLUSTER']==6]['LABEL']:\n",
    "    print(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3e967",
   "metadata": {},
   "source": [
    "There are, however, some interestingly noisy clusters, like cluster 13. With people like Queen Victoria and William McKinley in this cluster, it at first appears to be about national leaders of various sorts, but the inclusion of others like Al Capone (the ganster) and Ernie Pyle (a journalist) complicate this. If you take a closer look, what really seems to be tying these obituaries together is war. Nearly everyone here was involved in war in some fashion or another – save for Capone, whose inclusion makes for strange bedfellows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f945e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert E Lee\n",
      "Bedford Forrest\n",
      "Ulysses Grant\n",
      "William McKinley\n",
      "Queen Victoria\n",
      "Geronimo\n",
      "John P Holland\n",
      "Alfred Thayer Mahan\n",
      "Ernie Pyle\n",
      "George Patton\n",
      "Al Capone\n",
      "John Pershing\n",
      "Douglas MacArthur\n",
      "Chester Nimitz\n",
      "Florence Blanchfield\n",
      "The Duke of Windsor\n"
     ]
    }
   ],
   "source": [
    "for person in vis_data[vis_data['CLUSTER']==13]['LABEL']:\n",
    "    print(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c23c1",
   "metadata": {},
   "source": [
    "Depending on your task, these detailed distinctions may not be so desirable. But for us, the document embeddings provide a wonderfully nuanced view of the kinds of people in the obituaries. From here, further exploration might involve focusing on misfits and outliers. Why, for example, is Capone in cluster 13? Or why is Lou Gehrig all by himself in his own cluster? Of course, we could always recluster this data, which would redraw such groupings, but perhaps there is something indeed significant about the way things are divided up as they stand. Word embeddings help bring us to a point where we can begin to undertake such investigations – what comes next depends on which questions we want to ask."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
