{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ae298c",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ff353",
   "metadata": {},
   "source": [
    "Loading the Data\n",
    "--------------------\n",
    "\n",
    "Before we begin working with word embeddings in full, let's load a corpus manifest file, which will help us keep track of all the obituaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff68e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manifest = pd.read_csv('data/session_three/manifest.csv', index_col=0)\n",
    "manifest = manifest.assign(YEAR = pd.to_datetime(manifest['YEAR'], format='%Y').dt.year)\n",
    "\n",
    "manifest.groupby('YEAR').count().plot(\n",
    "    figsize=(15, 5),\n",
    "    y='NAME',\n",
    "    title='Obituaries per Year',\n",
    "    ylabel='Num. obituaries',\n",
    "    xlabel='Year',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7777",
   "metadata": {},
   "source": [
    "Here's a sampling of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18acb156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samuel Beckett (1989)\n",
      "Henry R Luce (1967)\n",
      "Jean Piaget (1980)\n",
      "Adlai Ewing Stevenson (1965)\n",
      "Andrei A Gromyko (1989)\n"
     ]
    }
   ],
   "source": [
    "for idx in manifest.sample(5).index:\n",
    "    name, date = manifest.loc[idx, 'NAME'], manifest.loc[idx, 'YEAR']\n",
    "    print(f\"{name} ({date})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bd292",
   "metadata": {},
   "source": [
    "Now we can load the obituaries themselves. While the past two sessions have required fulltext representations of documents, word embeddings work best with bags of words, especially when it comes to doing analysis with them. Accordingly, each of the files in the corpus have already processed by a text cleaning pipeline: they represent the lowercase, stopped, and lemmatized versions of the originals.\n",
    "\n",
    "No extra loading considerations are needed here either. We'll just use `glob` to get our file paths and iterate through the list, loading each document into a `corpus` list. Note that we still must split the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2575e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob('data/session_three/obits/*.txt')\n",
    "paths.sort()\n",
    "\n",
    "corpus = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as fin:\n",
    "        doc = fin.read()\n",
    "        doc = doc.split()\n",
    "        corpus.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d5031",
   "metadata": {},
   "source": [
    "With this done, we can move on to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01442f",
   "metadata": {},
   "source": [
    "Using an Embeddings Model\n",
    "----------------------------------\n",
    "\n",
    "At this point, we are at a crossroads. On the one hand, we could train a word embeddings model using our corpus documents as is. The `gensim` library offers functionality for this, and it's a relatively easy operation. On the other, we could use premade embeddings, which are usually trained on a more general  and much larger  set of documents. There is a tradeoff here:\n",
    "\n",
    "+ Training a corpus-specific model will more faithfully represent the token behavior of the texts we'd like to analyze, but these representations could be _too_ specific, especially if the model doesn't have enough data to train on\n",
    "+ Using premade embeddings gives us the benefit of generalization: the vectors will cleave more closely to how we understand language; but such embeddings might a) miss out on certain nuances we'd like to capture, or b) introduce biases into our corpus (more on this below)\n",
    "\n",
    "In our case, the decision is difficult. When preparing this reader, we (Tyler and Carl) found that a model trained on the obituaries alone did not produce vectors that could fully demonstrate the capabilities of the word embedding technique. The corpus is just a little too specific, and perhaps a little too small. We could've used a larger corpus, but doing so would introduce slow-downs in the workshop session. Because of this, we decided to use a premade model, in this case, the Stanford [GloVe] embeddings (the 200-dimension version). GloVe was trained on billions of tokens, spanning Wikipedia data, newswire articles, even Twitter. More, the model's developers offer several different dimension sizes, which are helpful for selecting embeddings with the right amount of detail.\n",
    "\n",
    "That said, going with GloVe introduces its own problems. For one thing, we can't show you how to train a word embeddings model itself  at least not live. The code to do so, however, is reproduced below:\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "n_dimensions = 100\n",
    "model = Word2Vec(n_dimensions) # There are several other optional parameters that we won't discuss\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_words=model.corpus_total_words, epochs=5)\n",
    "```\n",
    "\n",
    "Another problem has to do with bias. [Researchers have found] that general embeddings models reproduce gender-discriminatory language, even hate speech, by virtue of the fact that they are trained on huge amounts of text data, often without consideration of whether the content of such data is something one would endorse. GloVe is [known to be biased] in this way. We'll show an example later on in this chapter and will discuss this in much more detail during our live session, but for now just note that the effects of bias _do_ shape how we represent our corpus, and it's important to keep an eye out for this when working with the data.\n",
    "\n",
    "[GloVe]: https://nlp.stanford.edu/projects/glove/\n",
    "[Researchers have found]: https://www.technologyreview.com/2016/07/27/158634/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/\n",
    "[known to be biased]: http://arxiv.org/abs/1607.06520\n",
    "\n",
    "### Loading a model\n",
    "\n",
    "With all that said, we can move on. Below, we load GloVe embeddings into our workspace using a `gensim` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa49bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load('data/session_three/glove/glove-wiki-gigaword_200d.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e8a26",
   "metadata": {},
   "source": [
    "The `KeyedVectors` object acts almost like a dictionary. You can do certain Python operations directly on it, like using `len()` to find the number of tokens in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a4669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the model: 400,000\n"
     ]
    }
   ],
   "source": [
    "n_tokens = len(model)\n",
    "\n",
    "print(f\"Number of unique tokens in the model: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042060a1",
   "metadata": {},
   "source": [
    "### Token mappings\n",
    "\n",
    "Each token in the model (what `gensim` calls a \"key\") has an associated index. This mapping is accessible via the `.key_to_index` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82fb46d",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " 's': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843d4e5",
   "metadata": {},
   "source": [
    "If you want to get the vector representation for a token, you can use either the key or the index. The syntax is just like a Python `dict`. Below, we randomly select a single token from the model vocabulary's `.index_to_key` attribute and find the index associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b78a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index position for 'gokongwei' is 144989\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_token = random.choice(model.index_to_key)\n",
    "rand_idx = model.key_to_index[rand_token]\n",
    "\n",
    "print(f\"The index position for '{rand_token}' is {rand_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3faa4d",
   "metadata": {},
   "source": [
    "Here's its vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29dd6a22",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.5794e-01,  6.8779e-02,  7.7258e-02,  3.2368e-01,  1.1404e-01,\n",
       "        5.4577e-01,  7.3728e-01, -1.5120e-01, -4.2868e-01, -3.4521e-01,\n",
       "       -1.4292e-01, -9.5519e-01,  1.6060e-01, -2.4930e-01,  2.6067e-01,\n",
       "       -7.5981e-01,  2.3941e-01, -2.5972e-01, -2.3580e-01, -2.1307e-01,\n",
       "        1.4162e-01, -2.6145e-01,  9.0988e-02,  6.9848e-01,  2.0964e-01,\n",
       "       -2.8992e-02, -4.3161e-01,  2.9859e-01,  2.7377e-01, -4.8380e-01,\n",
       "        3.3074e-01,  1.4585e+00, -2.3323e-01, -2.1715e-01,  6.1824e-01,\n",
       "        6.5895e-01,  3.7640e-01,  7.0086e-01, -4.0577e-01, -5.9067e-01,\n",
       "       -4.9738e-01, -2.0935e-01, -1.4962e-01, -1.6226e-01, -6.7409e-01,\n",
       "       -1.0365e-01,  6.8456e-01,  3.3329e-01, -1.1712e-01, -4.1906e-01,\n",
       "        6.5250e-01,  4.0595e-01, -1.9849e-02, -9.4312e-01,  8.8016e-01,\n",
       "        7.3255e-01, -2.7395e-01,  9.6984e-03, -1.7534e-01, -5.4192e-01,\n",
       "        1.1536e+00,  2.8626e-01, -1.8013e-02,  4.1065e-01,  5.9536e-02,\n",
       "       -1.9131e-01,  4.0937e-01, -3.3246e-01, -5.4713e-02, -3.5759e-01,\n",
       "       -2.9997e-01,  5.1089e-01,  1.2396e-03,  1.4023e-01, -4.7628e-01,\n",
       "        2.4173e-01, -1.3660e-01,  1.5260e-02, -1.9875e-01,  5.3036e-01,\n",
       "        3.5228e-01,  1.3539e-01, -4.0379e-01, -6.7903e-01,  3.1690e-01,\n",
       "        5.8090e-01,  1.3807e-01,  5.0440e-01, -7.9035e-01,  8.5586e-02,\n",
       "       -4.1225e-02,  2.2773e-01,  2.8452e-02,  1.3309e-01, -2.4679e-01,\n",
       "        1.5191e-01,  4.3317e-01,  2.5018e-01,  7.3878e-03, -1.5690e-01,\n",
       "       -2.2783e-02,  2.7697e-01, -6.5056e-01, -1.9315e-01, -1.1945e-01,\n",
       "        1.5932e-01, -2.7983e-01, -7.1874e-01,  5.1263e-01,  3.1831e-01,\n",
       "        3.3992e-01, -7.6619e-01, -9.2430e-01, -9.2929e-02, -1.9506e-02,\n",
       "        1.5473e-01, -8.7091e-02,  2.0014e-01,  2.7536e-01, -1.8141e-01,\n",
       "        3.9932e-03,  2.4079e-01, -3.5055e-01, -2.0719e-02,  2.9924e-01,\n",
       "        1.6579e-01,  3.2149e-01, -2.0365e-01, -6.1284e-03,  7.2363e-01,\n",
       "        1.2343e-01, -3.2989e-01, -4.6532e-01,  4.1432e-01, -3.1287e-01,\n",
       "        1.6620e-01,  6.1713e-01,  8.5404e-01, -2.4054e-01,  4.7637e-01,\n",
       "        3.1763e-01,  3.0020e-01,  5.5537e-01,  2.1862e-01, -6.0856e-01,\n",
       "       -9.3278e-02, -5.5426e-01, -1.1844e-01, -3.1668e-01, -3.5787e-01,\n",
       "       -8.1700e-01, -6.3881e-01, -9.9093e-02, -2.7950e-01, -4.0629e-01,\n",
       "       -5.6969e-01,  8.6510e-01, -2.4984e-01, -8.9063e-01,  3.6295e-01,\n",
       "        1.8218e-01,  7.1905e-01, -1.3272e-01,  9.8870e-02, -2.0006e-01,\n",
       "       -2.2847e-01, -5.4243e-01, -4.6526e-02, -5.2964e-01, -5.7771e-01,\n",
       "        7.5096e-01,  4.4845e-01, -4.8447e-01, -2.2559e-01,  1.6020e-01,\n",
       "       -5.0548e-01, -3.3572e-01,  3.0256e-01, -2.2786e-01, -3.1915e-01,\n",
       "       -7.6104e-01,  2.3496e-01, -3.7990e-01, -7.0985e-01,  1.1126e-01,\n",
       "        1.5660e-01, -3.4369e-01, -5.3252e-01,  2.2199e-01, -6.9137e-02,\n",
       "       -5.9068e-01, -6.1044e-01, -1.5847e-01,  6.9934e-02, -6.1825e-01,\n",
       "        1.6664e-01,  4.3240e-01,  1.0666e-02,  3.3181e-01, -3.9575e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3732b60",
   "metadata": {},
   "source": [
    "And here we show that accessing this vector with either the index or key produces the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b87abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array_equal(model[rand_idx], model[rand_token]) is True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9c9b7",
   "metadata": {},
   "source": [
    "Finally, we can store the entire model vocabulary in a `set` and show a few examples of the tokens therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50e56cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inami\n",
      "doing\n",
      "ra\n",
      "crabgrass\n",
      "zenner\n",
      "extremist\n",
      "goodloe\n",
      "subtleties\n",
      "xiaojin\n",
      "ajedrez\n"
     ]
    }
   ],
   "source": [
    "model_vocab = set(model.index_to_key)\n",
    "\n",
    "for token in random.sample(model_vocab, 10):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4de567",
   "metadata": {},
   "source": [
    "You may find some unexpected tokens in this output. Though it has been ostensibly trained on an English corpus, GloVe contains multilingual text. It also contains lots of noisy tokens, which range from erroneous segmentations (\"drummer/percussionist\" is one token, for example) to password-like strings and even HTML markup. Depending on your task, you may not notice these tokens, but they do in fact influence the overall shape of the model, and sometimes you'll find them cropping up when you're hunting around for similar terms and the like (more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae401f",
   "metadata": {},
   "source": [
    "### Out-of-vocabulary tokens\n",
    "\n",
    "While GloVe's vocabulary sometimes seems _too_ expansive, there are other instances where it's too restricted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4abca184",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not in vocabulary!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_99428/744621433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'unshaped'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not in vocabulary!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Not in vocabulary!"
     ]
    }
   ],
   "source": [
    "assert 'unshaped' in model, \"Not in vocabulary!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c8da8",
   "metadata": {},
   "source": [
    "If the model wasn't trained on a particular word, it won't have a corresponding vector for that word either. This is crucial. Because models like GloVe only know what they've been trained on, you need to be aware of any potential discrepancies between their vocabularies and your corpus data. If you don't keep this in mind, sending unseen, or **out-of-vocabulary** tokens to GloVe will throw errors in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9210b8a9",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'unshaped' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_99428/3553718512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unshaped'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'unshaped' not present\""
     ]
    }
   ],
   "source": [
    "model['unshaped']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd017ccd",
   "metadata": {},
   "source": [
    "There are a few ways to handle this problem. The most common is to simply _not encode_ tokens in your corpus that don't have a corresponding vector in GloVe. Below, we construct three dictionaries for our corpus data. The first contains all tokens, while the second and third are comprised of tokens that are and are not in Glove, respectively. We identify whether the model has a token using its `.has_index_for()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92e2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the corpus vocabulary: 29,330 \n",
      "Number of corpus words in GloVe: 27,488 \n",
      "Number of corpus words not in GloVe: 1,842\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab = set(token for doc in corpus for token in doc)\n",
    "in_glove = set(token for token in corpus_vocab if model.has_index_for(token))\n",
    "no_glove = set(token for token in corpus_vocab if model.has_index_for(token) == False)\n",
    "\n",
    "print(\n",
    "    f\"Total words in the corpus vocabulary: {len(corpus_vocab):,}\",\n",
    "    f\"\\nNumber of corpus words in GloVe: {len(in_glove):,}\",\n",
    "    f\"\\nNumber of corpus words not in GloVe: {len(no_glove):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac92b5f",
   "metadata": {},
   "source": [
    "Any subsequent code we write will need to reference these dictionaries to determine whether it should encode a token.\n",
    "\n",
    "While this is what we'll indeed do below, obviously it isn't an ideal situation. But it's one of the consequences of using premade models. There are, however, a few other ways to handle out-of-vocabulary terms. Some models offer special \"UNK\" tokens, which you could associate with all of your problem tokens. This, at the very least, enables you to have _some_ representation of your data. A more complex approach involves taking the mean embedding of the word vectors surrounding an unknown token; and depending on the model, you can also train it further, adding extra tokens from your domain-specific text. Instructions for this last option are available [here] in the `gensim` documentation.\n",
    "\n",
    "[here]: https://radimrehurek.com/gensim/models/word2vec.html#usage-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268817e9",
   "metadata": {},
   "source": [
    "Word relationships\n",
    "----------------------\n",
    "\n",
    "Later on we'll use GloVe to encode our corpus texts. But before we do, it's worth demonstrating more generally some of the properties of word vectors. Vector representations of text allow us to perform various mathematical operations on our corpus that approximate (though maybe _only_ approximate) semantics. The most common among these operations is finding the **cosine similarity** between two vectors. Our Getting Started with Textual Data series has a whole [chapter] on this measure, so if you haven't encountered it before, we recommend you read that. But in short: cosine similarity measures the difference between vectors' orientation in a feature space; the closer two vectors are, the more likely they are to share semantic similarities.\n",
    "\n",
    "[chapter]: https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/05_clustering-and-classification.html#\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "`gensim` provides easy access to this measure and other such vector space operations, and we can use this functionality to explore relationships between words in a model. To find the cosine similarity between the vectors for two words in GloVe, simply use the model's `.similarity()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c30643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for 'calculate' and 'compute': 0.6991\n"
     ]
    }
   ],
   "source": [
    "a, b = 'calculate', 'compute'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for '{a}' and '{b}': {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bc5ae",
   "metadata": {},
   "source": [
    "The only difference between the score above and the one that you might produce, say, with `scikit-learn`'s cosine similarity implementation is that `gensim` bounds its values from `[-1,1]`, whereas the latter uses a `[0,1]` scale. While in `gensim` it's still the case that similar words score closer to `1`, highly dissimilar words will be closer to `-1`.\n",
    "\n",
    "At any rate, we can get the top _n_ most similar words for a word using `.most_similar()` (it defaults to 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412f067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'simplification':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplifying</td>\n",
       "      <td>0.622846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harmonization</td>\n",
       "      <td>0.579240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simplify</td>\n",
       "      <td>0.563626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simplified</td>\n",
       "      <td>0.561692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>streamlining</td>\n",
       "      <td>0.554468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>refinement</td>\n",
       "      <td>0.548576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>harmonisation</td>\n",
       "      <td>0.544845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>decentralization</td>\n",
       "      <td>0.523385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rationalization</td>\n",
       "      <td>0.502782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>simplifies</td>\n",
       "      <td>0.496589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               WORD     SCORE\n",
       "0       simplifying  0.622846\n",
       "1     harmonization  0.579240\n",
       "2          simplify  0.563626\n",
       "3        simplified  0.561692\n",
       "4      streamlining  0.554468\n",
       "5        refinement  0.548576\n",
       "6     harmonisation  0.544845\n",
       "7  decentralization  0.523385\n",
       "8   rationalization  0.502782\n",
       "9        simplifies  0.496589"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'racetrack':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woodbine</td>\n",
       "      <td>0.659025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>racetracks</td>\n",
       "      <td>0.596440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aqueduct</td>\n",
       "      <td>0.578504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>belmont</td>\n",
       "      <td>0.566376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thoroughbred</td>\n",
       "      <td>0.556048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fairgrounds</td>\n",
       "      <td>0.549623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meadowlands</td>\n",
       "      <td>0.526899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longacres</td>\n",
       "      <td>0.515037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saratoga</td>\n",
       "      <td>0.511240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turf</td>\n",
       "      <td>0.508318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0      woodbine  0.659025\n",
       "1    racetracks  0.596440\n",
       "2      aqueduct  0.578504\n",
       "3       belmont  0.566376\n",
       "4  thoroughbred  0.556048\n",
       "5   fairgrounds  0.549623\n",
       "6   meadowlands  0.526899\n",
       "7     longacres  0.515037\n",
       "8      saratoga  0.511240\n",
       "9          turf  0.508318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'ribbentrop':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-aggression</td>\n",
       "      <td>0.556521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>molotov-ribbentrop</td>\n",
       "      <td>0.534185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>molotov</td>\n",
       "      <td>0.515208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joachim</td>\n",
       "      <td>0.474670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goebbels</td>\n",
       "      <td>0.434550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hitler</td>\n",
       "      <td>0.433823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sandrart</td>\n",
       "      <td>0.424629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>beria</td>\n",
       "      <td>0.422216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>friedeburg</td>\n",
       "      <td>0.413931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lettow-vorbeck</td>\n",
       "      <td>0.412711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 WORD     SCORE\n",
       "0      non-aggression  0.556521\n",
       "1  molotov-ribbentrop  0.534185\n",
       "2             molotov  0.515208\n",
       "3             joachim  0.474670\n",
       "4            goebbels  0.434550\n",
       "5              hitler  0.433823\n",
       "6            sandrart  0.424629\n",
       "7               beria  0.422216\n",
       "8          friedeburg  0.413931\n",
       "9      lettow-vorbeck  0.412711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'presumption':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rebuttable</td>\n",
       "      <td>0.709488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>innocence</td>\n",
       "      <td>0.612634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guilt</td>\n",
       "      <td>0.577075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preponderance</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fairness</td>\n",
       "      <td>0.517174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hearsay</td>\n",
       "      <td>0.512928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pretense</td>\n",
       "      <td>0.503593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>contrary</td>\n",
       "      <td>0.499797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>presumptions</td>\n",
       "      <td>0.493690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unfairness</td>\n",
       "      <td>0.491527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WORD     SCORE\n",
       "0     rebuttable  0.709488\n",
       "1      innocence  0.612634\n",
       "2          guilt  0.577075\n",
       "3  preponderance  0.541000\n",
       "4       fairness  0.517174\n",
       "5        hearsay  0.512928\n",
       "6       pretense  0.503593\n",
       "7       contrary  0.499797\n",
       "8   presumptions  0.493690\n",
       "9     unfairness  0.491527"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'foothill':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foothills</td>\n",
       "      <td>0.496078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calabasas</td>\n",
       "      <td>0.484296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agoura</td>\n",
       "      <td>0.450929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yellow-legged</td>\n",
       "      <td>0.450821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sylmar</td>\n",
       "      <td>0.448964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>woodland</td>\n",
       "      <td>0.442699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bernardino</td>\n",
       "      <td>0.441338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>freeway</td>\n",
       "      <td>0.432781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chaparral</td>\n",
       "      <td>0.430598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i-210</td>\n",
       "      <td>0.429958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WORD     SCORE\n",
       "0      foothills  0.496078\n",
       "1      calabasas  0.484296\n",
       "2         agoura  0.450929\n",
       "3  yellow-legged  0.450821\n",
       "4         sylmar  0.448964\n",
       "5       woodland  0.442699\n",
       "6     bernardino  0.441338\n",
       "7        freeway  0.432781\n",
       "8      chaparral  0.430598\n",
       "9          i-210  0.429958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = random.sample(in_glove, 5)\n",
    "\n",
    "for token in targets:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3983f5",
   "metadata": {},
   "source": [
    "We can also find the _least_ similar word. This is useful to show, because it pressures our idea of what counts as similarity. Mathematical similarity does not always align with concepts like synonyms and antonyms. For example, it's probably safe to say that the semantic opposite of \"good\"  that is, its antonym  is \"evil.\" But in the world of vector spaces, the least similar word to \"good\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ddb5788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cw96', -0.6553234457969666)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good', topn=len(model))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afacde",
   "metadata": {},
   "source": [
    "Just noise! Relatively speaking, \"good\" and \"evil\" are actually quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfd03710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for good and evil: 0.3378\n"
     ]
    }
   ],
   "source": [
    "a, b = 'good', 'evil'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for {a} and {b}: {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ef20e",
   "metadata": {},
   "source": [
    "How do we make sense of this? Well, it has to do with the way the word embeddings are created. Since embeddings models are ultimately trained on co-occurrence data, words that tend to appear in similar kinds of contexts will be more similar in a mathematical sense than those that don't.\n",
    "\n",
    "Keeping this in mind is also important for considerations of bias. Since, in one sense, embeddings reflect the interchangeability between tokens, they will reinforce negative, even harmful patterns in the data (which is to say in culture at large). For example, consider the most similar words for \"doctor\" and \"nurse.\" The latter is locked up within gendered language: a nurse is like a midwife is like a mother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831246c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'doctor':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.736021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.672406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surgeon</td>\n",
       "      <td>0.655147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dr.</td>\n",
       "      <td>0.652498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.648189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.636380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.619159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.584747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>psychiatrist</td>\n",
       "      <td>0.568571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0     physician  0.736021\n",
       "1       doctors  0.672406\n",
       "2       surgeon  0.655147\n",
       "3           dr.  0.652498\n",
       "4         nurse  0.651449\n",
       "5       medical  0.648189\n",
       "6      hospital  0.636380\n",
       "7       patient  0.619159\n",
       "8       dentist  0.584747\n",
       "9  psychiatrist  0.568571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'nurse':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurses</td>\n",
       "      <td>0.714051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.626937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midwife</td>\n",
       "      <td>0.614592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anesthetist</td>\n",
       "      <td>0.610603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.610359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.609222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.586503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>therapist</td>\n",
       "      <td>0.580488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.573556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0       nurses  0.714051\n",
       "1       doctor  0.651449\n",
       "2      nursing  0.626937\n",
       "3      midwife  0.614592\n",
       "4  anesthetist  0.610603\n",
       "5    physician  0.610359\n",
       "6     hospital  0.609222\n",
       "7       mother  0.586503\n",
       "8    therapist  0.580488\n",
       "9      dentist  0.573556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bias_example = ['doctor', 'nurse']\n",
    "\n",
    "for token in bias_example:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f765b0",
   "metadata": {},
   "source": [
    "### Visualizing the vector space\n",
    "\n",
    "One way to start getting a feel for all this is to visualize the word vectors. We do so below by sampling a portion of the GloVe vectors and then reducing them into two-dimensional data, which we can plot.\n",
    "\n",
    "```{margin} How we create the visualization data\n",
    "`sample_embeddings()` takes a sample from GloVe:\n",
    "\n",
    "1. First it randomly selects indices in the model\n",
    "2. Then it uses these to subset the vectors\n",
    "3. Finally it associates the tokens with their respective indices to produce a set of labels\n",
    "\n",
    "`prepare_vis_data()` takes the sampled vectors and their labels and reduces them with a t-SNE embedder\n",
    "\n",
    "1. The `TSNE()` portion of the code does the work of reducing our 200-dimension vectors into only two dimensions\n",
    "2. Then the function converts the two-dimensional data into a dataframe and associates the labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "590ae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def sample_embeddings(vectors, samp=1000):\n",
    "    n_vectors = vectors.shape[0]\n",
    "    mask = random.sample(range(n_vectors), samp)\n",
    "    vectors = vectors[mask]\n",
    "    vocab = [model.index_to_key[idx] for idx in mask]\n",
    "    \n",
    "    return vectors, vocab\n",
    "\n",
    "def prepare_vis_data(vectors, labels):\n",
    "    reduced = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate='auto',\n",
    "        init='random',\n",
    "        angle=0.65,\n",
    "        random_state=357\n",
    "    ).fit_transform(vectors)\n",
    "    \n",
    "    vis_data = pd.DataFrame(reduced, columns=['X', 'Y'])\n",
    "    vis_data['TOKEN'] = labels\n",
    "    \n",
    "    return vis_data\n",
    "\n",
    "all_vectors = np.array([model[idx] for idx in model.key_to_index])\n",
    "sampled, sampled_vocab = sample_embeddings(all_vectors)\n",
    "vis_data = prepare_vis_data(sampled, sampled_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f25012",
   "metadata": {},
   "source": [
    "With the reduced embeddings made, it's time to plot them. Have a look around at the results. What seems right to you? What surprises you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18195af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-16fa920ff906415f9b70b894acc6e7ec\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-16fa920ff906415f9b70b894acc6e7ec\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-16fa920ff906415f9b70b894acc6e7ec\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-bd5a237178f7c4a7b9e8717319769d43\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"TOKEN\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-bd5a237178f7c4a7b9e8717319769d43\": [{\"X\": -0.11895104497671127, \"Y\": 6.045454978942871, \"TOKEN\": \"durant\"}, {\"X\": -3.5291690826416016, \"Y\": 3.6193957328796387, \"TOKEN\": \"sundaram\"}, {\"X\": 5.484399318695068, \"Y\": 6.7568206787109375, \"TOKEN\": \"mohk\"}, {\"X\": 6.080658912658691, \"Y\": 0.9978627562522888, \"TOKEN\": \"bo'ao\"}, {\"X\": -3.58855938911438, \"Y\": 5.413094520568848, \"TOKEN\": \"tackett\"}, {\"X\": -0.8355161547660828, \"Y\": 2.378265380859375, \"TOKEN\": \"murman\"}, {\"X\": 1.5463529825210571, \"Y\": 1.679119348526001, \"TOKEN\": \"beilschmiedia\"}, {\"X\": 0.7619501352310181, \"Y\": 2.850219964981079, \"TOKEN\": \"ksenija\"}, {\"X\": -1.2985320091247559, \"Y\": 2.434476137161255, \"TOKEN\": \"orduna\"}, {\"X\": 0.9276367425918579, \"Y\": -1.690956950187683, \"TOKEN\": \"ritualist\"}, {\"X\": 4.350553035736084, \"Y\": -1.9001832008361816, \"TOKEN\": \"mitchels\"}, {\"X\": 4.626021862030029, \"Y\": -0.2466546893119812, \"TOKEN\": \"hayatabad\"}, {\"X\": 4.155093193054199, \"Y\": -0.14407067000865936, \"TOKEN\": \"rumonge\"}, {\"X\": 2.672400712966919, \"Y\": 2.8784406185150146, \"TOKEN\": \"e-act\"}, {\"X\": -5.711997985839844, \"Y\": -3.637402057647705, \"TOKEN\": \"98.68\"}, {\"X\": 1.9780062437057495, \"Y\": 3.6246466636657715, \"TOKEN\": \"topdown\"}, {\"X\": -0.3359118700027466, \"Y\": -3.175471305847168, \"TOKEN\": \"osteopenia\"}, {\"X\": -1.6672040224075317, \"Y\": -0.9098203778266907, \"TOKEN\": \"nukem\"}, {\"X\": -0.24953733384609222, \"Y\": 2.3764774799346924, \"TOKEN\": \"yener\"}, {\"X\": -3.880373239517212, \"Y\": -7.042881965637207, \"TOKEN\": \"26-year\"}, {\"X\": 2.026703357696533, \"Y\": -2.017371416091919, \"TOKEN\": \"ollywood\"}, {\"X\": 2.4086763858795166, \"Y\": -3.19150447845459, \"TOKEN\": \"well-travelled\"}, {\"X\": 1.6009697914123535, \"Y\": 0.8899134993553162, \"TOKEN\": \"nasionale\"}, {\"X\": 1.9395486116409302, \"Y\": 4.638952255249023, \"TOKEN\": \"krasenkow\"}, {\"X\": 6.006889820098877, \"Y\": 0.32046765089035034, \"TOKEN\": \"jilin\"}, {\"X\": 3.455740213394165, \"Y\": 1.765151858329773, \"TOKEN\": \"pronto\"}, {\"X\": -4.073489189147949, \"Y\": -4.519307613372803, \"TOKEN\": \"1,234\"}, {\"X\": -0.8301144242286682, \"Y\": -4.379738807678223, \"TOKEN\": \"impaneled\"}, {\"X\": -4.141929626464844, \"Y\": -4.54295539855957, \"TOKEN\": \"1,680\"}, {\"X\": 2.540175676345825, \"Y\": 2.176332950592041, \"TOKEN\": \"fallersleben\"}, {\"X\": 4.57084321975708, \"Y\": -5.084754467010498, \"TOKEN\": \"rebroadcast\"}, {\"X\": -0.446696937084198, \"Y\": 1.8961458206176758, \"TOKEN\": \"kanaya\"}, {\"X\": 4.948108673095703, \"Y\": -0.22391460835933685, \"TOKEN\": \"heves\"}, {\"X\": 3.023740530014038, \"Y\": -4.3477325439453125, \"TOKEN\": \"ultimatums\"}, {\"X\": -0.9034287333488464, \"Y\": 2.8711037635803223, \"TOKEN\": \"villafane\"}, {\"X\": 0.1299525797367096, \"Y\": 3.0674610137939453, \"TOKEN\": \"stokke\"}, {\"X\": 4.977846145629883, \"Y\": 0.8877295851707458, \"TOKEN\": \"aerotropolis\"}, {\"X\": 3.8625681400299072, \"Y\": 1.1202912330627441, \"TOKEN\": \"digha\"}, {\"X\": 3.6736948490142822, \"Y\": 0.407203733921051, \"TOKEN\": \"ribeira\"}, {\"X\": -3.952073574066162, \"Y\": 0.5627608895301819, \"TOKEN\": \"re2\"}, {\"X\": -1.2141886949539185, \"Y\": -4.372848987579346, \"TOKEN\": \"ascertained\"}, {\"X\": -3.591639995574951, \"Y\": -0.297953337430954, \"TOKEN\": \"lld\"}, {\"X\": 2.3559951782226562, \"Y\": 3.2650692462921143, \"TOKEN\": \"15x\"}, {\"X\": 0.7863897681236267, \"Y\": -0.722449779510498, \"TOKEN\": \"ground-state\"}, {\"X\": -0.50650554895401, \"Y\": -1.6684162616729736, \"TOKEN\": \"then-unknown\"}, {\"X\": 1.440163016319275, \"Y\": 2.223958730697632, \"TOKEN\": \"robustus\"}, {\"X\": -0.4705065190792084, \"Y\": 1.497704267501831, \"TOKEN\": \"papadopulos\"}, {\"X\": -4.789638042449951, \"Y\": -2.5094192028045654, \"TOKEN\": \"34.43\"}, {\"X\": 4.516383171081543, \"Y\": 2.1956679821014404, \"TOKEN\": \"h\\u00f6chst\"}, {\"X\": -1.3463165760040283, \"Y\": 2.919473648071289, \"TOKEN\": \"mccallan\"}, {\"X\": 1.3574998378753662, \"Y\": 2.019669532775879, \"TOKEN\": \"raphanus\"}, {\"X\": -6.430027484893799, \"Y\": 4.6964497566223145, \"TOKEN\": \"crecimiento\"}, {\"X\": 3.743861436843872, \"Y\": 5.647861003875732, \"TOKEN\": \"cholangitis\"}, {\"X\": 7.450131416320801, \"Y\": 3.7714667320251465, \"TOKEN\": \"vantagepoint\"}, {\"X\": -1.1288594007492065, \"Y\": -8.923114776611328, \"TOKEN\": \"suggests\"}, {\"X\": -2.2202670574188232, \"Y\": -4.133818626403809, \"TOKEN\": \"salvadorians\"}, {\"X\": 1.6058878898620605, \"Y\": 7.0980048179626465, \"TOKEN\": \"irish-canadian\"}, {\"X\": -0.3590986728668213, \"Y\": -3.9357712268829346, \"TOKEN\": \"bigamy\"}, {\"X\": -3.3148341178894043, \"Y\": -3.009489059448242, \"TOKEN\": \"32,917\"}, {\"X\": -1.9871946573257446, \"Y\": -2.9278390407562256, \"TOKEN\": \"ruder\"}, {\"X\": -4.5012617111206055, \"Y\": -2.450958728790283, \"TOKEN\": \"65.46\"}, {\"X\": -1.836387038230896, \"Y\": 1.4410609006881714, \"TOKEN\": \"hutner\"}, {\"X\": 3.576427459716797, \"Y\": 0.8489468097686768, \"TOKEN\": \"tianshan\"}, {\"X\": -5.16193151473999, \"Y\": -2.6533615589141846, \"TOKEN\": \"45.55\"}, {\"X\": -4.945176124572754, \"Y\": 1.6996033191680908, \"TOKEN\": \"jugovic\"}, {\"X\": 1.4184739589691162, \"Y\": 0.05517401546239853, \"TOKEN\": \"3.3-mile\"}, {\"X\": 3.234647274017334, \"Y\": 6.656835556030273, \"TOKEN\": \"heafy\"}, {\"X\": 3.021456718444824, \"Y\": -1.8382303714752197, \"TOKEN\": \"cosmetologists\"}, {\"X\": -5.749655246734619, \"Y\": 1.3408749103546143, \"TOKEN\": \"helton\"}, {\"X\": 4.67191743850708, \"Y\": 3.256988048553467, \"TOKEN\": \"es300\"}, {\"X\": -4.159642219543457, \"Y\": -9.117549896240234, \"TOKEN\": \"1289\"}, {\"X\": 1.4396332502365112, \"Y\": -1.8983899354934692, \"TOKEN\": \"1980/1981\"}, {\"X\": 0.892703115940094, \"Y\": 4.785688877105713, \"TOKEN\": \"khuzai\"}, {\"X\": 0.18637098371982574, \"Y\": -0.928215503692627, \"TOKEN\": \"floodings\"}, {\"X\": 0.5502164959907532, \"Y\": 6.056127548217773, \"TOKEN\": \"taghavi\"}, {\"X\": -2.178058624267578, \"Y\": 4.287962913513184, \"TOKEN\": \"berndtson\"}, {\"X\": 4.64320182800293, \"Y\": -1.5395327806472778, \"TOKEN\": \"etherow\"}, {\"X\": 4.102268218994141, \"Y\": 1.325479507446289, \"TOKEN\": \"lichterfelde\"}, {\"X\": 3.872666835784912, \"Y\": -3.4093217849731445, \"TOKEN\": \"remittances\"}, {\"X\": -2.691866159439087, \"Y\": -3.530364513397217, \"TOKEN\": \"5,565\"}, {\"X\": -6.181819915771484, \"Y\": -1.882286548614502, \"TOKEN\": \".88\"}, {\"X\": 5.4491400718688965, \"Y\": -3.951087236404419, \"TOKEN\": \"conasupo\"}, {\"X\": 3.1765711307525635, \"Y\": -1.856148362159729, \"TOKEN\": \"wc2003-rsa\"}, {\"X\": -7.250322341918945, \"Y\": -3.3161394596099854, \"TOKEN\": \"52-0\"}, {\"X\": -0.374247670173645, \"Y\": -4.070150375366211, \"TOKEN\": \"solicitation\"}, {\"X\": 4.477075576782227, \"Y\": -0.6441580653190613, \"TOKEN\": \"sumter\"}, {\"X\": 0.7678576707839966, \"Y\": -2.4194931983947754, \"TOKEN\": \"realizations\"}, {\"X\": 1.2769124507904053, \"Y\": -3.791569709777832, \"TOKEN\": \"machination\"}, {\"X\": 0.3099864423274994, \"Y\": -5.582823276519775, \"TOKEN\": \"artfully\"}, {\"X\": -2.354962110519409, \"Y\": 1.1592319011688232, \"TOKEN\": \"turmel\"}, {\"X\": 3.944352149963379, \"Y\": 0.03935009986162186, \"TOKEN\": \"bahawalnagar\"}, {\"X\": -3.0827159881591797, \"Y\": 4.2837815284729, \"TOKEN\": \"chilufya\"}, {\"X\": -5.813993453979492, \"Y\": 2.5156662464141846, \"TOKEN\": \"celis\"}, {\"X\": 3.4029934406280518, \"Y\": 0.661145031452179, \"TOKEN\": \"a180\"}, {\"X\": 1.0802850723266602, \"Y\": 4.019589900970459, \"TOKEN\": \"laar\"}, {\"X\": -5.106714248657227, \"Y\": -6.9261627197265625, \"TOKEN\": \"thirty-five\"}, {\"X\": -0.10555291175842285, \"Y\": 2.0728092193603516, \"TOKEN\": \"eljero\"}, {\"X\": -1.9382961988449097, \"Y\": -9.181614875793457, \"TOKEN\": \"helicopter\"}, {\"X\": -2.906317710876465, \"Y\": 0.9623902440071106, \"TOKEN\": \"adot\"}, {\"X\": 4.038043022155762, \"Y\": 2.764848232269287, \"TOKEN\": \"gencher\"}, {\"X\": 3.392139434814453, \"Y\": 0.9072710275650024, \"TOKEN\": \"city/22\"}, {\"X\": 2.497992753982544, \"Y\": -0.6626247763633728, \"TOKEN\": \"capi\"}, {\"X\": -0.26020094752311707, \"Y\": -0.5340131521224976, \"TOKEN\": \"hunas\"}, {\"X\": -2.976655960083008, \"Y\": -0.31454384326934814, \"TOKEN\": \"lionhead\"}, {\"X\": 2.3648011684417725, \"Y\": -1.4225893020629883, \"TOKEN\": \"dailykos\"}, {\"X\": 2.33349347114563, \"Y\": 7.132610321044922, \"TOKEN\": \"powel\"}, {\"X\": 1.886603593826294, \"Y\": 1.7105001211166382, \"TOKEN\": \"keluarga\"}, {\"X\": 0.444138765335083, \"Y\": -0.8414486646652222, \"TOKEN\": \"hsps\"}, {\"X\": -4.185897350311279, \"Y\": -9.110316276550293, \"TOKEN\": \"1167\"}, {\"X\": 5.732806205749512, \"Y\": -0.03196104243397713, \"TOKEN\": \"zhuzhou\"}, {\"X\": 2.4743707180023193, \"Y\": 3.941458225250244, \"TOKEN\": \"maraga\"}, {\"X\": 4.229452610015869, \"Y\": -1.0082319974899292, \"TOKEN\": \"lakhish\"}, {\"X\": -3.360625743865967, \"Y\": 2.1702914237976074, \"TOKEN\": \"arora\"}, {\"X\": -1.5271327495574951, \"Y\": -7.51652193069458, \"TOKEN\": \"esteemed\"}, {\"X\": -7.219956398010254, \"Y\": 2.6746909618377686, \"TOKEN\": \"\\u0db8\"}, {\"X\": 1.8819419145584106, \"Y\": -0.832044780254364, \"TOKEN\": \"jls\"}, {\"X\": 7.553920745849609, \"Y\": -2.9233665466308594, \"TOKEN\": \"kumeyaay\"}, {\"X\": 0.26915889978408813, \"Y\": -4.750962257385254, \"TOKEN\": \"rightist\"}, {\"X\": 4.6624755859375, \"Y\": 1.450608253479004, \"TOKEN\": \"troisdorf\"}, {\"X\": -2.3813743591308594, \"Y\": -3.434457778930664, \"TOKEN\": \"4400\"}, {\"X\": -1.330790400505066, \"Y\": 2.372467279434204, \"TOKEN\": \"schodt\"}, {\"X\": -2.656764268875122, \"Y\": -8.523124694824219, \"TOKEN\": \"mann\"}, {\"X\": 1.3480151891708374, \"Y\": 3.605506181716919, \"TOKEN\": \"miniver\"}, {\"X\": -3.23344349861145, \"Y\": 2.4584171772003174, \"TOKEN\": \"vicelich\"}, {\"X\": 0.2727673351764679, \"Y\": -2.4073095321655273, \"TOKEN\": \"wargaming\"}, {\"X\": -2.0169432163238525, \"Y\": -4.231354236602783, \"TOKEN\": \"apprehensive\"}, {\"X\": 1.2549190521240234, \"Y\": -4.6108832359313965, \"TOKEN\": \"airbrush\"}, {\"X\": 1.7099756002426147, \"Y\": -0.3342243731021881, \"TOKEN\": \"bakwena\"}, {\"X\": -4.6016926765441895, \"Y\": 0.8382647037506104, \"TOKEN\": \"absentions\"}, {\"X\": -3.41052508354187, \"Y\": 0.9741301536560059, \"TOKEN\": \"vilayat\"}, {\"X\": -1.3839335441589355, \"Y\": -1.2095515727996826, \"TOKEN\": \"xiongnu\"}, {\"X\": -1.8536595106124878, \"Y\": 6.0918192863464355, \"TOKEN\": \"phoebe\"}, {\"X\": -0.20095251500606537, \"Y\": 3.576618194580078, \"TOKEN\": \"zakri\"}, {\"X\": -0.49293288588523865, \"Y\": 3.6326587200164795, \"TOKEN\": \"ritvo\"}, {\"X\": -3.279287338256836, \"Y\": -2.0377283096313477, \"TOKEN\": \"counterproposal\"}, {\"X\": 4.557764053344727, \"Y\": -4.005543231964111, \"TOKEN\": \"wasbir\"}, {\"X\": 1.0719367265701294, \"Y\": -3.2196433544158936, \"TOKEN\": \"cutover\"}, {\"X\": 3.385880708694458, \"Y\": -1.1821693181991577, \"TOKEN\": \"adiabene\"}, {\"X\": 0.9076757431030273, \"Y\": 3.446303129196167, \"TOKEN\": \"cheri\"}, {\"X\": 5.92351770401001, \"Y\": 1.683191180229187, \"TOKEN\": \"tsotsin\"}, {\"X\": -7.378539562225342, \"Y\": 2.6359496116638184, \"TOKEN\": \"http://www.twitter.com\"}, {\"X\": 4.597172260284424, \"Y\": 1.593641996383667, \"TOKEN\": \"tonnelle\"}, {\"X\": -2.3556764125823975, \"Y\": 3.8814749717712402, \"TOKEN\": \"goonetilleke\"}, {\"X\": -2.42231822013855, \"Y\": -3.359685182571411, \"TOKEN\": \"3,633\"}, {\"X\": 3.824547529220581, \"Y\": -3.9441609382629395, \"TOKEN\": \"polecat\"}, {\"X\": -5.298802375793457, \"Y\": 1.9446996450424194, \"TOKEN\": \"deaker\"}, {\"X\": 5.950491905212402, \"Y\": -3.2104501724243164, \"TOKEN\": \"aud\\u00e9\"}, {\"X\": 3.3800063133239746, \"Y\": -0.019184647127985954, \"TOKEN\": \"comala\"}, {\"X\": 2.0277466773986816, \"Y\": 1.732783317565918, \"TOKEN\": \"wytw\\u00f3rnia\"}, {\"X\": -0.728157639503479, \"Y\": 3.5302209854125977, \"TOKEN\": \"meral\"}, {\"X\": 0.1779002547264099, \"Y\": 2.748889684677124, \"TOKEN\": \"ionita\"}, {\"X\": 3.2183585166931152, \"Y\": 5.820489883422852, \"TOKEN\": \"powlus\"}, {\"X\": -1.5838470458984375, \"Y\": -9.04137134552002, \"TOKEN\": \"involved\"}, {\"X\": -1.191792368888855, \"Y\": 2.5581276416778564, \"TOKEN\": \"bohlmann\"}, {\"X\": 0.2030971497297287, \"Y\": -0.7865172028541565, \"TOKEN\": \"oligosaccharides\"}, {\"X\": 1.409325361251831, \"Y\": -4.630308151245117, \"TOKEN\": \"rykiel\"}, {\"X\": 4.746158599853516, \"Y\": 4.901268005371094, \"TOKEN\": \"interactivecorp\"}, {\"X\": -1.2829009294509888, \"Y\": -5.891459941864014, \"TOKEN\": \"reacquire\"}, {\"X\": 2.862626552581787, \"Y\": 0.407999187707901, \"TOKEN\": \"pooram\"}, {\"X\": -6.171298980712891, \"Y\": -0.6488455533981323, \"TOKEN\": \"jojohnson\"}, {\"X\": -6.697997093200684, \"Y\": 1.441027045249939, \"TOKEN\": \"\\u0111\\u00f4n\"}, {\"X\": 5.447312831878662, \"Y\": -3.91129732131958, \"TOKEN\": \"cfp\"}, {\"X\": -4.0838823318481445, \"Y\": -4.491476058959961, \"TOKEN\": \"1,587\"}, {\"X\": -2.725095748901367, \"Y\": -2.18930721282959, \"TOKEN\": \"27-4\"}, {\"X\": -8.609112739562988, \"Y\": -0.7260926365852356, \"TOKEN\": \"0740\"}, {\"X\": -1.5901074409484863, \"Y\": 2.3614704608917236, \"TOKEN\": \"teunis\"}, {\"X\": 1.1455433368682861, \"Y\": -1.8119877576828003, \"TOKEN\": \"seson\"}, {\"X\": 5.8616814613342285, \"Y\": -1.8784698247909546, \"TOKEN\": \"blennerhassett\"}, {\"X\": -5.522304058074951, \"Y\": -2.5633957386016846, \"TOKEN\": \"forint\"}, {\"X\": 0.14634810388088226, \"Y\": -0.7929096221923828, \"TOKEN\": \"1860-1861\"}, {\"X\": 1.9520313739776611, \"Y\": 1.2956833839416504, \"TOKEN\": \"gaulois\"}, {\"X\": 1.2530933618545532, \"Y\": -4.10386848449707, \"TOKEN\": \"legalities\"}, {\"X\": -0.3864002525806427, \"Y\": -9.0521879196167, \"TOKEN\": \"lighter\"}, {\"X\": 0.8575023412704468, \"Y\": -0.7089601755142212, \"TOKEN\": \"ikettes\"}, {\"X\": 1.8119299411773682, \"Y\": 4.201323509216309, \"TOKEN\": \"existant\"}, {\"X\": 1.1892846822738647, \"Y\": -1.0798813104629517, \"TOKEN\": \"im\\u0101ms\"}, {\"X\": 3.823589324951172, \"Y\": 4.052796840667725, \"TOKEN\": \"fonty\"}, {\"X\": 3.3032708168029785, \"Y\": -3.2416937351226807, \"TOKEN\": \"keralite\"}, {\"X\": 0.5862939357757568, \"Y\": 0.5576120615005493, \"TOKEN\": \"2275\"}, {\"X\": 0.6167628169059753, \"Y\": 4.6367411613464355, \"TOKEN\": \"windecker\"}, {\"X\": -3.492281436920166, \"Y\": -3.9182300567626953, \"TOKEN\": \"122,500\"}, {\"X\": -5.299801826477051, \"Y\": -2.652994394302368, \"TOKEN\": \"54.95\"}, {\"X\": 2.8560874462127686, \"Y\": 1.670202612876892, \"TOKEN\": \"norseman\"}, {\"X\": -4.004284381866455, \"Y\": -2.1668195724487305, \"TOKEN\": \"64.69\"}, {\"X\": 1.2066339254379272, \"Y\": 4.165261745452881, \"TOKEN\": \"krasselt\"}, {\"X\": -3.7596096992492676, \"Y\": 2.4090542793273926, \"TOKEN\": \"deupree\"}, {\"X\": -4.781200408935547, \"Y\": -2.5140492916107178, \"TOKEN\": \"46.19\"}, {\"X\": -4.097189903259277, \"Y\": -4.562097072601318, \"TOKEN\": \"162,000\"}, {\"X\": -3.556168794631958, \"Y\": -0.4003411829471588, \"TOKEN\": \"m.th\"}, {\"X\": 5.842758655548096, \"Y\": -1.845564603805542, \"TOKEN\": \"anglesey\"}, {\"X\": 5.684627056121826, \"Y\": -0.3557279407978058, \"TOKEN\": \"kuusalu\"}, {\"X\": 6.641257286071777, \"Y\": -3.9492101669311523, \"TOKEN\": \"lordi\"}, {\"X\": -3.2987473011016846, \"Y\": -8.147955894470215, \"TOKEN\": \"mccauley\"}, {\"X\": 2.840944290161133, \"Y\": -2.8192298412323, \"TOKEN\": \"private-owned\"}, {\"X\": 5.836918354034424, \"Y\": 2.3428027629852295, \"TOKEN\": \"sur\"}, {\"X\": 3.33288836479187, \"Y\": -5.6778459548950195, \"TOKEN\": \"wrt\"}, {\"X\": 0.5863471627235413, \"Y\": 8.055322647094727, \"TOKEN\": \"geewax\"}, {\"X\": 0.19585852324962616, \"Y\": 0.9711520075798035, \"TOKEN\": \"glabrio\"}, {\"X\": -1.1734986305236816, \"Y\": 5.786144256591797, \"TOKEN\": \"beerbaum\"}, {\"X\": -4.630850315093994, \"Y\": -1.206767201423645, \"TOKEN\": \"gev\"}, {\"X\": 3.5478320121765137, \"Y\": -0.06617560237646103, \"TOKEN\": \"tailandia\"}, {\"X\": -4.333964824676514, \"Y\": 1.5122714042663574, \"TOKEN\": \"welham\"}, {\"X\": -2.3448774814605713, \"Y\": 2.972834587097168, \"TOKEN\": \"mey\"}, {\"X\": -0.5309673547744751, \"Y\": 6.283788681030273, \"TOKEN\": \"bild\"}, {\"X\": -1.2378865480422974, \"Y\": 2.538062334060669, \"TOKEN\": \"schornack\"}, {\"X\": 0.04058516398072243, \"Y\": 4.669422626495361, \"TOKEN\": \"lalive\"}, {\"X\": 0.30209922790527344, \"Y\": -4.037438869476318, \"TOKEN\": \"ploddingly\"}, {\"X\": 0.5152921676635742, \"Y\": 4.9941792488098145, \"TOKEN\": \"713-3752\"}, {\"X\": -0.6985568404197693, \"Y\": -8.613425254821777, \"TOKEN\": \"pockets\"}, {\"X\": 1.453913688659668, \"Y\": 1.508331298828125, \"TOKEN\": \"lottia\"}, {\"X\": -3.384652853012085, \"Y\": 7.262294292449951, \"TOKEN\": \"jadunath\"}, {\"X\": 2.45377779006958, \"Y\": 3.3212175369262695, \"TOKEN\": \"gaud\"}, {\"X\": 6.7274250984191895, \"Y\": 3.0923595428466797, \"TOKEN\": \"esenboga\"}, {\"X\": 5.865557670593262, \"Y\": -1.882195234298706, \"TOKEN\": \"ulvi\"}, {\"X\": -1.6514419317245483, \"Y\": -3.338344097137451, \"TOKEN\": \"santas\"}, {\"X\": 5.341333389282227, \"Y\": -0.032251354306936264, \"TOKEN\": \"nusajaya\"}, {\"X\": 3.559262752532959, \"Y\": 0.41712525486946106, \"TOKEN\": \"wansdyke\"}, {\"X\": -5.904440402984619, \"Y\": -1.4299980401992798, \"TOKEN\": \"2.744\"}, {\"X\": -1.3211644887924194, \"Y\": -2.3169546127319336, \"TOKEN\": \"minx\"}, {\"X\": -0.43349558115005493, \"Y\": 0.43985074758529663, \"TOKEN\": \"quenya\"}, {\"X\": 0.6774746775627136, \"Y\": 1.4286119937896729, \"TOKEN\": \"zhengyi\"}, {\"X\": -2.2666780948638916, \"Y\": 1.0733367204666138, \"TOKEN\": \"rootham\"}, {\"X\": 3.3487565517425537, \"Y\": 0.9882307648658752, \"TOKEN\": \"tollhouse\"}, {\"X\": -1.0317965745925903, \"Y\": 1.5343663692474365, \"TOKEN\": \"asimco\"}, {\"X\": 4.939158916473389, \"Y\": 3.945302963256836, \"TOKEN\": \"sulfoxide\"}, {\"X\": 2.367784023284912, \"Y\": 2.6199722290039062, \"TOKEN\": \"liebe\"}, {\"X\": -3.176424503326416, \"Y\": 1.7406998872756958, \"TOKEN\": \"vrdoljak\"}, {\"X\": -0.7677643299102783, \"Y\": 8.146442413330078, \"TOKEN\": \"rigondeaux\"}, {\"X\": 1.12343168258667, \"Y\": 1.5688834190368652, \"TOKEN\": \"tessellate\"}, {\"X\": 8.183143615722656, \"Y\": -1.8774234056472778, \"TOKEN\": \"wsb\"}, {\"X\": -5.032670021057129, \"Y\": -0.9960605502128601, \"TOKEN\": \"\\u02dac\"}, {\"X\": -1.4555190801620483, \"Y\": 3.9244911670684814, \"TOKEN\": \"zemeri\"}, {\"X\": 2.91788911819458, \"Y\": -1.2221181392669678, \"TOKEN\": \"asahi\"}, {\"X\": 0.9056366682052612, \"Y\": 4.5234479904174805, \"TOKEN\": \"ranicki\"}, {\"X\": -4.062025547027588, \"Y\": -0.934537947177887, \"TOKEN\": \"diavik\"}, {\"X\": 5.360457897186279, \"Y\": 1.013445496559143, \"TOKEN\": \"morotai\"}, {\"X\": 5.793198108673096, \"Y\": 0.33876514434814453, \"TOKEN\": \"langfang\"}, {\"X\": 2.988696575164795, \"Y\": -4.585669040679932, \"TOKEN\": \"miniaturize\"}, {\"X\": -2.366706132888794, \"Y\": 2.2843430042266846, \"TOKEN\": \"isely\"}, {\"X\": 1.4265984296798706, \"Y\": 5.020030498504639, \"TOKEN\": \"19,000-square\"}, {\"X\": -0.4971320629119873, \"Y\": 4.772917747497559, \"TOKEN\": \"julin\"}, {\"X\": -3.082263708114624, \"Y\": -9.12528133392334, \"TOKEN\": \"1942\"}, {\"X\": 1.9622251987457275, \"Y\": -2.8509249687194824, \"TOKEN\": \"rereleasing\"}, {\"X\": -0.28891298174858093, \"Y\": 1.4113459587097168, \"TOKEN\": \"wajahatullah\"}, {\"X\": 1.1190706491470337, \"Y\": 0.07675223052501678, \"TOKEN\": \"thayar\"}, {\"X\": -0.4131123125553131, \"Y\": 4.128356456756592, \"TOKEN\": \"koka\"}, {\"X\": 1.7335666418075562, \"Y\": -1.181048035621643, \"TOKEN\": \"heathens\"}, {\"X\": -1.9502896070480347, \"Y\": 3.2576186656951904, \"TOKEN\": \"urschel\"}, {\"X\": 1.865788221359253, \"Y\": 0.18513920903205872, \"TOKEN\": \"acidified\"}, {\"X\": 0.18469370901584625, \"Y\": -3.5055346488952637, \"TOKEN\": \"11-fold\"}, {\"X\": 4.923969268798828, \"Y\": 2.0847420692443848, \"TOKEN\": \"u5\"}, {\"X\": -2.2288923263549805, \"Y\": -5.345607757568359, \"TOKEN\": \"duracraft\"}, {\"X\": 3.4411184787750244, \"Y\": 3.592583179473877, \"TOKEN\": \"10-seeded\"}, {\"X\": -3.958643674850464, \"Y\": 3.3522348403930664, \"TOKEN\": \"alwyn\"}, {\"X\": -1.8798552751541138, \"Y\": 3.3663365840911865, \"TOKEN\": \"fallis\"}, {\"X\": 1.0337257385253906, \"Y\": -1.2512460947036743, \"TOKEN\": \"matriclan\"}, {\"X\": 1.0372964143753052, \"Y\": -5.161895751953125, \"TOKEN\": \"doled\"}, {\"X\": 4.060685157775879, \"Y\": 0.7220119833946228, \"TOKEN\": \"kildin\"}, {\"X\": 3.255646228790283, \"Y\": -8.336398124694824, \"TOKEN\": \"california-based\"}, {\"X\": -2.7565829753875732, \"Y\": 2.989229440689087, \"TOKEN\": \"petrolite\"}, {\"X\": 5.229283809661865, \"Y\": 0.4897192716598511, \"TOKEN\": \"oporto\"}, {\"X\": 0.0488901287317276, \"Y\": -5.36124324798584, \"TOKEN\": \"untried\"}, {\"X\": 1.038029432296753, \"Y\": -5.458702087402344, \"TOKEN\": \"shriveled\"}, {\"X\": -5.740139007568359, \"Y\": -3.627342462539673, \"TOKEN\": \"97.51\"}, {\"X\": 1.896523118019104, \"Y\": 1.4942033290863037, \"TOKEN\": \"cemetry\"}, {\"X\": 0.5906159281730652, \"Y\": 0.05083366483449936, \"TOKEN\": \"dld\"}, {\"X\": -0.8170753121376038, \"Y\": 0.2264767736196518, \"TOKEN\": \"l-4\"}, {\"X\": 0.6896417140960693, \"Y\": 3.337526798248291, \"TOKEN\": \"dinu\"}, {\"X\": -6.66247034072876, \"Y\": -1.8985974788665771, \"TOKEN\": \"600-800\"}, {\"X\": -3.804046869277954, \"Y\": -4.175081729888916, \"TOKEN\": \"3,690\"}, {\"X\": 1.9938620328903198, \"Y\": -6.939980506896973, \"TOKEN\": \"cuff\"}, {\"X\": 0.8650065064430237, \"Y\": 2.5830612182617188, \"TOKEN\": \"radiodurans\"}, {\"X\": 0.05255594477057457, \"Y\": 0.0005049833562225103, \"TOKEN\": \"saturnian\"}, {\"X\": -1.79227614402771, \"Y\": 2.611802577972412, \"TOKEN\": \"denyer\"}, {\"X\": -2.883754014968872, \"Y\": -6.192086696624756, \"TOKEN\": \"amador\"}, {\"X\": -4.278799533843994, \"Y\": 3.1542277336120605, \"TOKEN\": \"rees-mogg\"}, {\"X\": -3.965102195739746, \"Y\": 3.919512987136841, \"TOKEN\": \"schwab\"}, {\"X\": -3.0713050365448, \"Y\": 3.8430449962615967, \"TOKEN\": \"luth\"}, {\"X\": 1.0122214555740356, \"Y\": -2.810633659362793, \"TOKEN\": \"superviser\"}, {\"X\": -1.5206624269485474, \"Y\": -8.850589752197266, \"TOKEN\": \"grand\"}, {\"X\": -6.257290840148926, \"Y\": -1.9482184648513794, \"TOKEN\": \"euro7\"}, {\"X\": -0.33902761340141296, \"Y\": -1.8808372020721436, \"TOKEN\": \"double-barrel\"}, {\"X\": 4.49393367767334, \"Y\": -8.185430526733398, \"TOKEN\": \"compiles\"}, {\"X\": -2.642587184906006, \"Y\": 0.7704499959945679, \"TOKEN\": \"anquetil\"}, {\"X\": 0.47471925616264343, \"Y\": -3.3808979988098145, \"TOKEN\": \"succinctness\"}, {\"X\": -4.193638324737549, \"Y\": -2.4527955055236816, \"TOKEN\": \"93.02\"}, {\"X\": 0.7124928832054138, \"Y\": 1.9923315048217773, \"TOKEN\": \"kont\"}, {\"X\": -6.252468109130859, \"Y\": 0.20445843040943146, \"TOKEN\": \"plurality\"}, {\"X\": -3.6497511863708496, \"Y\": -3.15397572517395, \"TOKEN\": \"9,150\"}, {\"X\": -4.6870269775390625, \"Y\": -2.4711854457855225, \"TOKEN\": \"34.39\"}, {\"X\": -3.6557538509368896, \"Y\": 6.636441230773926, \"TOKEN\": \"guiraud\"}, {\"X\": -1.2443146705627441, \"Y\": -9.245711326599121, \"TOKEN\": \"supreme\"}, {\"X\": 6.944912433624268, \"Y\": -0.30960720777511597, \"TOKEN\": \"washingtonville\"}, {\"X\": -1.8437529802322388, \"Y\": 4.232559680938721, \"TOKEN\": \"shanor\"}, {\"X\": -0.4182816743850708, \"Y\": 5.200358867645264, \"TOKEN\": \"scroggins\"}, {\"X\": -1.4205657243728638, \"Y\": -5.397712230682373, \"TOKEN\": \"sampaio\"}, {\"X\": -0.9392279386520386, \"Y\": 4.891716003417969, \"TOKEN\": \"thubten\"}, {\"X\": 0.3692604899406433, \"Y\": -1.1819840669631958, \"TOKEN\": \"blindspot\"}, {\"X\": -0.666486382484436, \"Y\": 3.6065635681152344, \"TOKEN\": \"suphamongkhon\"}, {\"X\": -4.8144097328186035, \"Y\": -2.567222833633423, \"TOKEN\": \"38.45\"}, {\"X\": -6.212403774261475, \"Y\": -1.6828417778015137, \"TOKEN\": \"euro415\"}, {\"X\": -1.5146464109420776, \"Y\": -9.003388404846191, \"TOKEN\": \"happened\"}, {\"X\": 0.19699449837207794, \"Y\": -5.5406999588012695, \"TOKEN\": \"eccentrically\"}, {\"X\": 5.4449310302734375, \"Y\": 6.687944412231445, \"TOKEN\": \"shahrd\"}, {\"X\": 6.618799686431885, \"Y\": -2.4321913719177246, \"TOKEN\": \"wesco\"}, {\"X\": 3.6650822162628174, \"Y\": -0.2607676684856415, \"TOKEN\": \"ringerike\"}, {\"X\": -0.21616923809051514, \"Y\": 7.13012170791626, \"TOKEN\": \"guest-star\"}, {\"X\": -1.2382224798202515, \"Y\": 2.643350124359131, \"TOKEN\": \"walling\"}, {\"X\": 4.628227710723877, \"Y\": -0.2444543093442917, \"TOKEN\": \"wangqing\"}, {\"X\": -3.082946538925171, \"Y\": -1.380333662033081, \"TOKEN\": \"backend\"}, {\"X\": 2.239543914794922, \"Y\": 0.8654650449752808, \"TOKEN\": \"allemands\"}, {\"X\": 7.053160190582275, \"Y\": 2.0567123889923096, \"TOKEN\": \"dniester\"}, {\"X\": -1.6419438123703003, \"Y\": -9.141887664794922, \"TOKEN\": \"group\"}, {\"X\": 5.505568027496338, \"Y\": 3.1083736419677734, \"TOKEN\": \"ladybugs\"}, {\"X\": 2.921154022216797, \"Y\": -0.08110534399747849, \"TOKEN\": \"cliquot\"}, {\"X\": -2.0793724060058594, \"Y\": -1.806443452835083, \"TOKEN\": \"niortais\"}, {\"X\": -3.867225408554077, \"Y\": -5.908262252807617, \"TOKEN\": \"tongji\"}, {\"X\": 0.8584017157554626, \"Y\": 2.315723180770874, \"TOKEN\": \"winget\"}, {\"X\": -2.7181460857391357, \"Y\": -8.493032455444336, \"TOKEN\": \"murphy\"}, {\"X\": -4.814939975738525, \"Y\": -5.401973247528076, \"TOKEN\": \"558\"}, {\"X\": -1.9859287738800049, \"Y\": 2.4470229148864746, \"TOKEN\": \"walpin\"}, {\"X\": -5.046627521514893, \"Y\": -0.23788565397262573, \"TOKEN\": \"kish\"}, {\"X\": -2.179734468460083, \"Y\": -0.6315831542015076, \"TOKEN\": \"rgc\"}, {\"X\": 3.866849184036255, \"Y\": -4.743912696838379, \"TOKEN\": \"fourball\"}, {\"X\": -0.9827025532722473, \"Y\": 0.9318475723266602, \"TOKEN\": \"kessenich\"}, {\"X\": -6.368143558502197, \"Y\": -1.5280568599700928, \"TOKEN\": \"llion\"}, {\"X\": -0.6911982297897339, \"Y\": 1.7994849681854248, \"TOKEN\": \"matlala\"}, {\"X\": 1.4331433773040771, \"Y\": -0.9704139232635498, \"TOKEN\": \"4-1-2\"}, {\"X\": 3.280824899673462, \"Y\": 2.963179349899292, \"TOKEN\": \"zayda\"}, {\"X\": -0.4095038175582886, \"Y\": 2.6817665100097656, \"TOKEN\": \"nishikori\"}, {\"X\": 6.0808281898498535, \"Y\": -6.0840959548950195, \"TOKEN\": \"electrabel\"}, {\"X\": 0.204802006483078, \"Y\": -0.07445121556520462, \"TOKEN\": \".454\"}, {\"X\": -0.7584663033485413, \"Y\": 0.07244569808244705, \"TOKEN\": \"reanimate\"}, {\"X\": 6.0219855308532715, \"Y\": -4.478334426879883, \"TOKEN\": \"malam\"}, {\"X\": 2.721672296524048, \"Y\": 0.48128825426101685, \"TOKEN\": \"homosapien\"}, {\"X\": -1.0871235132217407, \"Y\": 0.4642641544342041, \"TOKEN\": \"enkhbold\"}, {\"X\": -3.4973948001861572, \"Y\": 1.6543909311294556, \"TOKEN\": \"jord\\u00e3o\"}, {\"X\": 0.6612094640731812, \"Y\": 1.6590662002563477, \"TOKEN\": \"otelo\"}, {\"X\": 1.803376317024231, \"Y\": 2.916581153869629, \"TOKEN\": \"proth\"}, {\"X\": -3.88139271736145, \"Y\": 2.2200846672058105, \"TOKEN\": \"filion\"}, {\"X\": -1.9140372276306152, \"Y\": 7.2745490074157715, \"TOKEN\": \"all-pac-12\"}, {\"X\": -4.917239665985107, \"Y\": -2.5751378536224365, \"TOKEN\": \"26.68\"}, {\"X\": 1.8276093006134033, \"Y\": -2.94364333152771, \"TOKEN\": \"vernaculars\"}, {\"X\": -4.72952938079834, \"Y\": -2.46987247467041, \"TOKEN\": \"32.92\"}, {\"X\": 2.115865707397461, \"Y\": 4.889372825622559, \"TOKEN\": \"shiang\"}, {\"X\": 1.6413602828979492, \"Y\": 3.0790631771087646, \"TOKEN\": \"demokratiko\"}, {\"X\": -4.594971656799316, \"Y\": 3.1112160682678223, \"TOKEN\": \"backhouse\"}, {\"X\": -2.07694935798645, \"Y\": -1.727738618850708, \"TOKEN\": \"giannina\"}, {\"X\": -4.730849266052246, \"Y\": -2.520561456680298, \"TOKEN\": \"42.58\"}, {\"X\": -3.733447551727295, \"Y\": 5.564455032348633, \"TOKEN\": \"pati\"}, {\"X\": 0.5550671219825745, \"Y\": 3.4421463012695312, \"TOKEN\": \"paudwal\"}, {\"X\": 2.5815725326538086, \"Y\": -5.086757183074951, \"TOKEN\": \"hazama\"}, {\"X\": 1.0543314218521118, \"Y\": -1.5222193002700806, \"TOKEN\": \"750-word\"}, {\"X\": -0.1777489334344864, \"Y\": -1.271344542503357, \"TOKEN\": \"mid-fifteenth\"}, {\"X\": -2.7431657314300537, \"Y\": -3.590113639831543, \"TOKEN\": \"3,583\"}, {\"X\": -0.9531739354133606, \"Y\": -8.270195960998535, \"TOKEN\": \"stranger\"}, {\"X\": 1.4647313356399536, \"Y\": 1.7788102626800537, \"TOKEN\": \"couvert\"}, {\"X\": 0.07178334146738052, \"Y\": 0.6407687664031982, \"TOKEN\": \"cantus\"}, {\"X\": 0.6063838005065918, \"Y\": -8.82540512084961, \"TOKEN\": \"optional\"}, {\"X\": -0.4277896583080292, \"Y\": 6.898221015930176, \"TOKEN\": \"mcconaughey\"}, {\"X\": 4.497198581695557, \"Y\": 0.8660377860069275, \"TOKEN\": \"dornach\"}, {\"X\": -2.5512280464172363, \"Y\": 0.2318640798330307, \"TOKEN\": \"veera\"}, {\"X\": -5.0007758140563965, \"Y\": 2.8344566822052, \"TOKEN\": \"consecration\"}, {\"X\": -1.7073462009429932, \"Y\": 7.821371078491211, \"TOKEN\": \"curated\"}, {\"X\": -3.091804027557373, \"Y\": 5.953807353973389, \"TOKEN\": \"conita\"}, {\"X\": 2.284656524658203, \"Y\": 0.592175304889679, \"TOKEN\": \"bullrich\"}, {\"X\": 4.67317008972168, \"Y\": -1.4012149572372437, \"TOKEN\": \"quequechan\"}, {\"X\": 2.129852771759033, \"Y\": -5.431641578674316, \"TOKEN\": \"expressionism\"}, {\"X\": -2.1143054962158203, \"Y\": -1.595406174659729, \"TOKEN\": \"annville\"}, {\"X\": -0.4341637194156647, \"Y\": 0.7573947310447693, \"TOKEN\": \"ky\\u014dsuke\"}, {\"X\": 1.5674803256988525, \"Y\": -0.23607893288135529, \"TOKEN\": \"kfve\"}, {\"X\": -1.2963119745254517, \"Y\": -9.004715919494629, \"TOKEN\": \"demise\"}, {\"X\": -4.701084613800049, \"Y\": 0.9868893623352051, \"TOKEN\": \"kemprecos\"}, {\"X\": -1.9660214185714722, \"Y\": -7.7042646408081055, \"TOKEN\": \"composer\"}, {\"X\": -5.435179710388184, \"Y\": 3.3991994857788086, \"TOKEN\": \"silverjet\"}, {\"X\": 3.022958278656006, \"Y\": -1.1547175645828247, \"TOKEN\": \"27-time\"}, {\"X\": -1.6525861024856567, \"Y\": 0.45372456312179565, \"TOKEN\": \"mahtab\"}, {\"X\": 2.1633691787719727, \"Y\": -2.5873684883117676, \"TOKEN\": \"2,000-yard\"}, {\"X\": 6.0252275466918945, \"Y\": 3.7191290855407715, \"TOKEN\": \"knockoff\"}, {\"X\": -1.5070559978485107, \"Y\": 5.49469518661499, \"TOKEN\": \"angelia\"}, {\"X\": -1.340378761291504, \"Y\": 2.067554235458374, \"TOKEN\": \"maiyo\"}, {\"X\": 2.2919437885284424, \"Y\": -2.2194833755493164, \"TOKEN\": \"rostelecom\"}, {\"X\": -2.1967873573303223, \"Y\": 2.729353904724121, \"TOKEN\": \"lagana\"}, {\"X\": 2.2928497791290283, \"Y\": 2.628793239593506, \"TOKEN\": \"sechs\"}, {\"X\": 1.356519103050232, \"Y\": 0.33953896164894104, \"TOKEN\": \"sandfly\"}, {\"X\": -2.8218631744384766, \"Y\": -1.0107632875442505, \"TOKEN\": \"tftp\"}, {\"X\": -2.849973678588867, \"Y\": -0.34242767095565796, \"TOKEN\": \"uniloc\"}, {\"X\": 5.4725236892700195, \"Y\": 6.723276615142822, \"TOKEN\": \"gyt\"}, {\"X\": -2.726600408554077, \"Y\": -6.089389801025391, \"TOKEN\": \"nu\\u00f1ez\"}, {\"X\": -4.960446834564209, \"Y\": 4.488407135009766, \"TOKEN\": \"edler\"}, {\"X\": -1.6673035621643066, \"Y\": -3.9551453590393066, \"TOKEN\": \"beirutis\"}, {\"X\": 1.116285800933838, \"Y\": -1.8901313543319702, \"TOKEN\": \"countesses\"}, {\"X\": -8.601531982421875, \"Y\": -0.7615561485290527, \"TOKEN\": \"12:14\"}, {\"X\": -2.90358567237854, \"Y\": 1.9330084323883057, \"TOKEN\": \"benedito\"}, {\"X\": -3.316685676574707, \"Y\": 4.472309589385986, \"TOKEN\": \"bohnen\"}, {\"X\": -3.770129442214966, \"Y\": -3.9067888259887695, \"TOKEN\": \"27,700\"}, {\"X\": 3.2881767749786377, \"Y\": 1.34856116771698, \"TOKEN\": \"rushbrooke\"}, {\"X\": 2.950620412826538, \"Y\": -2.439591407775879, \"TOKEN\": \"homebuilders\"}, {\"X\": -6.327258586883545, \"Y\": -0.9684196710586548, \"TOKEN\": \"non-medal\"}, {\"X\": -1.2445800304412842, \"Y\": -0.9165826439857483, \"TOKEN\": \"wodeyars\"}, {\"X\": -5.051043510437012, \"Y\": -2.4905033111572266, \"TOKEN\": \"39.37\"}, {\"X\": -3.7990927696228027, \"Y\": -9.181625366210938, \"TOKEN\": \"1775\"}, {\"X\": -8.643572807312012, \"Y\": -0.7155796885490417, \"TOKEN\": \"0830\"}, {\"X\": 4.886225700378418, \"Y\": 1.4146193265914917, \"TOKEN\": \"wetzlar\"}, {\"X\": -0.6841774582862854, \"Y\": -4.362939834594727, \"TOKEN\": \"unshakably\"}, {\"X\": -0.011728199198842049, \"Y\": 5.496047019958496, \"TOKEN\": \"b\\u00f6lkow\"}, {\"X\": -4.425971031188965, \"Y\": 3.843562364578247, \"TOKEN\": \"zhaowen\"}, {\"X\": -3.4358949661254883, \"Y\": 3.111959457397461, \"TOKEN\": \"aikins\"}, {\"X\": 0.7356833815574646, \"Y\": 1.924687147140503, \"TOKEN\": \"septima\"}, {\"X\": -1.755234718322754, \"Y\": 0.294526606798172, \"TOKEN\": \"cipa\"}, {\"X\": -9.948189735412598, \"Y\": -1.7860705852508545, \"TOKEN\": \"hermogenes\"}, {\"X\": 0.5725592970848083, \"Y\": -3.39821720123291, \"TOKEN\": \"ambidexterity\"}, {\"X\": -5.199862003326416, \"Y\": -2.5443851947784424, \"TOKEN\": \"15.98\"}, {\"X\": -0.5600321292877197, \"Y\": 5.967442989349365, \"TOKEN\": \"apfelbaum\"}, {\"X\": -2.991274833679199, \"Y\": -7.519075870513916, \"TOKEN\": \"dion\"}, {\"X\": 3.2542500495910645, \"Y\": -8.335367202758789, \"TOKEN\": \"angeles-based\"}, {\"X\": 2.584132671356201, \"Y\": 0.0898575708270073, \"TOKEN\": \"5.5645\"}, {\"X\": -1.2854665517807007, \"Y\": 1.5059257745742798, \"TOKEN\": \"maxym\"}, {\"X\": -1.100131630897522, \"Y\": -1.7554962635040283, \"TOKEN\": \"vichy\"}, {\"X\": 0.7796422839164734, \"Y\": -0.1118934229016304, \"TOKEN\": \"ruggedized\"}, {\"X\": 0.7755040526390076, \"Y\": -6.964107513427734, \"TOKEN\": \"inspires\"}, {\"X\": 2.166576862335205, \"Y\": -2.6669294834136963, \"TOKEN\": \"oboists\"}, {\"X\": -2.477435827255249, \"Y\": 5.399312496185303, \"TOKEN\": \"nakajima\"}, {\"X\": -1.3733747005462646, \"Y\": 2.5659966468811035, \"TOKEN\": \"bobbe\"}, {\"X\": 3.482304096221924, \"Y\": -0.9861575365066528, \"TOKEN\": \"glenfield\"}, {\"X\": -0.965747058391571, \"Y\": -2.5944106578826904, \"TOKEN\": \"ycc\"}, {\"X\": 0.04806632548570633, \"Y\": 3.1796905994415283, \"TOKEN\": \"sasich\"}, {\"X\": -3.718977689743042, \"Y\": -3.9072132110595703, \"TOKEN\": \"666,000\"}, {\"X\": -2.8384592533111572, \"Y\": 1.4688175916671753, \"TOKEN\": \"simm\"}, {\"X\": 2.211263656616211, \"Y\": -0.038537051528692245, \"TOKEN\": \"gvrd\"}, {\"X\": 3.6752898693084717, \"Y\": -2.1933717727661133, \"TOKEN\": \"reef\"}, {\"X\": -2.1582436561584473, \"Y\": -7.862069606781006, \"TOKEN\": \"solo\"}, {\"X\": -0.1426718384027481, \"Y\": 2.4505367279052734, \"TOKEN\": \"onuki\"}, {\"X\": 5.485925674438477, \"Y\": -1.2687978744506836, \"TOKEN\": \"potton\"}, {\"X\": 1.9254802465438843, \"Y\": 1.919015884399414, \"TOKEN\": \"jutro\"}, {\"X\": 4.648977279663086, \"Y\": 2.8229572772979736, \"TOKEN\": \"sheldonian\"}, {\"X\": -3.8468754291534424, \"Y\": -5.813128471374512, \"TOKEN\": \"amirkabir\"}, {\"X\": 2.245205879211426, \"Y\": -0.48707300424575806, \"TOKEN\": \"tapout\"}, {\"X\": -8.604597091674805, \"Y\": -0.7704980969429016, \"TOKEN\": \"4:51\"}, {\"X\": -4.772418022155762, \"Y\": 5.90720796585083, \"TOKEN\": \"skuas\"}, {\"X\": 3.4026153087615967, \"Y\": -1.8963475227355957, \"TOKEN\": \"drought-tolerant\"}, {\"X\": 6.070556163787842, \"Y\": -6.051581382751465, \"TOKEN\": \"lagoven\"}, {\"X\": 2.6850039958953857, \"Y\": -0.48028454184532166, \"TOKEN\": \"area-based\"}, {\"X\": -2.8016607761383057, \"Y\": -0.5717361569404602, \"TOKEN\": \"wats\"}, {\"X\": 4.117867946624756, \"Y\": -4.684544086456299, \"TOKEN\": \"wednsday\"}, {\"X\": -5.627135276794434, \"Y\": 1.5600535869598389, \"TOKEN\": \"rusch\"}, {\"X\": -0.2749963402748108, \"Y\": 4.2487664222717285, \"TOKEN\": \"coseteng\"}, {\"X\": 0.8906390070915222, \"Y\": 5.357461929321289, \"TOKEN\": \"jassam\"}, {\"X\": 3.668700695037842, \"Y\": 2.137521743774414, \"TOKEN\": \"duurstede\"}, {\"X\": 4.174766540527344, \"Y\": -3.4456562995910645, \"TOKEN\": \"swedenborgian\"}, {\"X\": 2.156863212585449, \"Y\": -6.341279029846191, \"TOKEN\": \"wiring\"}, {\"X\": 5.495119571685791, \"Y\": 6.731674671173096, \"TOKEN\": \"syf\"}, {\"X\": -5.401120185852051, \"Y\": 3.8456592559814453, \"TOKEN\": \"eberle\"}, {\"X\": -2.7000648975372314, \"Y\": 2.148387908935547, \"TOKEN\": \"holovak\"}, {\"X\": 1.147771954536438, \"Y\": -2.470273017883301, \"TOKEN\": \"character-building\"}, {\"X\": 1.722019910812378, \"Y\": 0.441986620426178, \"TOKEN\": \"tevaram\"}, {\"X\": -2.1244866847991943, \"Y\": -4.424713134765625, \"TOKEN\": \"leoneans\"}, {\"X\": -4.2404279708862305, \"Y\": -2.5708231925964355, \"TOKEN\": \"200.00\"}, {\"X\": 7.5847086906433105, \"Y\": -2.948653221130371, \"TOKEN\": \"chumash\"}, {\"X\": 1.2421913146972656, \"Y\": -4.301100730895996, \"TOKEN\": \"zaniness\"}, {\"X\": 0.8382660150527954, \"Y\": -6.92715311050415, \"TOKEN\": \"craves\"}, {\"X\": 2.2099571228027344, \"Y\": -6.086332321166992, \"TOKEN\": \"armatures\"}, {\"X\": 0.7191411852836609, \"Y\": -3.3333895206451416, \"TOKEN\": \"entrepreneurism\"}, {\"X\": -0.7306424975395203, \"Y\": -7.907174110412598, \"TOKEN\": \"deserted\"}, {\"X\": 4.223280906677246, \"Y\": 2.248677968978882, \"TOKEN\": \"metrotown\"}, {\"X\": 3.731653928756714, \"Y\": 0.04652376472949982, \"TOKEN\": \"druskininkai\"}, {\"X\": -1.453542947769165, \"Y\": 0.9231002926826477, \"TOKEN\": \"rudess\"}, {\"X\": 5.385557174682617, \"Y\": 0.4810841679573059, \"TOKEN\": \"sacheon\"}, {\"X\": 1.5327094793319702, \"Y\": 0.504365861415863, \"TOKEN\": \"cardoon\"}, {\"X\": -3.258026599884033, \"Y\": -2.5830442905426025, \"TOKEN\": \"moxley\"}, {\"X\": -0.15725542604923248, \"Y\": 1.937958002090454, \"TOKEN\": \"heyting\"}, {\"X\": -5.4690093994140625, \"Y\": 1.4801431894302368, \"TOKEN\": \"belitz\"}, {\"X\": 0.4625832438468933, \"Y\": -2.8345015048980713, \"TOKEN\": \"energy-absorbing\"}, {\"X\": 0.508070170879364, \"Y\": -3.9989583492279053, \"TOKEN\": \"tooted\"}, {\"X\": 3.8703346252441406, \"Y\": 4.264429569244385, \"TOKEN\": \"fotos\"}, {\"X\": -0.3338259756565094, \"Y\": -1.6740663051605225, \"TOKEN\": \"voivodes\"}, {\"X\": -2.074258804321289, \"Y\": 4.5598835945129395, \"TOKEN\": \"endocrinologist\"}, {\"X\": -1.1420873403549194, \"Y\": 3.06958270072937, \"TOKEN\": \"mbowe\"}, {\"X\": -2.8607571125030518, \"Y\": 5.330495834350586, \"TOKEN\": \"avraham\"}, {\"X\": 2.302940845489502, \"Y\": -4.504836559295654, \"TOKEN\": \"personifying\"}, {\"X\": -5.200090408325195, \"Y\": -7.028336048126221, \"TOKEN\": \"quorum\"}, {\"X\": -0.03359353542327881, \"Y\": 2.630465507507324, \"TOKEN\": \"salguero\"}, {\"X\": 3.5964016914367676, \"Y\": -3.1030385494232178, \"TOKEN\": \"h/aca\"}, {\"X\": 1.6819225549697876, \"Y\": 6.268127918243408, \"TOKEN\": \"menezes\"}, {\"X\": 3.4818553924560547, \"Y\": -2.226658821105957, \"TOKEN\": \"horticulture\"}, {\"X\": -4.934319019317627, \"Y\": -2.5660576820373535, \"TOKEN\": \"45.83\"}, {\"X\": -4.202744960784912, \"Y\": -2.4088265895843506, \"TOKEN\": \"88.69\"}, {\"X\": -3.617953062057495, \"Y\": 1.2183103561401367, \"TOKEN\": \"statelets\"}, {\"X\": 0.49254006147384644, \"Y\": 3.1097350120544434, \"TOKEN\": \"qingpeng\"}, {\"X\": -1.0938371419906616, \"Y\": -9.10611343383789, \"TOKEN\": \"conclude\"}, {\"X\": -3.9150891304016113, \"Y\": -2.2657907009124756, \"TOKEN\": \"112.35\"}, {\"X\": -1.9210015535354614, \"Y\": 2.7306289672851562, \"TOKEN\": \"zaehner\"}, {\"X\": 0.8676338791847229, \"Y\": -6.5381293296813965, \"TOKEN\": \"teamwork\"}, {\"X\": 0.3619656562805176, \"Y\": 2.5357861518859863, \"TOKEN\": \"sekiguchi\"}, {\"X\": -1.8306183815002441, \"Y\": 5.784470081329346, \"TOKEN\": \"newlove\"}, {\"X\": -1.121915578842163, \"Y\": -9.076736450195312, \"TOKEN\": \"findings\"}, {\"X\": 7.5224175453186035, \"Y\": 3.839071035385132, \"TOKEN\": \"bancboston\"}, {\"X\": -0.13347339630126953, \"Y\": 1.8583672046661377, \"TOKEN\": \"preki\"}, {\"X\": -5.6030802726745605, \"Y\": 0.9354186058044434, \"TOKEN\": \"stsmith\"}, {\"X\": 2.5454487800598145, \"Y\": 5.221979141235352, \"TOKEN\": \"bonnevie\"}, {\"X\": -1.1432862281799316, \"Y\": -0.7664085626602173, \"TOKEN\": \"neustria\"}, {\"X\": 3.6678011417388916, \"Y\": 0.4741080701351166, \"TOKEN\": \"ijara\"}, {\"X\": 3.0972044467926025, \"Y\": 8.177419662475586, \"TOKEN\": \"torte\"}, {\"X\": -1.1970136165618896, \"Y\": 2.8913421630859375, \"TOKEN\": \"gerin\"}, {\"X\": -4.284700393676758, \"Y\": 2.246316909790039, \"TOKEN\": \"crecion\"}, {\"X\": -3.7923150062561035, \"Y\": -1.9163844585418701, \"TOKEN\": \"star-telegram\"}, {\"X\": 8.090168952941895, \"Y\": 1.149025797843933, \"TOKEN\": \"1430gmt\"}, {\"X\": -5.5128560066223145, \"Y\": 5.525435447692871, \"TOKEN\": \"-64\"}, {\"X\": 0.36374175548553467, \"Y\": -0.322931170463562, \"TOKEN\": \"jiuta\"}, {\"X\": 1.6954258680343628, \"Y\": 1.953199028968811, \"TOKEN\": \"sterben\"}, {\"X\": -0.09790448844432831, \"Y\": -4.72565221786499, \"TOKEN\": \"cultists\"}, {\"X\": -3.541592836380005, \"Y\": 4.659641742706299, \"TOKEN\": \"madhusudan\"}, {\"X\": -1.9695117473602295, \"Y\": -0.4768633246421814, \"TOKEN\": \"spindleshanks\"}, {\"X\": 1.8432879447937012, \"Y\": -3.7630746364593506, \"TOKEN\": \"romanticizing\"}, {\"X\": 3.0109121799468994, \"Y\": -6.6694207191467285, \"TOKEN\": \"abscond\"}, {\"X\": 4.28929328918457, \"Y\": -0.12740783393383026, \"TOKEN\": \"kabgayi\"}, {\"X\": 3.733924150466919, \"Y\": 1.0189896821975708, \"TOKEN\": \"townhead\"}, {\"X\": -3.559352397918701, \"Y\": 4.607999801635742, \"TOKEN\": \"pesek\"}, {\"X\": -0.7456741333007812, \"Y\": 2.0179080963134766, \"TOKEN\": \"aulisi\"}, {\"X\": 3.4923927783966064, \"Y\": -5.051375865936279, \"TOKEN\": \"despatches\"}, {\"X\": 2.9071905612945557, \"Y\": -0.9023000597953796, \"TOKEN\": \"nisa\"}, {\"X\": -4.939437389373779, \"Y\": -2.79794979095459, \"TOKEN\": \"16.94\"}, {\"X\": -0.2812194228172302, \"Y\": -1.53914475440979, \"TOKEN\": \"voicetracked\"}, {\"X\": -0.3273802697658539, \"Y\": -1.010378360748291, \"TOKEN\": \"1976-1978\"}, {\"X\": -0.1951807737350464, \"Y\": -2.044625759124756, \"TOKEN\": \"printing-press\"}, {\"X\": 2.668341636657715, \"Y\": 4.113138675689697, \"TOKEN\": \"bryanne\"}, {\"X\": 3.6274573802948, \"Y\": -1.7334425449371338, \"TOKEN\": \"baw\"}, {\"X\": 7.339224338531494, \"Y\": -0.3826999366283417, \"TOKEN\": \"waterville\"}, {\"X\": 6.6213059425354, \"Y\": 2.981431722640991, \"TOKEN\": \"dongcheon\"}, {\"X\": -0.6457532048225403, \"Y\": -5.5690083503723145, \"TOKEN\": \"wryly\"}, {\"X\": 0.9574442505836487, \"Y\": 5.555083751678467, \"TOKEN\": \"scheft\"}, {\"X\": 4.7585320472717285, \"Y\": 1.325708270072937, \"TOKEN\": \"skokie\"}, {\"X\": 3.9654932022094727, \"Y\": -0.8416271805763245, \"TOKEN\": \"chilcotin\"}, {\"X\": -4.9323272705078125, \"Y\": -1.006134033203125, \"TOKEN\": \"29c\"}, {\"X\": 2.879265785217285, \"Y\": -2.2598042488098145, \"TOKEN\": \"a-league\"}, {\"X\": -1.1224194765090942, \"Y\": -1.9473202228546143, \"TOKEN\": \"lorax.ldc.upenn.edu\"}, {\"X\": -0.6831379532814026, \"Y\": -1.2314846515655518, \"TOKEN\": \"legitimists\"}, {\"X\": -3.2250750064849854, \"Y\": 0.25494030117988586, \"TOKEN\": \"vaxgen\"}, {\"X\": -1.8071585893630981, \"Y\": 0.3570829927921295, \"TOKEN\": \"bimal\"}, {\"X\": 0.10590732842683792, \"Y\": -5.3701910972595215, \"TOKEN\": \"unconventionally\"}, {\"X\": 4.496291637420654, \"Y\": -1.3569035530090332, \"TOKEN\": \"sub-basin\"}, {\"X\": -0.03320879489183426, \"Y\": 2.801802635192871, \"TOKEN\": \"gotsch\"}, {\"X\": -2.3977549076080322, \"Y\": 1.1252402067184448, \"TOKEN\": \"niculae\"}, {\"X\": 6.5226898193359375, \"Y\": 1.574853777885437, \"TOKEN\": \"charentes\"}, {\"X\": -0.7971537113189697, \"Y\": -1.314652919769287, \"TOKEN\": \"cholas\"}, {\"X\": 3.4922244548797607, \"Y\": 3.4602744579315186, \"TOKEN\": \"wibier\"}, {\"X\": -3.1999106407165527, \"Y\": -1.7547460794448853, \"TOKEN\": \"srd\"}, {\"X\": 1.0228970050811768, \"Y\": 0.0016983160749077797, \"TOKEN\": \"extinguishes\"}, {\"X\": 1.3162153959274292, \"Y\": -0.8047680854797363, \"TOKEN\": \"bracteates\"}, {\"X\": 1.4086121320724487, \"Y\": 2.027024745941162, \"TOKEN\": \"gerygone\"}, {\"X\": -4.853578090667725, \"Y\": -2.505862236022949, \"TOKEN\": \"55.70\"}, {\"X\": 2.1411292552948, \"Y\": -5.4352827072143555, \"TOKEN\": \"representational\"}, {\"X\": -4.907486438751221, \"Y\": -6.357901096343994, \"TOKEN\": \"average\"}, {\"X\": 1.515243411064148, \"Y\": -8.182640075683594, \"TOKEN\": \"add-ons\"}, {\"X\": -3.0732784271240234, \"Y\": 6.660426139831543, \"TOKEN\": \"ahtisaari\"}, {\"X\": 2.355792760848999, \"Y\": -1.688447117805481, \"TOKEN\": \"shakespeareans\"}, {\"X\": -0.08697384595870972, \"Y\": -1.7696826457977295, \"TOKEN\": \"photo-journalist\"}, {\"X\": -7.282003402709961, \"Y\": 2.6595308780670166, \"TOKEN\": \"cyberscope\"}, {\"X\": -2.03532338142395, \"Y\": 5.163765907287598, \"TOKEN\": \"barata\"}, {\"X\": 2.2659528255462646, \"Y\": -3.0454905033111572, \"TOKEN\": \"bilbie\"}, {\"X\": -0.7388038635253906, \"Y\": 3.229285478591919, \"TOKEN\": \"hagglund\"}, {\"X\": -2.850111246109009, \"Y\": -1.2582144737243652, \"TOKEN\": \"pajaritos\"}, {\"X\": -6.01678466796875, \"Y\": -0.7972886562347412, \"TOKEN\": \"nfeliz\"}, {\"X\": -0.059082500636577606, \"Y\": -1.3611057996749878, \"TOKEN\": \"slaveowner\"}, {\"X\": -2.4647130966186523, \"Y\": -2.979701280593872, \"TOKEN\": \"unroofed\"}, {\"X\": 7.752511024475098, \"Y\": -4.157939910888672, \"TOKEN\": \"hohenzollern-sigmaringen\"}, {\"X\": 0.8708380460739136, \"Y\": -4.558597564697266, \"TOKEN\": \"polemic\"}, {\"X\": -3.9318039417266846, \"Y\": -3.2023239135742188, \"TOKEN\": \"682.5\"}, {\"X\": 1.8608564138412476, \"Y\": -4.072483062744141, \"TOKEN\": \"pursuance\"}, {\"X\": -0.257547527551651, \"Y\": 2.5317511558532715, \"TOKEN\": \"grassman\"}, {\"X\": 0.5861473083496094, \"Y\": -1.244521975517273, \"TOKEN\": \"cumulonimbus\"}, {\"X\": -1.5509660243988037, \"Y\": 3.469106674194336, \"TOKEN\": \"abramkin\"}, {\"X\": 1.5461808443069458, \"Y\": 2.6636221408843994, \"TOKEN\": \"bombardir\"}, {\"X\": 4.934678554534912, \"Y\": 0.035840291529893875, \"TOKEN\": \"copperbelt\"}, {\"X\": -0.6296182870864868, \"Y\": 1.8591666221618652, \"TOKEN\": \"enderlein\"}, {\"X\": 5.999582767486572, \"Y\": -0.37680765986442566, \"TOKEN\": \"kahnuj\"}, {\"X\": -0.7104083895683289, \"Y\": 3.3522160053253174, \"TOKEN\": \"oulai\"}, {\"X\": -0.5651318430900574, \"Y\": 0.6215614676475525, \"TOKEN\": \"burnum\"}, {\"X\": 1.0487598180770874, \"Y\": 7.132232189178467, \"TOKEN\": \"schulken\"}, {\"X\": 3.3715052604675293, \"Y\": 2.9449961185455322, \"TOKEN\": \"j-2x\"}, {\"X\": 1.0953094959259033, \"Y\": -2.6229312419891357, \"TOKEN\": \"worst-ever\"}, {\"X\": -0.3539491891860962, \"Y\": -0.9617049098014832, \"TOKEN\": \"soomra\"}, {\"X\": 1.4974737167358398, \"Y\": -1.736025094985962, \"TOKEN\": \"13-22\"}, {\"X\": -1.4915870428085327, \"Y\": 3.7148032188415527, \"TOKEN\": \"haverkamp\"}, {\"X\": -7.400049209594727, \"Y\": -3.2620022296905518, \"TOKEN\": \"99-90\"}, {\"X\": -1.915268898010254, \"Y\": 0.4015344977378845, \"TOKEN\": \"isda\"}, {\"X\": -1.6619662046432495, \"Y\": 2.5728185176849365, \"TOKEN\": \"beinisch\"}, {\"X\": 0.7557347416877747, \"Y\": 3.1956403255462646, \"TOKEN\": \"tjarnqvist\"}, {\"X\": 1.2184488773345947, \"Y\": -5.584362983703613, \"TOKEN\": \"spongy\"}, {\"X\": 0.8039149045944214, \"Y\": -2.4232187271118164, \"TOKEN\": \"propinquity\"}, {\"X\": -1.7821928262710571, \"Y\": 4.855127334594727, \"TOKEN\": \"renn\"}, {\"X\": 6.037576675415039, \"Y\": 1.3704135417938232, \"TOKEN\": \"bacc\"}, {\"X\": -2.9057958126068115, \"Y\": 3.364262342453003, \"TOKEN\": \"fitkin\"}, {\"X\": 5.473352432250977, \"Y\": 6.739565849304199, \"TOKEN\": \"seye\"}, {\"X\": -2.0068681240081787, \"Y\": -3.3602185249328613, \"TOKEN\": \"eye-witnesses\"}, {\"X\": 6.028566360473633, \"Y\": 4.733551025390625, \"TOKEN\": \"kestrel\"}, {\"X\": 1.9100724458694458, \"Y\": -2.455641746520996, \"TOKEN\": \"top-grade\"}, {\"X\": 0.7834218740463257, \"Y\": -4.302350044250488, \"TOKEN\": \"handsomeness\"}, {\"X\": -4.285731792449951, \"Y\": -0.45738908648490906, \"TOKEN\": \"nf6\"}, {\"X\": 0.9107462167739868, \"Y\": -1.2049057483673096, \"TOKEN\": \"gandhism\"}, {\"X\": -5.150500297546387, \"Y\": -2.6257574558258057, \"TOKEN\": \"20.75\"}, {\"X\": 6.966492176055908, \"Y\": -0.3818070590496063, \"TOKEN\": \"10021\"}, {\"X\": -2.6038897037506104, \"Y\": 3.1688544750213623, \"TOKEN\": \"guralnick\"}, {\"X\": 2.862842321395874, \"Y\": -0.9471113085746765, \"TOKEN\": \"ravil\"}, {\"X\": 3.837254762649536, \"Y\": 0.22416824102401733, \"TOKEN\": \"ashkabad\"}, {\"X\": -2.4897007942199707, \"Y\": -0.535322368144989, \"TOKEN\": \"71b\"}, {\"X\": -0.11340179294347763, \"Y\": -3.31974196434021, \"TOKEN\": \"italian-style\"}, {\"X\": 2.3246278762817383, \"Y\": 0.72246915102005, \"TOKEN\": \"rexburg\"}, {\"X\": -4.062510013580322, \"Y\": -1.1749801635742188, \"TOKEN\": \"ligairi\"}, {\"X\": -1.3149011135101318, \"Y\": 3.7789013385772705, \"TOKEN\": \"irianto\"}, {\"X\": -0.972953200340271, \"Y\": -8.068353652954102, \"TOKEN\": \"natives\"}, {\"X\": -0.8793536424636841, \"Y\": -9.158438682556152, \"TOKEN\": \"propose\"}, {\"X\": 5.807419300079346, \"Y\": -1.1242237091064453, \"TOKEN\": \"causeway\"}, {\"X\": -1.3834006786346436, \"Y\": 6.159331321716309, \"TOKEN\": \"okerlund\"}, {\"X\": -2.1963157653808594, \"Y\": 3.517472743988037, \"TOKEN\": \"criser\"}, {\"X\": -0.9501429200172424, \"Y\": -3.141413927078247, \"TOKEN\": \"executioners\"}, {\"X\": 6.311330795288086, \"Y\": 1.7069753408432007, \"TOKEN\": \"tarapur\"}, {\"X\": -0.25422051548957825, \"Y\": 0.24808605015277863, \"TOKEN\": \"jamai\"}, {\"X\": -0.7199321389198303, \"Y\": -8.022360801696777, \"TOKEN\": \"kissing\"}, {\"X\": 3.1108150482177734, \"Y\": 8.156189918518066, \"TOKEN\": \"pecan\"}, {\"X\": -6.277740478515625, \"Y\": -1.6113872528076172, \"TOKEN\": \"euro148\"}, {\"X\": 4.97318172454834, \"Y\": 1.97905433177948, \"TOKEN\": \"keihan\"}, {\"X\": -4.91208553314209, \"Y\": -6.045576095581055, \"TOKEN\": \"129\"}, {\"X\": -9.948871612548828, \"Y\": -1.787086844444275, \"TOKEN\": \"ebdane\"}, {\"X\": -7.448671340942383, \"Y\": -3.234501600265503, \"TOKEN\": \"92-91\"}, {\"X\": 1.6645793914794922, \"Y\": -8.078573226928711, \"TOKEN\": \"installable\"}, {\"X\": -0.24389813840389252, \"Y\": 6.909463405609131, \"TOKEN\": \"karthi\"}, {\"X\": -1.7906500101089478, \"Y\": 2.2799665927886963, \"TOKEN\": \"cobbold\"}, {\"X\": 0.7094672322273254, \"Y\": 0.39857903122901917, \"TOKEN\": \"calcitonin\"}, {\"X\": -2.23776912689209, \"Y\": 3.3149561882019043, \"TOKEN\": \"schagrin\"}, {\"X\": 2.6529552936553955, \"Y\": -5.898869514465332, \"TOKEN\": \"papules\"}, {\"X\": 3.4776148796081543, \"Y\": -0.551689863204956, \"TOKEN\": \"lulworth\"}, {\"X\": -0.9632366299629211, \"Y\": -0.5502599477767944, \"TOKEN\": \"kichaka\"}, {\"X\": 0.8452842831611633, \"Y\": -0.5509284138679504, \"TOKEN\": \"cafos\"}, {\"X\": -0.49672985076904297, \"Y\": 0.788371205329895, \"TOKEN\": \"nannerl\"}, {\"X\": -3.3397350311279297, \"Y\": -3.647258758544922, \"TOKEN\": \"b-2s\"}, {\"X\": -2.193847894668579, \"Y\": -8.117020606994629, \"TOKEN\": \"travelled\"}, {\"X\": -5.180981636047363, \"Y\": -2.682309865951538, \"TOKEN\": \"13.71\"}, {\"X\": 4.48861026763916, \"Y\": -8.188952445983887, \"TOKEN\": \"collects\"}, {\"X\": 1.435855507850647, \"Y\": 2.0431392192840576, \"TOKEN\": \"tsuga\"}, {\"X\": -5.652712345123291, \"Y\": -0.17961223423480988, \"TOKEN\": \"simi\"}, {\"X\": 5.02488899230957, \"Y\": -5.167140483856201, \"TOKEN\": \"14-track\"}, {\"X\": 1.3386422395706177, \"Y\": 8.034041404724121, \"TOKEN\": \"encke\"}, {\"X\": 4.812539100646973, \"Y\": 1.5834155082702637, \"TOKEN\": \"broadhall\"}, {\"X\": 5.439513683319092, \"Y\": -2.229078769683838, \"TOKEN\": \"swains\"}, {\"X\": 6.114960193634033, \"Y\": -6.124338626861572, \"TOKEN\": \"telefonos\"}, {\"X\": 0.06373107433319092, \"Y\": 1.6566195487976074, \"TOKEN\": \"rubaiyat\"}, {\"X\": -2.7400879859924316, \"Y\": 5.087080478668213, \"TOKEN\": \"pushkov\"}, {\"X\": -0.07013393938541412, \"Y\": -4.732158184051514, \"TOKEN\": \"beheadings\"}, {\"X\": -2.1252281665802, \"Y\": 3.6633729934692383, \"TOKEN\": \"raffetto\"}, {\"X\": 1.603337287902832, \"Y\": 0.4667321741580963, \"TOKEN\": \"corps.\"}, {\"X\": 2.1413538455963135, \"Y\": -0.35900381207466125, \"TOKEN\": \"sandag\"}, {\"X\": -6.919590473175049, \"Y\": -3.590670108795166, \"TOKEN\": \"42-40\"}, {\"X\": -0.24297955632209778, \"Y\": 3.0716702938079834, \"TOKEN\": \"keyu\"}, {\"X\": -4.884461402893066, \"Y\": -2.813934564590454, \"TOKEN\": \"26.30\"}, {\"X\": 3.7382538318634033, \"Y\": 2.5810766220092773, \"TOKEN\": \"catapano\"}, {\"X\": 1.2835067510604858, \"Y\": -0.692736804485321, \"TOKEN\": \"measurability\"}, {\"X\": -3.5265982151031494, \"Y\": 7.443506717681885, \"TOKEN\": \"aptidon\"}, {\"X\": 0.23777738213539124, \"Y\": 0.7739425897598267, \"TOKEN\": \"fzs\"}, {\"X\": -8.624239921569824, \"Y\": -0.728956401348114, \"TOKEN\": \"11:15\"}, {\"X\": -2.785381317138672, \"Y\": -1.7998722791671753, \"TOKEN\": \"m1917\"}, {\"X\": -0.7204879522323608, \"Y\": 1.8972127437591553, \"TOKEN\": \"tanii\"}, {\"X\": 3.6663413047790527, \"Y\": -0.41577649116516113, \"TOKEN\": \"gandia\"}, {\"X\": -6.841984272003174, \"Y\": 0.24141429364681244, \"TOKEN\": \"17-kilometer\"}, {\"X\": -4.096613883972168, \"Y\": -2.8554160594940186, \"TOKEN\": \"306.5\"}, {\"X\": 2.670663356781006, \"Y\": 0.7900027632713318, \"TOKEN\": \"kaset\"}, {\"X\": -2.119013786315918, \"Y\": -3.4488062858581543, \"TOKEN\": \"mid-year\"}, {\"X\": 5.105766773223877, \"Y\": -5.252553462982178, \"TOKEN\": \"re-issued\"}, {\"X\": -1.086676001548767, \"Y\": 4.170971870422363, \"TOKEN\": \"manirakiza\"}, {\"X\": -2.270230770111084, \"Y\": -3.539963960647583, \"TOKEN\": \"disruptions\"}, {\"X\": 1.1957963705062866, \"Y\": -5.690578937530518, \"TOKEN\": \"coarsely\"}, {\"X\": -0.8462870717048645, \"Y\": -0.4642387926578522, \"TOKEN\": \"g\\u00e1is\"}, {\"X\": 2.2787022590637207, \"Y\": 1.7663187980651855, \"TOKEN\": \"pada\"}, {\"X\": -1.428406834602356, \"Y\": -3.1893489360809326, \"TOKEN\": \"toxicants\"}, {\"X\": -1.8214164972305298, \"Y\": -9.06258487701416, \"TOKEN\": \"struck\"}, {\"X\": -5.760387897491455, \"Y\": 5.690589427947998, \"TOKEN\": \"avonside\"}, {\"X\": 1.9689792394638062, \"Y\": -4.223338603973389, \"TOKEN\": \"thereof\"}, {\"X\": 2.6904044151306152, \"Y\": -3.298159122467041, \"TOKEN\": \"waymarked\"}, {\"X\": 3.053459644317627, \"Y\": 8.067951202392578, \"TOKEN\": \"biscotti\"}, {\"X\": -1.9937199354171753, \"Y\": 4.439645290374756, \"TOKEN\": \"rossman\"}, {\"X\": -2.1528289318084717, \"Y\": 1.5213440656661987, \"TOKEN\": \"boorstin\"}, {\"X\": 1.2292499542236328, \"Y\": 1.752335786819458, \"TOKEN\": \"guatemalensis\"}, {\"X\": 1.3027619123458862, \"Y\": -3.4375, \"TOKEN\": \"imaginable\"}, {\"X\": -2.2630369663238525, \"Y\": 2.335665464401245, \"TOKEN\": \"dubos\"}, {\"X\": 0.11845490336418152, \"Y\": -7.59114408493042, \"TOKEN\": \"swear\"}, {\"X\": 2.546736717224121, \"Y\": -3.467360258102417, \"TOKEN\": \"left-handed\"}, {\"X\": 5.019492149353027, \"Y\": 1.6831508874893188, \"TOKEN\": \"mbh\"}, {\"X\": 1.679503321647644, \"Y\": 2.1367061138153076, \"TOKEN\": \"noorden\"}, {\"X\": -4.311694145202637, \"Y\": 4.415222644805908, \"TOKEN\": \"coello\"}, {\"X\": 1.1749671697616577, \"Y\": -2.7204339504241943, \"TOKEN\": \"mideastern\"}, {\"X\": 1.749215006828308, \"Y\": -3.343226671218872, \"TOKEN\": \"disgorging\"}, {\"X\": -2.9806127548217773, \"Y\": -8.406304359436035, \"TOKEN\": \"fullback\"}, {\"X\": -0.14392448961734772, \"Y\": 1.2428089380264282, \"TOKEN\": \"ltc\"}, {\"X\": -2.0250751972198486, \"Y\": 2.934821128845215, \"TOKEN\": \"groeschel\"}, {\"X\": -1.9623112678527832, \"Y\": 2.382704734802246, \"TOKEN\": \"bithell\"}, {\"X\": 6.771389007568359, \"Y\": -1.4992843866348267, \"TOKEN\": \"superlobbyist\"}, {\"X\": -0.797808051109314, \"Y\": 4.187891006469727, \"TOKEN\": \"prestigioso\"}, {\"X\": 7.699285984039307, \"Y\": -0.528725802898407, \"TOKEN\": \"killington\"}, {\"X\": -0.16953350603580475, \"Y\": -1.7994614839553833, \"TOKEN\": \"cincinnati-based\"}, {\"X\": -3.4184863567352295, \"Y\": -1.9050989151000977, \"TOKEN\": \"236.00\"}, {\"X\": -0.9914904236793518, \"Y\": -9.189492225646973, \"TOKEN\": \"refusal\"}, {\"X\": -3.6150434017181396, \"Y\": -3.6380913257598877, \"TOKEN\": \"50million\"}, {\"X\": -3.038712739944458, \"Y\": -4.873974323272705, \"TOKEN\": \"hooky\"}, {\"X\": -4.0496506690979, \"Y\": -4.476826190948486, \"TOKEN\": \"7,700\"}, {\"X\": 3.4824564456939697, \"Y\": -0.1638001650571823, \"TOKEN\": \"hobbiton\"}, {\"X\": -1.4629716873168945, \"Y\": 4.022221565246582, \"TOKEN\": \"seow\"}, {\"X\": -2.740269184112549, \"Y\": 1.1618043184280396, \"TOKEN\": \"nekrasov\"}, {\"X\": -0.6018377542495728, \"Y\": 3.7575347423553467, \"TOKEN\": \"muklas\"}, {\"X\": -0.3836546242237091, \"Y\": 4.162849426269531, \"TOKEN\": \"signalman\"}, {\"X\": 3.2581915855407715, \"Y\": -2.808248519897461, \"TOKEN\": \"aquariums\"}, {\"X\": 2.9686169624328613, \"Y\": 2.5905933380126953, \"TOKEN\": \"mexbol\"}, {\"X\": -0.44366738200187683, \"Y\": -3.0295846462249756, \"TOKEN\": \"todd-ao\"}, {\"X\": -2.4772753715515137, \"Y\": -5.6976518630981445, \"TOKEN\": \"gaona\"}, {\"X\": 2.819679021835327, \"Y\": -2.06321120262146, \"TOKEN\": \"life-saving\"}, {\"X\": -1.4665888547897339, \"Y\": -1.857283353805542, \"TOKEN\": \"phils\"}, {\"X\": -0.15199004113674164, \"Y\": -0.657976508140564, \"TOKEN\": \"merryfield\"}, {\"X\": -4.810243129730225, \"Y\": -5.421699047088623, \"TOKEN\": \"494\"}, {\"X\": -0.5519276261329651, \"Y\": 0.5948353409767151, \"TOKEN\": \"tendzin\"}, {\"X\": -2.0670101642608643, \"Y\": 1.6410971879959106, \"TOKEN\": \"kalemba\"}, {\"X\": -0.6046299934387207, \"Y\": -0.865059494972229, \"TOKEN\": \"bayi\"}, {\"X\": 1.8749208450317383, \"Y\": -0.0905420258641243, \"TOKEN\": \"topa\"}, {\"X\": -2.1665358543395996, \"Y\": -1.772615909576416, \"TOKEN\": \"adanaspor\"}, {\"X\": 3.1262080669403076, \"Y\": -0.14908729493618011, \"TOKEN\": \"shenzong\"}, {\"X\": -4.448359489440918, \"Y\": -2.4569058418273926, \"TOKEN\": \"76.61\"}, {\"X\": -2.5230460166931152, \"Y\": -0.3729492425918579, \"TOKEN\": \"mns\"}, {\"X\": 3.835087299346924, \"Y\": 1.9807392358779907, \"TOKEN\": \"grisoft\"}, {\"X\": -2.5818519592285156, \"Y\": 1.4603825807571411, \"TOKEN\": \"tallarico\"}, {\"X\": -0.8298438191413879, \"Y\": -2.680429220199585, \"TOKEN\": \"toga\"}, {\"X\": 0.8884772658348083, \"Y\": 1.3626350164413452, \"TOKEN\": \"premer\"}, {\"X\": 4.208885669708252, \"Y\": 3.8347411155700684, \"TOKEN\": \"muzzy\"}, {\"X\": 1.4628297090530396, \"Y\": -0.5119044184684753, \"TOKEN\": \"filmfour\"}, {\"X\": 2.592447280883789, \"Y\": 1.4639030694961548, \"TOKEN\": \"novomoskovsk\"}, {\"X\": 3.2640812397003174, \"Y\": -4.048592567443848, \"TOKEN\": \"delicacies\"}, {\"X\": -5.595730781555176, \"Y\": 3.8600316047668457, \"TOKEN\": \"niedermayer\"}, {\"X\": 1.036428451538086, \"Y\": -2.0016932487487793, \"TOKEN\": \"ia-32\"}, {\"X\": -4.847442150115967, \"Y\": 2.5030829906463623, \"TOKEN\": \"alington\"}, {\"X\": 0.3960150182247162, \"Y\": -1.8153494596481323, \"TOKEN\": \"derogation\"}, {\"X\": 4.441334247589111, \"Y\": -3.0166475772857666, \"TOKEN\": \"mappings\"}, {\"X\": 3.3564324378967285, \"Y\": -3.0993213653564453, \"TOKEN\": \"deregulate\"}, {\"X\": -0.2778718173503876, \"Y\": 5.011848449707031, \"TOKEN\": \"steelman\"}, {\"X\": -1.1096595525741577, \"Y\": 4.876177787780762, \"TOKEN\": \"wlodarczyk\"}, {\"X\": 0.12329046428203583, \"Y\": -0.1775503307580948, \"TOKEN\": \"bancnet\"}, {\"X\": -0.6590400338172913, \"Y\": 1.914136290550232, \"TOKEN\": \"farkle\"}, {\"X\": -6.392557621002197, \"Y\": 4.624783515930176, \"TOKEN\": \"documentos\"}, {\"X\": -2.9152944087982178, \"Y\": -8.52668571472168, \"TOKEN\": \"kaiser\"}, {\"X\": 3.3763082027435303, \"Y\": 5.1104278564453125, \"TOKEN\": \"shafiqul\"}, {\"X\": 5.272067070007324, \"Y\": -0.44466620683670044, \"TOKEN\": \"coahoma\"}, {\"X\": -3.879777431488037, \"Y\": 6.695119857788086, \"TOKEN\": \"plantier\"}, {\"X\": 1.141663670539856, \"Y\": 1.964657187461853, \"TOKEN\": \"annularis\"}, {\"X\": -4.070957183837891, \"Y\": 1.8096070289611816, \"TOKEN\": \"dullea\"}, {\"X\": 0.9363947510719299, \"Y\": 0.6268678903579712, \"TOKEN\": \"thelonius\"}, {\"X\": 0.7482510209083557, \"Y\": 1.13161301612854, \"TOKEN\": \"koray\"}, {\"X\": -2.7320518493652344, \"Y\": -6.1175994873046875, \"TOKEN\": \"jes\\u00fas\"}, {\"X\": -4.615527153015137, \"Y\": -2.233876943588257, \"TOKEN\": \"62-2\"}, {\"X\": -1.0430642366409302, \"Y\": -3.8677420616149902, \"TOKEN\": \"nanomaterials\"}, {\"X\": -1.6759893894195557, \"Y\": 0.6359914541244507, \"TOKEN\": \"pisanello\"}, {\"X\": 4.859740734100342, \"Y\": -3.7387988567352295, \"TOKEN\": \"strontium\"}, {\"X\": -4.023574352264404, \"Y\": 0.6362406611442566, \"TOKEN\": \"scabies\"}, {\"X\": 2.2517852783203125, \"Y\": 5.896418571472168, \"TOKEN\": \"rostropovich\"}, {\"X\": -0.7938376069068909, \"Y\": -2.5403079986572266, \"TOKEN\": \"constituency-based\"}, {\"X\": -8.61225414276123, \"Y\": -0.7566255331039429, \"TOKEN\": \"9:38\"}, {\"X\": 2.963097333908081, \"Y\": 2.1326944828033447, \"TOKEN\": \"c\\u1ea9m\"}, {\"X\": -0.7300595045089722, \"Y\": -8.26671028137207, \"TOKEN\": \"unconscious\"}, {\"X\": 0.42133504152297974, \"Y\": 1.4644076824188232, \"TOKEN\": \"murge\"}, {\"X\": 1.2735881805419922, \"Y\": 2.5595788955688477, \"TOKEN\": \"ferit\"}, {\"X\": 1.5001347064971924, \"Y\": 0.2353125363588333, \"TOKEN\": \"partenope\"}, {\"X\": -5.115128517150879, \"Y\": -6.939947128295898, \"TOKEN\": \"seventy\"}, {\"X\": -1.194986343383789, \"Y\": 1.2049715518951416, \"TOKEN\": \"dikko\"}, {\"X\": -1.04288911819458, \"Y\": -4.995075702667236, \"TOKEN\": \"groggily\"}, {\"X\": 4.998645305633545, \"Y\": 4.15200662612915, \"TOKEN\": \"acetone\"}, {\"X\": -1.2222729921340942, \"Y\": 2.815281867980957, \"TOKEN\": \"gaydos\"}, {\"X\": 0.29641857743263245, \"Y\": -1.672248125076294, \"TOKEN\": \"safe-haven\"}, {\"X\": -4.417812824249268, \"Y\": 5.17587947845459, \"TOKEN\": \"maske\"}, {\"X\": 0.6038616895675659, \"Y\": -0.7731325626373291, \"TOKEN\": \"s-wave\"}, {\"X\": 3.0272974967956543, \"Y\": 1.7424830198287964, \"TOKEN\": \"bulford\"}, {\"X\": -4.2731475830078125, \"Y\": -0.3746960759162903, \"TOKEN\": \"sfh\"}, {\"X\": -1.3391470909118652, \"Y\": 0.40712589025497437, \"TOKEN\": \"rattakul\"}, {\"X\": 0.19170904159545898, \"Y\": 0.9529939293861389, \"TOKEN\": \"valentins\"}, {\"X\": -1.1005418300628662, \"Y\": -0.5158998966217041, \"TOKEN\": \"demak\"}, {\"X\": 0.17593877017498016, \"Y\": -0.39454036951065063, \"TOKEN\": \"busra\"}, {\"X\": 1.6893177032470703, \"Y\": 4.394657611846924, \"TOKEN\": \"malacateco\"}, {\"X\": 1.677506446838379, \"Y\": 6.029801845550537, \"TOKEN\": \"forestiere\"}, {\"X\": -1.0067800283432007, \"Y\": -1.8666281700134277, \"TOKEN\": \"borchin\"}, {\"X\": -3.233591318130493, \"Y\": 4.531583786010742, \"TOKEN\": \"brockman\"}, {\"X\": -3.344428539276123, \"Y\": -3.47175931930542, \"TOKEN\": \"winnings\"}, {\"X\": 1.6078803539276123, \"Y\": -3.9805030822753906, \"TOKEN\": \"hollowing\"}, {\"X\": -1.081223487854004, \"Y\": -8.880887985229492, \"TOKEN\": \"purely\"}, {\"X\": -4.550213813781738, \"Y\": -3.7995758056640625, \"TOKEN\": \"gwh\"}, {\"X\": 3.2582364082336426, \"Y\": 6.709224224090576, \"TOKEN\": \"taibbi\"}, {\"X\": 1.086448073387146, \"Y\": -2.5141003131866455, \"TOKEN\": \"onlive\"}, {\"X\": 0.31039923429489136, \"Y\": 5.568732738494873, \"TOKEN\": \"pousner\"}, {\"X\": 2.0246338844299316, \"Y\": -1.1224008798599243, \"TOKEN\": \"cockfight\"}, {\"X\": -1.6600502729415894, \"Y\": 1.9480254650115967, \"TOKEN\": \"krysiak\"}, {\"X\": -3.083824872970581, \"Y\": 2.7881741523742676, \"TOKEN\": \"nyrop\"}, {\"X\": -1.0700618028640747, \"Y\": -0.7322286367416382, \"TOKEN\": \"malwa\"}, {\"X\": -4.159036159515381, \"Y\": -4.587136745452881, \"TOKEN\": \"1,105\"}, {\"X\": -4.671017646789551, \"Y\": -2.5079236030578613, \"TOKEN\": \"42.07\"}, {\"X\": -5.062933921813965, \"Y\": 4.540100574493408, \"TOKEN\": \"semin\"}, {\"X\": -3.2309868335723877, \"Y\": 0.126808300614357, \"TOKEN\": \"apco\"}, {\"X\": 2.6309595108032227, \"Y\": -1.0261399745941162, \"TOKEN\": \"focusrite\"}, {\"X\": -3.430574417114258, \"Y\": 7.283551216125488, \"TOKEN\": \"satow\"}, {\"X\": -0.8812133073806763, \"Y\": 1.4564248323440552, \"TOKEN\": \"butron\"}, {\"X\": 0.6711984872817993, \"Y\": 4.6015849113464355, \"TOKEN\": \"tassan\"}, {\"X\": 6.5776753425598145, \"Y\": -3.900426149368286, \"TOKEN\": \"hellhammer\"}, {\"X\": 1.819806694984436, \"Y\": -1.522576093673706, \"TOKEN\": \"noninvasive\"}, {\"X\": 5.468145370483398, \"Y\": 6.70731258392334, \"TOKEN\": \"shtahd\"}, {\"X\": 8.271739959716797, \"Y\": -1.882555603981018, \"TOKEN\": \"kgo\"}, {\"X\": -2.919412136077881, \"Y\": -9.161078453063965, \"TOKEN\": \"ii\"}, {\"X\": -2.6284570693969727, \"Y\": 3.7061285972595215, \"TOKEN\": \"halfon\"}, {\"X\": -1.9701876640319824, \"Y\": 3.031540870666504, \"TOKEN\": \"norling\"}, {\"X\": -0.339046835899353, \"Y\": 3.9078969955444336, \"TOKEN\": \"svenson\"}, {\"X\": 1.054972529411316, \"Y\": 3.1522343158721924, \"TOKEN\": \"hile\"}, {\"X\": 4.605759143829346, \"Y\": -5.127237319946289, \"TOKEN\": \"two-hour\"}, {\"X\": -4.168739318847656, \"Y\": 3.0996475219726562, \"TOKEN\": \"tomkins\"}, {\"X\": 1.0396603345870972, \"Y\": 6.4276838302612305, \"TOKEN\": \"nivel\"}, {\"X\": -2.6692113876342773, \"Y\": -3.389967918395996, \"TOKEN\": \"2,938\"}, {\"X\": 0.5862517356872559, \"Y\": -4.577463626861572, \"TOKEN\": \"floater\"}, {\"X\": -2.0495221614837646, \"Y\": 5.892630577087402, \"TOKEN\": \"eluard\"}, {\"X\": 0.7508294582366943, \"Y\": -5.6831793785095215, \"TOKEN\": \"blackened\"}, {\"X\": -1.3467884063720703, \"Y\": -8.101470947265625, \"TOKEN\": \"merit\"}, {\"X\": 2.117554187774658, \"Y\": 5.68432092666626, \"TOKEN\": \"vishneva\"}, {\"X\": -5.305400371551514, \"Y\": -2.76455020904541, \"TOKEN\": \"5.82\"}, {\"X\": 1.2372773885726929, \"Y\": 1.7289574146270752, \"TOKEN\": \"sadh\"}, {\"X\": 1.7573771476745605, \"Y\": 0.8547850847244263, \"TOKEN\": \"booke\"}, {\"X\": -0.8809714317321777, \"Y\": 4.3066487312316895, \"TOKEN\": \"soubert\"}, {\"X\": -0.38279086351394653, \"Y\": 1.870775818824768, \"TOKEN\": \"gu\\u00f0ni\"}, {\"X\": 1.1602061986923218, \"Y\": -4.30076265335083, \"TOKEN\": \"angularity\"}, {\"X\": 1.6961009502410889, \"Y\": -0.01280960813164711, \"TOKEN\": \"kandu\"}, {\"X\": 1.544854760169983, \"Y\": -0.06145387142896652, \"TOKEN\": \"enn\"}, {\"X\": -0.5591149926185608, \"Y\": -0.38513845205307007, \"TOKEN\": \"tingwell\"}, {\"X\": 1.2838950157165527, \"Y\": 4.17857551574707, \"TOKEN\": \"owairan\"}, {\"X\": -5.057081699371338, \"Y\": -1.5584626197814941, \"TOKEN\": \"hryvnia\"}, {\"X\": -4.881701946258545, \"Y\": -6.314428806304932, \"TOKEN\": \"34\"}, {\"X\": 4.862727642059326, \"Y\": -1.9755140542984009, \"TOKEN\": \"pergau\"}, {\"X\": 6.124482154846191, \"Y\": -0.4199434518814087, \"TOKEN\": \"sarbisheh\"}, {\"X\": -1.1800422668457031, \"Y\": 7.301516056060791, \"TOKEN\": \"kittani\"}, {\"X\": -0.5295019745826721, \"Y\": -9.31401538848877, \"TOKEN\": \"upgrade\"}, {\"X\": 1.97218918800354, \"Y\": 1.7323471307754517, \"TOKEN\": \"chahta\"}, {\"X\": 1.0159627199172974, \"Y\": 3.9207890033721924, \"TOKEN\": \"nurme\"}, {\"X\": -4.991239070892334, \"Y\": -2.5696282386779785, \"TOKEN\": \"16.53\"}, {\"X\": -7.359493255615234, \"Y\": -3.1558730602264404, \"TOKEN\": \"82-76\"}, {\"X\": 1.1850130558013916, \"Y\": 4.420103549957275, \"TOKEN\": \"saadawi\"}, {\"X\": -1.3527672290802002, \"Y\": 3.0749027729034424, \"TOKEN\": \"nahodha\"}, {\"X\": 1.71190345287323, \"Y\": -2.4980437755584717, \"TOKEN\": \"prizefighting\"}, {\"X\": 2.4014394283294678, \"Y\": 3.8357133865356445, \"TOKEN\": \"burlsworth\"}, {\"X\": 2.3085083961486816, \"Y\": 0.08197923749685287, \"TOKEN\": \"tarki\"}, {\"X\": -3.289616823196411, \"Y\": 3.509641647338867, \"TOKEN\": \"kandiah\"}, {\"X\": -2.51906156539917, \"Y\": -3.895031213760376, \"TOKEN\": \"corks\"}, {\"X\": -3.1608612537384033, \"Y\": -4.480015754699707, \"TOKEN\": \"tassie\"}, {\"X\": 0.8337767124176025, \"Y\": 0.5694953203201294, \"TOKEN\": \"monosomy\"}, {\"X\": -0.009745223447680473, \"Y\": -2.0185303688049316, \"TOKEN\": \"medi\\u00e6val\"}, {\"X\": 0.7302379608154297, \"Y\": -1.8833659887313843, \"TOKEN\": \"permeability\"}, {\"X\": -0.30581384897232056, \"Y\": -2.6542701721191406, \"TOKEN\": \"dandyish\"}, {\"X\": 1.1788862943649292, \"Y\": -0.7460550665855408, \"TOKEN\": \"uncured\"}, {\"X\": -7.23287296295166, \"Y\": 2.715909719467163, \"TOKEN\": \"hhhh\"}, {\"X\": 3.0373504161834717, \"Y\": 4.827491283416748, \"TOKEN\": \"rilo\"}, {\"X\": 1.7842187881469727, \"Y\": -1.0651041269302368, \"TOKEN\": \"indianness\"}, {\"X\": 4.201224327087402, \"Y\": -0.09897583723068237, \"TOKEN\": \"zhuolu\"}, {\"X\": 5.075993061065674, \"Y\": -2.921509265899658, \"TOKEN\": \"milchan\"}, {\"X\": -0.10608875006437302, \"Y\": -6.241819858551025, \"TOKEN\": \"in-between\"}, {\"X\": 4.470709800720215, \"Y\": -6.507388591766357, \"TOKEN\": \"haz-mat\"}, {\"X\": 1.4062434434890747, \"Y\": 0.9505739212036133, \"TOKEN\": \"greenshoe\"}, {\"X\": -0.9447884559631348, \"Y\": 2.5614683628082275, \"TOKEN\": \"hofstetter\"}, {\"X\": 1.488736629486084, \"Y\": -2.290353536605835, \"TOKEN\": \"260th\"}, {\"X\": -2.386516809463501, \"Y\": 2.8425345420837402, \"TOKEN\": \"chikowore\"}, {\"X\": -0.45239830017089844, \"Y\": 2.94528865814209, \"TOKEN\": \"troiani\"}, {\"X\": -1.718317985534668, \"Y\": 3.8608460426330566, \"TOKEN\": \"oryem\"}, {\"X\": -0.15556098520755768, \"Y\": 1.1889944076538086, \"TOKEN\": \"s.k.warne\"}, {\"X\": 1.070032000541687, \"Y\": 4.1476922035217285, \"TOKEN\": \"tsakopoulos\"}, {\"X\": 7.7211594581604, \"Y\": 0.3034469187259674, \"TOKEN\": \"marigolds\"}, {\"X\": -4.352896213531494, \"Y\": -2.456860303878784, \"TOKEN\": \"41.54\"}, {\"X\": 0.8945838212966919, \"Y\": 0.980741024017334, \"TOKEN\": \"g.o.p.\"}, {\"X\": -3.204007863998413, \"Y\": 2.9580891132354736, \"TOKEN\": \"scheinin\"}, {\"X\": 2.341686725616455, \"Y\": 2.451073408126831, \"TOKEN\": \"ikoma\"}, {\"X\": -1.417566180229187, \"Y\": 0.9497506618499756, \"TOKEN\": \"boediono\"}, {\"X\": 4.4799580574035645, \"Y\": 0.5831148624420166, \"TOKEN\": \"varberg\"}, {\"X\": -3.8392488956451416, \"Y\": -1.1024670600891113, \"TOKEN\": \"nns4\"}, {\"X\": 2.874258041381836, \"Y\": -3.4959559440612793, \"TOKEN\": \"proto\"}, {\"X\": -2.452381134033203, \"Y\": -9.124780654907227, \"TOKEN\": \"commander\"}, {\"X\": -4.597843647003174, \"Y\": -2.4284822940826416, \"TOKEN\": \"69.40\"}, {\"X\": 2.878775119781494, \"Y\": -4.0228962898254395, \"TOKEN\": \"herbals\"}, {\"X\": -0.6012671589851379, \"Y\": 1.227659821510315, \"TOKEN\": \"verlag\"}, {\"X\": 4.032949924468994, \"Y\": -6.110406875610352, \"TOKEN\": \"swissport\"}, {\"X\": 4.4674601554870605, \"Y\": 3.4577465057373047, \"TOKEN\": \"acela\"}, {\"X\": 1.6066256761550903, \"Y\": 3.666038990020752, \"TOKEN\": \"chilo\"}, {\"X\": 4.593764781951904, \"Y\": -1.3087010383605957, \"TOKEN\": \"trotu\\u015f\"}, {\"X\": -1.0355361700057983, \"Y\": 3.4826290607452393, \"TOKEN\": \"kapell\"}, {\"X\": -0.6842846274375916, \"Y\": 1.8705666065216064, \"TOKEN\": \"giovanelli\"}, {\"X\": -2.21799373626709, \"Y\": 6.618083953857422, \"TOKEN\": \"501s\"}, {\"X\": -1.8392850160598755, \"Y\": 4.597718715667725, \"TOKEN\": \"rideaux\"}, {\"X\": -0.6792501211166382, \"Y\": 5.3980865478515625, \"TOKEN\": \"esty\"}, {\"X\": -0.47284871339797974, \"Y\": 0.7631514072418213, \"TOKEN\": \"nspo\"}, {\"X\": 1.2419902086257935, \"Y\": -3.3312320709228516, \"TOKEN\": \"interrelationships\"}, {\"X\": 1.246817708015442, \"Y\": 1.87595796585083, \"TOKEN\": \"cocha\"}, {\"X\": 3.2206740379333496, \"Y\": 4.5073442459106445, \"TOKEN\": \"dvu\"}, {\"X\": 5.58833122253418, \"Y\": 3.108297348022461, \"TOKEN\": \"oseberg\"}, {\"X\": 2.7200920581817627, \"Y\": 4.432415008544922, \"TOKEN\": \"crooned\"}, {\"X\": 5.333977699279785, \"Y\": -2.800300121307373, \"TOKEN\": \"bluiett\"}, {\"X\": -1.1097900867462158, \"Y\": -6.551793575286865, \"TOKEN\": \"harass\"}, {\"X\": 0.3723612427711487, \"Y\": 3.991133451461792, \"TOKEN\": \"azema\"}, {\"X\": 3.5895023345947266, \"Y\": 0.385659396648407, \"TOKEN\": \"h\\u00f6fn\"}, {\"X\": 4.231079578399658, \"Y\": 1.2260456085205078, \"TOKEN\": \"balwyn\"}, {\"X\": 0.013071341440081596, \"Y\": 2.1315860748291016, \"TOKEN\": \"kohistani\"}, {\"X\": -0.20703157782554626, \"Y\": 7.045194149017334, \"TOKEN\": \"pooja\"}, {\"X\": 1.844779133796692, \"Y\": -4.76218318939209, \"TOKEN\": \"salles\"}, {\"X\": -0.17349304258823395, \"Y\": -3.0004663467407227, \"TOKEN\": \"all-in-one\"}, {\"X\": -2.45589542388916, \"Y\": -4.457401752471924, \"TOKEN\": \"nonviolently\"}, {\"X\": 0.5506600737571716, \"Y\": -3.6129353046417236, \"TOKEN\": \"implosions\"}, {\"X\": 6.069182872772217, \"Y\": 1.2773820161819458, \"TOKEN\": \"kuibyshev\"}, {\"X\": -1.150214672088623, \"Y\": 0.05170446261763573, \"TOKEN\": \"muammer\"}, {\"X\": 6.697784423828125, \"Y\": 0.9156460762023926, \"TOKEN\": \"beauce\"}, {\"X\": -0.6196639537811279, \"Y\": 0.62519770860672, \"TOKEN\": \"adianto\"}, {\"X\": -1.5635480880737305, \"Y\": 4.767997741699219, \"TOKEN\": \"girman\"}, {\"X\": -2.652425527572632, \"Y\": -2.6361212730407715, \"TOKEN\": \"sveaborg\"}, {\"X\": 4.194564342498779, \"Y\": -0.8596717119216919, \"TOKEN\": \"angat\"}, {\"X\": -6.995305061340332, \"Y\": -2.903717517852783, \"TOKEN\": \"to-1\"}, {\"X\": -1.1004114151000977, \"Y\": -1.2975349426269531, \"TOKEN\": \"turenne\"}, {\"X\": 5.706786632537842, \"Y\": -0.3398704528808594, \"TOKEN\": \"jarahi\"}, {\"X\": 0.5377272963523865, \"Y\": 0.9364517331123352, \"TOKEN\": \"nhmuk\"}, {\"X\": 3.404829502105713, \"Y\": -5.74696683883667, \"TOKEN\": \"rupavahini\"}, {\"X\": -0.04870009794831276, \"Y\": -3.340280771255493, \"TOKEN\": \"stablemates\"}, {\"X\": -5.108724594116211, \"Y\": -2.6798760890960693, \"TOKEN\": \"16.16\"}, {\"X\": -5.956661701202393, \"Y\": -4.545639991760254, \"TOKEN\": \"guta\"}, {\"X\": 0.19754624366760254, \"Y\": 3.8884105682373047, \"TOKEN\": \"kulasegaran\"}, {\"X\": 2.145493984222412, \"Y\": -0.8561220169067383, \"TOKEN\": \"moonies\"}, {\"X\": -0.21124587953090668, \"Y\": 0.3914722800254822, \"TOKEN\": \"maraini\"}, {\"X\": 3.0077733993530273, \"Y\": 0.38521021604537964, \"TOKEN\": \"mandal\"}, {\"X\": -6.494798183441162, \"Y\": -4.885110855102539, \"TOKEN\": \"aldar\"}, {\"X\": -0.5986002683639526, \"Y\": 3.8703718185424805, \"TOKEN\": \"bazile\"}, {\"X\": -2.620591878890991, \"Y\": -3.264629602432251, \"TOKEN\": \"colonoscopies\"}, {\"X\": 1.686784029006958, \"Y\": 1.438027262687683, \"TOKEN\": \"com\\u00fan\"}, {\"X\": -1.130553960800171, \"Y\": 2.7535550594329834, \"TOKEN\": \"scandone\"}, {\"X\": 1.941821813583374, \"Y\": -6.861609935760498, \"TOKEN\": \"pendant\"}, {\"X\": -4.03463077545166, \"Y\": -1.8448954820632935, \"TOKEN\": \"www.travel.state.gov\"}, {\"X\": 6.179051876068115, \"Y\": 1.3054041862487793, \"TOKEN\": \"cuvette\"}, {\"X\": -2.7711668014526367, \"Y\": 2.265848159790039, \"TOKEN\": \"gilson\"}, {\"X\": -4.686021327972412, \"Y\": 0.4111335575580597, \"TOKEN\": \"nataly\"}, {\"X\": 0.8034059405326843, \"Y\": -2.815089225769043, \"TOKEN\": \"terrorism-related\"}, {\"X\": 1.7463252544403076, \"Y\": -2.204878330230713, \"TOKEN\": \"36-race\"}, {\"X\": -1.6482833623886108, \"Y\": 1.7321457862854004, \"TOKEN\": \"dami\\u00e3o\"}, {\"X\": -2.6319074630737305, \"Y\": 4.366091251373291, \"TOKEN\": \"chepe\"}, {\"X\": 4.164472579956055, \"Y\": -2.634920358657837, \"TOKEN\": \"semifinished\"}, {\"X\": 2.945488214492798, \"Y\": -0.833939254283905, \"TOKEN\": \"boorowa\"}, {\"X\": 0.06075311079621315, \"Y\": 2.010594129562378, \"TOKEN\": \"cotgrave\"}, {\"X\": -2.0336194038391113, \"Y\": -6.182982921600342, \"TOKEN\": \"ant\\u00f3n\"}, {\"X\": 6.904908180236816, \"Y\": 1.4668021202087402, \"TOKEN\": \"maule\"}, {\"X\": -4.311279773712158, \"Y\": -4.506290435791016, \"TOKEN\": \"2,175\"}, {\"X\": 0.30315321683883667, \"Y\": -1.6879658699035645, \"TOKEN\": \"detraction\"}, {\"X\": 4.813814163208008, \"Y\": 0.011311596259474754, \"TOKEN\": \"yongning\"}, {\"X\": 0.1693553626537323, \"Y\": 1.1184046268463135, \"TOKEN\": \"arpeggione\"}, {\"X\": -2.086827278137207, \"Y\": -2.488938331604004, \"TOKEN\": \"a&w\"}, {\"X\": 0.5540039539337158, \"Y\": 7.9195709228515625, \"TOKEN\": \"crispell\"}, {\"X\": -0.9888994097709656, \"Y\": -8.22412395477295, \"TOKEN\": \"outsiders\"}, {\"X\": -4.374548435211182, \"Y\": -2.415771961212158, \"TOKEN\": \"70.10\"}, {\"X\": 2.1012990474700928, \"Y\": -6.385683059692383, \"TOKEN\": \"perforated\"}, {\"X\": 0.8703944683074951, \"Y\": -1.4178818464279175, \"TOKEN\": \"middle-range\"}, {\"X\": 0.8702506422996521, \"Y\": -6.767219066619873, \"TOKEN\": \"wrestles\"}, {\"X\": -4.331688404083252, \"Y\": -2.3908767700195312, \"TOKEN\": \"69.64\"}, {\"X\": 0.9400896430015564, \"Y\": 4.482117176055908, \"TOKEN\": \"hammash\"}, {\"X\": 1.4307796955108643, \"Y\": 0.636262834072113, \"TOKEN\": \"rubber-tyred\"}, {\"X\": 0.6553577184677124, \"Y\": 0.14612507820129395, \"TOKEN\": \"maxpreps\"}, {\"X\": -1.1990442276000977, \"Y\": 1.6068097352981567, \"TOKEN\": \"vidadi\"}, {\"X\": 4.714045524597168, \"Y\": -1.8115394115447998, \"TOKEN\": \"hurlingham\"}, {\"X\": 1.1000795364379883, \"Y\": -1.4936490058898926, \"TOKEN\": \"non-perturbative\"}, {\"X\": 3.98480224609375, \"Y\": -2.490034580230713, \"TOKEN\": \"altech\"}, {\"X\": -1.2926254272460938, \"Y\": 1.9713131189346313, \"TOKEN\": \"roberdeau\"}, {\"X\": 4.8533244132995605, \"Y\": -1.5062776803970337, \"TOKEN\": \"dodder\"}, {\"X\": 4.487529754638672, \"Y\": 0.2724341154098511, \"TOKEN\": \"virac\"}, {\"X\": 2.5471560955047607, \"Y\": 1.4245481491088867, \"TOKEN\": \"dieu\"}, {\"X\": 1.3602975606918335, \"Y\": -0.15022851526737213, \"TOKEN\": \"ufx\"}, {\"X\": -0.9504305124282837, \"Y\": 3.0714590549468994, \"TOKEN\": \"alarc?n\"}, {\"X\": 0.35771459341049194, \"Y\": -3.641653537750244, \"TOKEN\": \"celadon\"}, {\"X\": 7.698513031005859, \"Y\": -4.111000061035156, \"TOKEN\": \"l\\u00f6wenstein-wertheim-rosenberg\"}, {\"X\": -0.451145738363266, \"Y\": -5.438970565795898, \"TOKEN\": \"affably\"}, {\"X\": 6.6513447761535645, \"Y\": -1.0541901588439941, \"TOKEN\": \"olso\"}, {\"X\": -1.0982729196548462, \"Y\": -9.072938919067383, \"TOKEN\": \"essential\"}, {\"X\": 2.2811050415039062, \"Y\": 0.5398857593536377, \"TOKEN\": \"paralia\"}, {\"X\": 0.5219822525978088, \"Y\": 0.04726701229810715, \"TOKEN\": \"piemont\"}, {\"X\": -1.4333624839782715, \"Y\": -4.9982476234436035, \"TOKEN\": \"trussed\"}, {\"X\": -5.705475330352783, \"Y\": -3.6397902965545654, \"TOKEN\": \"98.34\"}, {\"X\": 0.6371405124664307, \"Y\": 0.5953715443611145, \"TOKEN\": \"mastaba\"}, {\"X\": 6.49686861038208, \"Y\": -2.709845542907715, \"TOKEN\": \"pancuronium\"}, {\"X\": -6.091076374053955, \"Y\": -1.6027449369430542, \"TOKEN\": \"euro473\"}, {\"X\": -1.9112588167190552, \"Y\": 2.075207471847534, \"TOKEN\": \"akasaki\"}, {\"X\": -1.4039509296417236, \"Y\": 5.829905033111572, \"TOKEN\": \"ticotin\"}, {\"X\": -1.7346779108047485, \"Y\": -0.544187605381012, \"TOKEN\": \"wadiyar\"}, {\"X\": 1.026168942451477, \"Y\": 5.702484607696533, \"TOKEN\": \"muntazer\"}, {\"X\": -6.757138729095459, \"Y\": -3.6744632720947266, \"TOKEN\": \"8-22\"}, {\"X\": 4.339326858520508, \"Y\": 5.259089946746826, \"TOKEN\": \"torchbearer\"}, {\"X\": 0.3749423623085022, \"Y\": -2.932260036468506, \"TOKEN\": \"eurodif\"}, {\"X\": -2.2650773525238037, \"Y\": 0.6811550259590149, \"TOKEN\": \"zolt\\u00e1n\"}, {\"X\": 5.999904155731201, \"Y\": 4.65025520324707, \"TOKEN\": \"sphinx\"}, {\"X\": 3.5886032581329346, \"Y\": 0.1935572326183319, \"TOKEN\": \"corregimiento\"}, {\"X\": 0.9005707502365112, \"Y\": 4.57661247253418, \"TOKEN\": \"leonie\"}, {\"X\": -2.3321492671966553, \"Y\": -5.57842493057251, \"TOKEN\": \"mckesson\"}, {\"X\": -1.3060590028762817, \"Y\": -0.9550012350082397, \"TOKEN\": \"kandyan\"}, {\"X\": -0.12401764839887619, \"Y\": -6.34977912902832, \"TOKEN\": \"fillers\"}, {\"X\": 1.3391493558883667, \"Y\": 1.9151880741119385, \"TOKEN\": \"rupicapra\"}, {\"X\": -1.922096848487854, \"Y\": -0.5094906091690063, \"TOKEN\": \"vikramaditya\"}, {\"X\": -0.09933177381753922, \"Y\": 3.230682134628296, \"TOKEN\": \"razziq\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='TOKEN'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a21c2f",
   "metadata": {},
   "source": [
    "### Other relationships\n",
    "\n",
    "Beyond cosine similarity, there are other word relationships to explore via vector space math. For example, one way we might model something like a _concept_ is through addition. What would happen if we added to vectors together to create a new vector? Which words would this new vector be closest to in the vector space? Using the `.similar_by_vector()` method, we can find out.\n",
    "\n",
    "```{margin} What this loop does\n",
    "For each concept in our `concepts` dictionary:\n",
    "\n",
    "1. Get its associated pair of words\n",
    "2. Query the model for those words' vectors and add them together to create a new vector\n",
    "3. Find the most similar words to this new vector\n",
    "4. Use a dataframe to display the results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "432fc44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'sand' + 'ocean' (for 'beach')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sand</td>\n",
       "      <td>0.845458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocean</td>\n",
       "      <td>0.845268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.687682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaches</td>\n",
       "      <td>0.667521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waters</td>\n",
       "      <td>0.664894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coastal</td>\n",
       "      <td>0.632485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>water</td>\n",
       "      <td>0.618701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coast</td>\n",
       "      <td>0.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dunes</td>\n",
       "      <td>0.599333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>surface</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0     sand  0.845458\n",
       "1    ocean  0.845268\n",
       "2      sea  0.687682\n",
       "3  beaches  0.667521\n",
       "4   waters  0.664894\n",
       "5  coastal  0.632485\n",
       "6    water  0.618701\n",
       "7    coast  0.604373\n",
       "8    dunes  0.599333\n",
       "9  surface  0.597545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'vacation' + 'room' (for 'hotel')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vacation</td>\n",
       "      <td>0.823460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>room</td>\n",
       "      <td>0.810719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rooms</td>\n",
       "      <td>0.704233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>0.658199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel</td>\n",
       "      <td>0.647865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dining</td>\n",
       "      <td>0.634925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stay</td>\n",
       "      <td>0.617807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apartment</td>\n",
       "      <td>0.616495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>staying</td>\n",
       "      <td>0.615182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>home</td>\n",
       "      <td>0.606009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0   vacation  0.823460\n",
       "1       room  0.810719\n",
       "2      rooms  0.704233\n",
       "3    bedroom  0.658199\n",
       "4      hotel  0.647865\n",
       "5     dining  0.634925\n",
       "6       stay  0.617807\n",
       "7  apartment  0.616495\n",
       "8    staying  0.615182\n",
       "9       home  0.606009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'air' + 'car' (for 'airplane')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air</td>\n",
       "      <td>0.827957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>0.810086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>0.719382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cars</td>\n",
       "      <td>0.671697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.645963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>0.637166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>passenger</td>\n",
       "      <td>0.625993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aircraft</td>\n",
       "      <td>0.624820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jet</td>\n",
       "      <td>0.618584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.610345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0        air  0.827957\n",
       "1        car  0.810086\n",
       "2    vehicle  0.719382\n",
       "3       cars  0.671697\n",
       "4      truck  0.645963\n",
       "5   vehicles  0.637166\n",
       "6  passenger  0.625993\n",
       "7   aircraft  0.624820\n",
       "8        jet  0.618584\n",
       "9   airplane  0.610345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concepts = {'beach': ('sand', 'ocean'), 'hotel': ('vacation', 'room'), 'airplane': ('air', 'car')}\n",
    "for concept in concepts:\n",
    "    pair = concepts[concept]\n",
    "    generated_concept = model[pair[0]] + model[pair[1]]\n",
    "    similarities = model.similar_by_vector(generated_concept)\n",
    "    print(f\"Most similar tokens to '{pair[0]}' + '{pair[1]}' (for '{concept}')\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b64296",
   "metadata": {},
   "source": [
    "Not bad! Our target concept isn't the most similar word for either of these examples, but it's in the top 10.\n",
    "\n",
    "Most famously, word embeddings enable quasi-logical reasoning. Though, as we mentioned earlier, relationships between antonyms and synonyms do not necessarily map to a vector space, certain analogies do. The logic here is that we add together two vectors, analogical together and then subtract a third vector, which has some kind of relation to one of the words in the analogue pair. Querying for the resultant vector should produce a similar relation between the resultant word and the other word in the analogue pair.\n",
    "\n",
    "Here, we ask: \"king is to man what X is to woman?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ae0fa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.697868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.608175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.588975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throne</td>\n",
       "      <td>0.577511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elizabeth</td>\n",
       "      <td>0.546360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.539913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kingdom</td>\n",
       "      <td>0.531805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.516854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crown</td>\n",
       "      <td>0.516447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0      queen  0.697868\n",
       "1   princess  0.608175\n",
       "2    monarch  0.588975\n",
       "3     throne  0.577511\n",
       "4     prince  0.575100\n",
       "5  elizabeth  0.546360\n",
       "6   daughter  0.539913\n",
       "7    kingdom  0.531805\n",
       "8     mother  0.516854\n",
       "9      crown  0.516447"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "pd.DataFrame(analogies, columns=['WORD', 'SCORE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecf5b1",
   "metadata": {},
   "source": [
    "And here, we ask: \"France is to Paris what X is to Berlin\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f1b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>0.835242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german</td>\n",
       "      <td>0.684480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austria</td>\n",
       "      <td>0.612803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poland</td>\n",
       "      <td>0.581331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germans</td>\n",
       "      <td>0.574868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>munich</td>\n",
       "      <td>0.543591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>belgium</td>\n",
       "      <td>0.532413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>britain</td>\n",
       "      <td>0.529541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>europe</td>\n",
       "      <td>0.524402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>czech</td>\n",
       "      <td>0.515241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0  germany  0.835242\n",
       "1   german  0.684480\n",
       "2  austria  0.612803\n",
       "3   poland  0.581331\n",
       "4  germans  0.574868\n",
       "5   munich  0.543591\n",
       "6  belgium  0.532413\n",
       "7  britain  0.529541\n",
       "8   europe  0.524402\n",
       "9    czech  0.515241"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['france', 'berlin'], negative=['paris'])\n",
    "pd.DataFrame(analogies, columns=['WORD', 'SCORE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702496a",
   "metadata": {},
   "source": [
    "Both of the above produce compelling results, though your mileage may vary. Consider the following: \"arm is to hand what leg is to X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b08a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legs</td>\n",
       "      <td>0.519315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groin</td>\n",
       "      <td>0.501345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spinner</td>\n",
       "      <td>0.493277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thigh</td>\n",
       "      <td>0.476108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ankle</td>\n",
       "      <td>0.465417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>knee</td>\n",
       "      <td>0.461705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wrist</td>\n",
       "      <td>0.455966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>seamer</td>\n",
       "      <td>0.454812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>calf</td>\n",
       "      <td>0.453236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paceman</td>\n",
       "      <td>0.453196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0     legs  0.519315\n",
       "1    groin  0.501345\n",
       "2  spinner  0.493277\n",
       "3    thigh  0.476108\n",
       "4    ankle  0.465417\n",
       "5     knee  0.461705\n",
       "6    wrist  0.455966\n",
       "7   seamer  0.454812\n",
       "8     calf  0.453236\n",
       "9  paceman  0.453196"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['arm', 'leg'], negative=['hand'])\n",
    "pd.DataFrame(analogies, columns=['WORD', 'SCORE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5ded1",
   "metadata": {},
   "source": [
    "Document similarity\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58fee46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = [[token for token in doc if token in in_glove] for doc in corpus]\n",
    "doc_embeddings = [np.mean(model[doc], axis=0) for doc in pruned]\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "vis_data = prepare_vis_data(doc_embeddings, manifest['NAME'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d708fb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-25e1e065fd694983b853ca6ce639516d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-25e1e065fd694983b853ca6ce639516d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-25e1e065fd694983b853ca6ce639516d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-61f387aa92192012459f5273bcb96484\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"TOKEN\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-61f387aa92192012459f5273bcb96484\": [{\"X\": 9.629589080810547, \"Y\": 0.9365087747573853, \"TOKEN\": \"Ada Lovelace\"}, {\"X\": 4.867339134216309, \"Y\": -13.727782249450684, \"TOKEN\": \"Robert E Lee\"}, {\"X\": 2.516657829284668, \"Y\": -18.354692459106445, \"TOKEN\": \"Andrew Johnson\"}, {\"X\": 5.089731216430664, \"Y\": -12.619865417480469, \"TOKEN\": \"Bedford Forrest\"}, {\"X\": -0.30361512303352356, \"Y\": -9.816756248474121, \"TOKEN\": \"Lucretia Mott\"}, {\"X\": 11.412369728088379, \"Y\": 0.43414148688316345, \"TOKEN\": \"Charles Darwin\"}, {\"X\": 3.7749245166778564, \"Y\": -13.713809967041016, \"TOKEN\": \"Ulysses Grant\"}, {\"X\": -10.804316520690918, \"Y\": 3.915217161178589, \"TOKEN\": \"Mary Ewing Outerbridge\"}, {\"X\": 5.313749313354492, \"Y\": 7.089348793029785, \"TOKEN\": \"Emma Lazarus\"}, {\"X\": 0.1769566833972931, \"Y\": 4.888604164123535, \"TOKEN\": \"Louisa M Alcott\"}, {\"X\": -2.5393667221069336, \"Y\": -1.0644351243972778, \"TOKEN\": \"P T Barnum\"}, {\"X\": 2.2706544399261475, \"Y\": 4.895766735076904, \"TOKEN\": \"R L Stevenson\"}, {\"X\": 1.4236607551574707, \"Y\": -11.55919361114502, \"TOKEN\": \"Fred Douglass\"}, {\"X\": 1.1170852184295654, \"Y\": 5.052165508270264, \"TOKEN\": \"Harriet Beecher Stowe\"}, {\"X\": 1.3462700843811035, \"Y\": 3.350062131881714, \"TOKEN\": \"Stephen Crane\"}, {\"X\": 11.823625564575195, \"Y\": 5.069955348968506, \"TOKEN\": \"Nietzsche\"}, {\"X\": 3.474562406539917, \"Y\": -14.088415145874023, \"TOKEN\": \"William McKinley\"}, {\"X\": -5.221592903137207, \"Y\": -9.45670223236084, \"TOKEN\": \"Queen Victoria\"}, {\"X\": -0.3932449519634247, \"Y\": -16.153573989868164, \"TOKEN\": \"Benjamin Harrison\"}, {\"X\": 0.8819878101348877, \"Y\": -10.319847106933594, \"TOKEN\": \"Elizabeth Cady Stanton\"}, {\"X\": 5.680935859680176, \"Y\": 11.466192245483398, \"TOKEN\": \"James M N Whistler\"}, {\"X\": -6.502901077270508, \"Y\": -3.6163651943206787, \"TOKEN\": \"Emily Warren Roebling\"}, {\"X\": 0.9912346601486206, \"Y\": -10.336971282958984, \"TOKEN\": \"Susan B Anthony\"}, {\"X\": 13.706411361694336, \"Y\": -15.936660766601562, \"TOKEN\": \"Qiu Jin\"}, {\"X\": 1.1903727054595947, \"Y\": -15.833939552307129, \"TOKEN\": \"Cleveland\"}, {\"X\": 0.06014833599328995, \"Y\": 3.009098529815674, \"TOKEN\": \"Sarah Orne Jewett\"}, {\"X\": 5.059043884277344, \"Y\": -12.145303726196289, \"TOKEN\": \"Geronimo\"}, {\"X\": 8.49888801574707, \"Y\": -6.145576000213623, \"TOKEN\": \"William James\"}, {\"X\": -5.059149742126465, \"Y\": -8.053226470947266, \"TOKEN\": \"Florence Nightingale\"}, {\"X\": 9.45051097869873, \"Y\": 4.464698314666748, \"TOKEN\": \"Tolstoy\"}, {\"X\": 2.86517333984375, \"Y\": -4.517377853393555, \"TOKEN\": \"Joseph Pulitzer\"}, {\"X\": 7.037650108337402, \"Y\": -10.98705768585205, \"TOKEN\": \"John P Holland\"}, {\"X\": 7.9997172355651855, \"Y\": -11.967913627624512, \"TOKEN\": \"Alfred Thayer Mahan\"}, {\"X\": 7.371427536010742, \"Y\": -2.075955390930176, \"TOKEN\": \"John Muir\"}, {\"X\": -0.6195724010467529, \"Y\": -4.894759654998779, \"TOKEN\": \"F W Taylor\"}, {\"X\": -1.4776277542114258, \"Y\": -9.218297958374023, \"TOKEN\": \"B T Washington\"}, {\"X\": -3.7878124713897705, \"Y\": -4.599079608917236, \"TOKEN\": \"J J Hill\"}, {\"X\": -1.648998737335205, \"Y\": 2.9855120182037354, \"TOKEN\": \"Jack London\"}, {\"X\": 9.92032241821289, \"Y\": -1.5003327131271362, \"TOKEN\": \"Martian Theory\"}, {\"X\": 7.427640914916992, \"Y\": 11.48624324798584, \"TOKEN\": \"Hilaire G E Degas\"}, {\"X\": -4.147123336791992, \"Y\": -3.4893581867218018, \"TOKEN\": \"C J Walker\"}, {\"X\": -2.7459497451782227, \"Y\": -4.899856090545654, \"TOKEN\": \"Carnegie Started\"}, {\"X\": 0.44971612095832825, \"Y\": -10.595927238464355, \"TOKEN\": \"Anna H Shaw\"}, {\"X\": -1.8113512992858887, \"Y\": 13.811561584472656, \"TOKEN\": \"Marlene Dietrich\"}, {\"X\": -1.9595662355422974, \"Y\": 1.944274663925171, \"TOKEN\": \"Nellie Bly\"}, {\"X\": -3.6001923084259033, \"Y\": 0.19057707488536835, \"TOKEN\": \"Alexander Graham Bell\"}, {\"X\": 1.1338599920272827, \"Y\": -17.525060653686523, \"TOKEN\": \"Warren Harding\"}, {\"X\": -3.8007283210754395, \"Y\": 5.842870712280273, \"TOKEN\": \"Harry Houdini\"}, {\"X\": 0.49707192182540894, \"Y\": -9.317200660705566, \"TOKEN\": \"Victoria Martin\"}, {\"X\": -2.137650966644287, \"Y\": -11.47287368774414, \"TOKEN\": \"Mabel Craty\"}, {\"X\": 13.6246976852417, \"Y\": -0.4075273275375366, \"TOKEN\": \"Marie Curie\"}, {\"X\": 9.034079551696777, \"Y\": -13.869738578796387, \"TOKEN\": \"Balfour\"}, {\"X\": 6.1557183265686035, \"Y\": -4.759181499481201, \"TOKEN\": \"Elmer Sperry\"}, {\"X\": 2.7730519771575928, \"Y\": -17.90154457092285, \"TOKEN\": \"William Howard Taft\"}, {\"X\": 4.122943878173828, \"Y\": 5.514073848724365, \"TOKEN\": \"Conan Doyle\"}, {\"X\": 3.894010305404663, \"Y\": -7.373220443725586, \"TOKEN\": \"Ida B Wells\"}, {\"X\": -0.6402124762535095, \"Y\": -2.9942216873168945, \"TOKEN\": \"Melvil Dewey\"}, {\"X\": 0.8306495547294617, \"Y\": 10.522436141967773, \"TOKEN\": \"Thomas Edison\"}, {\"X\": -13.630939483642578, \"Y\": 3.5920586585998535, \"TOKEN\": \"Knute Rocke\"}, {\"X\": -11.455645561218262, \"Y\": 17.965166091918945, \"TOKEN\": \"John Philip Sousa\"}, {\"X\": -6.687372207641602, \"Y\": 12.15854263305664, \"TOKEN\": \"Florenz Ziegfeld\"}, {\"X\": 2.818314552307129, \"Y\": -17.133880615234375, \"TOKEN\": \"Calvin Coolidge\"}, {\"X\": -1.5672276020050049, \"Y\": 4.57751989364624, \"TOKEN\": \"Ring Lardner\"}, {\"X\": -7.8486647605896, \"Y\": -0.3336794078350067, \"TOKEN\": \"Louis C Tiffany\"}, {\"X\": -3.5341227054595947, \"Y\": 0.47870972752571106, \"TOKEN\": \"T A Watson\"}, {\"X\": 4.497917652130127, \"Y\": -9.539314270019531, \"TOKEN\": \"Justice Holmes\"}, {\"X\": 2.8870668411254883, \"Y\": -10.985051155090332, \"TOKEN\": \"Jane Addams\"}, {\"X\": -2.852731466293335, \"Y\": 5.445327281951904, \"TOKEN\": \"Will Rogers\"}, {\"X\": 2.634998083114624, \"Y\": -4.535669803619385, \"TOKEN\": \"Adolph S Ochs\"}, {\"X\": -0.8043901920318604, \"Y\": 6.835999965667725, \"TOKEN\": \"Anne Macy\"}, {\"X\": -14.196510314941406, \"Y\": 2.408979892730713, \"TOKEN\": \"John W Heisman\"}, {\"X\": 9.145484924316406, \"Y\": 4.776538848876953, \"TOKEN\": \"Maxim Gorky\"}, {\"X\": -14.743207931518555, \"Y\": 16.750234603881836, \"TOKEN\": \"Maurice Ravel\"}, {\"X\": 2.715054750442505, \"Y\": 4.199475288391113, \"TOKEN\": \"Edith Wharton\"}, {\"X\": -2.939866542816162, \"Y\": -3.85809588432312, \"TOKEN\": \"John Rockefeller\"}, {\"X\": 4.025189399719238, \"Y\": -9.70034122467041, \"TOKEN\": \"Clarence Darrow\"}, {\"X\": 13.446882247924805, \"Y\": -3.987703800201416, \"TOKEN\": \"George E Hale\"}, {\"X\": 2.4567582607269287, \"Y\": 15.349662780761719, \"TOKEN\": \"Constantin Stanislavsky\"}, {\"X\": 3.4906468391418457, \"Y\": 7.675736427307129, \"TOKEN\": \"W B Yeats\"}, {\"X\": -8.473063468933105, \"Y\": -9.35084056854248, \"TOKEN\": \"Pope Pius XI\"}, {\"X\": -5.475143909454346, \"Y\": 3.418489933013916, \"TOKEN\": \"Howard Carter\"}, {\"X\": 1.8188273906707764, \"Y\": 4.609699726104736, \"TOKEN\": \"Scott Fitzgerald\"}, {\"X\": -1.1627827882766724, \"Y\": -7.4035563468933105, \"TOKEN\": \"Marcus Garvey\"}, {\"X\": -4.838081359863281, \"Y\": 0.37718555331230164, \"TOKEN\": \"Frank Conrad\"}, {\"X\": -16.145322799682617, \"Y\": 6.350594997406006, \"TOKEN\": \"Lou Gehrig\"}, {\"X\": 3.6721866130828857, \"Y\": 4.287343502044678, \"TOKEN\": \"James Joyce\"}, {\"X\": 2.9453887939453125, \"Y\": 2.5019946098327637, \"TOKEN\": \"Virginia Woolf\"}, {\"X\": -7.108323097229004, \"Y\": 14.725702285766602, \"TOKEN\": \"George M Cohan\"}, {\"X\": 8.089288711547852, \"Y\": -3.9795265197753906, \"TOKEN\": \"J H Kellogg\"}, {\"X\": 7.745167255401611, \"Y\": -3.375455617904663, \"TOKEN\": \"George Washington Carver\"}, {\"X\": 0.2762145698070526, \"Y\": -17.672365188598633, \"TOKEN\": \"Alfred E Smith\"}, {\"X\": 4.564617156982422, \"Y\": -2.2918553352355957, \"TOKEN\": \"Ida M Tarbell\"}, {\"X\": -3.5655741691589355, \"Y\": 4.17132568359375, \"TOKEN\": \"Ernie Pyle\"}, {\"X\": 4.576817512512207, \"Y\": -17.442001342773438, \"TOKEN\": \"Harry S Truman\"}, {\"X\": 5.769756317138672, \"Y\": -13.118968963623047, \"TOKEN\": \"George Patton\"}, {\"X\": -0.487134724855423, \"Y\": -17.166996002197266, \"TOKEN\": \"FDR\"}, {\"X\": -10.25243854522705, \"Y\": 16.744211196899414, \"TOKEN\": \"Jerome Kern\"}, {\"X\": 10.450125694274902, \"Y\": -17.857093811035156, \"TOKEN\": \"Adolf Hitler\"}, {\"X\": -15.567388534545898, \"Y\": 17.081615447998047, \"TOKEN\": \"Bela Bartok\"}, {\"X\": 3.3424673080444336, \"Y\": 4.723523139953613, \"TOKEN\": \"Gertrude Stein\"}, {\"X\": 9.325911521911621, \"Y\": -12.526713371276855, \"TOKEN\": \"Lord Keynes\"}, {\"X\": 14.586517333984375, \"Y\": -4.484496593475342, \"TOKEN\": \"C E M Clung\"}, {\"X\": 1.1403337717056274, \"Y\": 2.509639024734497, \"TOKEN\": \"Willa Cather\"}, {\"X\": -4.850642204284668, \"Y\": 6.138102054595947, \"TOKEN\": \"Al Capone\"}, {\"X\": 2.3354763984680176, \"Y\": -15.820819854736328, \"TOKEN\": \"Fiorello La Guardia\"}, {\"X\": 14.609024047851562, \"Y\": -0.10717794299125671, \"TOKEN\": \"Max Planck\"}, {\"X\": -1.1804206371307373, \"Y\": -5.477313041687012, \"TOKEN\": \"Henry Ford\"}, {\"X\": 6.029560565948486, \"Y\": -13.791752815246582, \"TOKEN\": \"John Pershing\"}, {\"X\": 2.0751850605010986, \"Y\": 15.046469688415527, \"TOKEN\": \"Sergei Eisenstein\"}, {\"X\": 8.11977481842041, \"Y\": -20.070783615112305, \"TOKEN\": \"Mohandas K Gandhi\"}, {\"X\": -14.677245140075684, \"Y\": 4.58611536026001, \"TOKEN\": \"Babe Ruth\"}, {\"X\": -0.5605786442756653, \"Y\": 1.7567886114120483, \"TOKEN\": \"Mitchell\"}, {\"X\": 14.690046310424805, \"Y\": -2.3300743103027344, \"TOKEN\": \"A J Dempster\"}, {\"X\": 0.5355619788169861, \"Y\": 4.450295925140381, \"TOKEN\": \"Edna St V Millay\"}, {\"X\": 5.150554656982422, \"Y\": -16.838817596435547, \"TOKEN\": \"Henry L Stimson\"}, {\"X\": -7.985596656799316, \"Y\": 14.369399070739746, \"TOKEN\": \"Fanny Brice\"}, {\"X\": 10.201523780822754, \"Y\": -3.951920509338379, \"TOKEN\": \"Henrietta Lacks\"}, {\"X\": 7.256105422973633, \"Y\": -15.879838943481445, \"TOKEN\": \"Eva Peron\"}, {\"X\": 7.958618640899658, \"Y\": -7.40358304977417, \"TOKEN\": \"John Dewey\"}, {\"X\": 8.137778282165527, \"Y\": -22.409303665161133, \"TOKEN\": \"Chaim Weizmann\"}, {\"X\": -3.047520399093628, \"Y\": -6.486069679260254, \"TOKEN\": \"Charles Spaulding\"}, {\"X\": 3.39670467376709, \"Y\": -18.607646942138672, \"TOKEN\": \"Fred Vinson\"}, {\"X\": 0.5231119394302368, \"Y\": 2.0844290256500244, \"TOKEN\": \"Marjorie Rawlings\"}, {\"X\": 11.478231430053711, \"Y\": -18.480487823486328, \"TOKEN\": \"Joseph Stalin\"}, {\"X\": -13.02806568145752, \"Y\": 4.952950477600098, \"TOKEN\": \"Jim Thorpe\"}, {\"X\": -1.8037348985671997, \"Y\": 3.449126958847046, \"TOKEN\": \"Eugene O Neill\"}, {\"X\": 3.311312437057495, \"Y\": -3.8299105167388916, \"TOKEN\": \"Anne O Hare McCormick\"}, {\"X\": 5.014588832855225, \"Y\": 10.005289077758789, \"TOKEN\": \"Frida Kahlo\"}, {\"X\": 8.413653373718262, \"Y\": -17.18549156188965, \"TOKEN\": \"Getulio Vargas\"}, {\"X\": 14.445099830627441, \"Y\": -1.8850390911102295, \"TOKEN\": \"Enrico Fermi\"}, {\"X\": 6.477755069732666, \"Y\": 11.336223602294922, \"TOKEN\": \"Henri Matisse\"}, {\"X\": 7.114038944244385, \"Y\": -2.987826347351074, \"TOKEN\": \"Liberty H Bailey\"}, {\"X\": -5.830735206604004, \"Y\": 15.424117088317871, \"TOKEN\": \"Lionel Barrymore\"}, {\"X\": 9.499425888061523, \"Y\": 6.423043251037598, \"TOKEN\": \"Thomas Mann\"}, {\"X\": 12.435925483703613, \"Y\": 0.6852051019668579, \"TOKEN\": \"Albert Einstein\"}, {\"X\": -12.06922435760498, \"Y\": 6.419966220855713, \"TOKEN\": \"Margaret Abbott\"}, {\"X\": -0.3511526584625244, \"Y\": -12.968914031982422, \"TOKEN\": \"Walter White\"}, {\"X\": -15.838933944702148, \"Y\": 5.7552266120910645, \"TOKEN\": \"Cy Young\"}, {\"X\": -0.6612284779548645, \"Y\": -0.9929090142250061, \"TOKEN\": \"Dale Carnegie\"}, {\"X\": -11.752488136291504, \"Y\": 4.983523368835449, \"TOKEN\": \"Babe Zaharias\"}, {\"X\": -2.0537309646606445, \"Y\": -3.7528295516967773, \"TOKEN\": \"Charles Merrill\"}, {\"X\": -2.085387706756592, \"Y\": -4.954380035400391, \"TOKEN\": \"Thomas J Watson Sr\"}, {\"X\": -2.122974395751953, \"Y\": -5.693045616149902, \"TOKEN\": \"Gerard Swope\"}, {\"X\": 6.393711090087891, \"Y\": 13.563359260559082, \"TOKEN\": \"Christian Dior\"}, {\"X\": -10.264820098876953, \"Y\": 19.21905517578125, \"TOKEN\": \"W C Handy\"}, {\"X\": -9.749808311462402, \"Y\": 19.22344207763672, \"TOKEN\": \"Billie Holiday\"}, {\"X\": -7.139647483825684, \"Y\": -2.679274320602417, \"TOKEN\": \"Frank Lloyd Wright\"}, {\"X\": -6.252832412719727, \"Y\": 15.452739715576172, \"TOKEN\": \"Ethel Barrymore\"}, {\"X\": -1.6900622844696045, \"Y\": 16.44804573059082, \"TOKEN\": \"Cecil De Mille\"}, {\"X\": 14.377174377441406, \"Y\": -4.461243152618408, \"TOKEN\": \"Ross G Harrison\"}, {\"X\": 5.730043411254883, \"Y\": -17.34250259399414, \"TOKEN\": \"John Dulles\"}, {\"X\": 8.909778594970703, \"Y\": 5.260741233825684, \"TOKEN\": \"Boris Pasternak\"}, {\"X\": 15.18835163116455, \"Y\": -3.321732997894287, \"TOKEN\": \"Beno Gutenberg\"}, {\"X\": 6.0727105140686035, \"Y\": 0.7039583921432495, \"TOKEN\": \"Emily Post\"}, {\"X\": 1.2883164882659912, \"Y\": 0.7812788486480713, \"TOKEN\": \"Richard Wright\"}, {\"X\": 6.606455326080322, \"Y\": -17.90180778503418, \"TOKEN\": \"Hammarskjold\"}, {\"X\": 2.51444411277771, \"Y\": 5.4511637687683105, \"TOKEN\": \"Ernest Hemingway\"}, {\"X\": 3.3103435039520264, \"Y\": 9.779006958007812, \"TOKEN\": \"Primitive Artist\"}, {\"X\": -1.2825796604156494, \"Y\": -11.307229995727539, \"TOKEN\": \"Emily Balch\"}, {\"X\": 1.1649328470230103, \"Y\": -18.612628936767578, \"TOKEN\": \"Sam Rayburn\"}, {\"X\": 12.17062759399414, \"Y\": 2.2361319065093994, \"TOKEN\": \"Carl G Jung\"}, {\"X\": -4.2258782386779785, \"Y\": 10.714470863342285, \"TOKEN\": \"Marilyn Monroe\"}, {\"X\": 1.90577232837677, \"Y\": -14.886177062988281, \"TOKEN\": \"Eleanor Roosevelt\"}, {\"X\": 3.077925443649292, \"Y\": 3.4568052291870117, \"TOKEN\": \"William Faulkner\"}, {\"X\": 3.701728343963623, \"Y\": 2.523127794265747, \"TOKEN\": \"Sylvia Plath\"}, {\"X\": 2.2289395332336426, \"Y\": -19.08131217956543, \"TOKEN\": \"John F Kennedy\"}, {\"X\": 4.192113876342773, \"Y\": 3.648496150970459, \"TOKEN\": \"Robert Frost\"}, {\"X\": -0.7063899636268616, \"Y\": -11.29255485534668, \"TOKEN\": \"W E B DuBois\"}, {\"X\": -1.7259103059768677, \"Y\": -14.54096508026123, \"TOKEN\": \"Herbert Hoover\"}, {\"X\": 6.282223224639893, \"Y\": -13.74743938446045, \"TOKEN\": \"Douglas MacArthur\"}, {\"X\": 1.7478026151657104, \"Y\": 7.9571709632873535, \"TOKEN\": \"Sean O Casey\"}, {\"X\": 8.371752738952637, \"Y\": -1.8794926404953003, \"TOKEN\": \"Rachel Carson\"}, {\"X\": -9.30225944519043, \"Y\": 16.191822052001953, \"TOKEN\": \"Cole Porter\"}, {\"X\": 3.076063871383667, \"Y\": 1.314249038696289, \"TOKEN\": \"Nella Larsen\"}, {\"X\": 1.6825166940689087, \"Y\": -17.522794723510742, \"TOKEN\": \"Adlai Ewing Stevenson\"}, {\"X\": -3.164984941482544, \"Y\": 17.577924728393555, \"TOKEN\": \"David O Selznick\"}, {\"X\": 8.69693660736084, \"Y\": -14.189995765686035, \"TOKEN\": \"Churchill\"}, {\"X\": -15.152002334594727, \"Y\": 3.9001054763793945, \"TOKEN\": \"Branch Rickey\"}, {\"X\": 11.748671531677246, \"Y\": 4.267360687255859, \"TOKEN\": \"Martin Buber\"}, {\"X\": 1.4773584604263306, \"Y\": -3.502977132797241, \"TOKEN\": \"Edward R Murrow\"}, {\"X\": 10.981849670410156, \"Y\": 3.3068172931671143, \"TOKEN\": \"Albert Schweitzer\"}, {\"X\": 1.810014009475708, \"Y\": 6.656957626342773, \"TOKEN\": \"Shirley Jackson\"}, {\"X\": 1.8925938606262207, \"Y\": -9.45506477355957, \"TOKEN\": \"Margaret Sanger\"}, {\"X\": 6.807074069976807, \"Y\": -11.948288917541504, \"TOKEN\": \"Chester Nimitz\"}, {\"X\": -2.056762218475342, \"Y\": 12.418746948242188, \"TOKEN\": \"Buster Keaton\"}, {\"X\": -2.6335644721984863, \"Y\": 6.498964309692383, \"TOKEN\": \"Lenny Bruce\"}, {\"X\": -0.9291820526123047, \"Y\": 17.55532455444336, \"TOKEN\": \"Walt Disney\"}, {\"X\": -1.7889564037322998, \"Y\": -5.057743072509766, \"TOKEN\": \"Alfred P Sloan Jr\"}, {\"X\": 10.934745788574219, \"Y\": -4.189286231994629, \"TOKEN\": \"Gregory Pincus\"}, {\"X\": 3.474595785140991, \"Y\": -1.6320171356201172, \"TOKEN\": \"Henry R Luce\"}, {\"X\": 5.184930801391602, \"Y\": 2.462585687637329, \"TOKEN\": \"Langston Hughes\"}, {\"X\": 12.949685096740723, \"Y\": -1.5517901182174683, \"TOKEN\": \"J Robert Oppenheimer\"}, {\"X\": 0.8506120443344116, \"Y\": -16.64697265625, \"TOKEN\": \"Robert Francis Kennedy\"}, {\"X\": 0.06093268468976021, \"Y\": 6.959182262420654, \"TOKEN\": \"Helen Keller\"}, {\"X\": 4.050485610961914, \"Y\": -1.4843933582305908, \"TOKEN\": \"Upton Sinclair\"}, {\"X\": 4.620257377624512, \"Y\": -8.236494064331055, \"TOKEN\": \"Martin Luther King Jr\"}, {\"X\": -6.331336498260498, \"Y\": -12.70248031616211, \"TOKEN\": \"Yuri Gagarin\"}, {\"X\": -7.336552619934082, \"Y\": -2.553863286972046, \"TOKEN\": \"Mies van der Rohe\"}, {\"X\": 4.473064422607422, \"Y\": -17.60696029663086, \"TOKEN\": \"David Eisenhower\"}, {\"X\": -12.037976264953613, \"Y\": 20.4371280670166, \"TOKEN\": \"Coleman Hawkins\"}, {\"X\": -1.0962679386138916, \"Y\": 14.572998046875, \"TOKEN\": \"Madhubala\"}, {\"X\": -6.652101516723633, \"Y\": 13.022965431213379, \"TOKEN\": \"Judy Garland\"}, {\"X\": -11.519660949707031, \"Y\": 5.149566173553467, \"TOKEN\": \"Maureen Connolly\"}, {\"X\": 11.490662574768066, \"Y\": -16.636425018310547, \"TOKEN\": \"Ho Chi Minh\"}, {\"X\": -12.171581268310547, \"Y\": 7.268080234527588, \"TOKEN\": \"Sonja Henie\"}, {\"X\": 2.202540159225464, \"Y\": -16.904741287231445, \"TOKEN\": \"Everett Dirksen\"}, {\"X\": -2.9562528133392334, \"Y\": -14.008023262023926, \"TOKEN\": \"Walter Reuther\"}, {\"X\": 10.0339937210083, \"Y\": -17.23975372314453, \"TOKEN\": \"Edouard Daladier\"}, {\"X\": 8.901248931884766, \"Y\": 6.926815032958984, \"TOKEN\": \"Erich Maria Remarque\"}, {\"X\": 9.946044921875, \"Y\": -16.896339416503906, \"TOKEN\": \"De Gaulle Rallied\"}, {\"X\": 6.375982284545898, \"Y\": 14.089868545532227, \"TOKEN\": \"Coco Chanel\"}, {\"X\": 5.421616554260254, \"Y\": -14.56261157989502, \"TOKEN\": \"Florence Blanchfield\"}, {\"X\": 11.947996139526367, \"Y\": -18.720796585083008, \"TOKEN\": \"Khrushchev\"}, {\"X\": 3.6141252517700195, \"Y\": 11.977758407592773, \"TOKEN\": \"Diane Arbus\"}, {\"X\": 3.14437198638916, \"Y\": -11.699508666992188, \"TOKEN\": \"Ralph Bunche\"}, {\"X\": -11.873806953430176, \"Y\": 5.324117183685303, \"TOKEN\": \"Bobby Jones\"}, {\"X\": -10.47250747680664, \"Y\": 19.456758499145508, \"TOKEN\": \"Louis Armstrong\"}, {\"X\": 5.490179538726807, \"Y\": -17.52106285095215, \"TOKEN\": \"Dean Acheson\"}, {\"X\": -14.25207233428955, \"Y\": 16.682188034057617, \"TOKEN\": \"Igor Stravinsky\"}, {\"X\": 3.6063179969787598, \"Y\": -20.43814468383789, \"TOKEN\": \"Hugo Black\"}, {\"X\": -8.741172790527344, \"Y\": 20.10617446899414, \"TOKEN\": \"Mahalia Jackson\"}, {\"X\": -14.888214111328125, \"Y\": 4.1791863441467285, \"TOKEN\": \"Jackie Robinson\"}, {\"X\": -5.368350982666016, \"Y\": -9.634806632995605, \"TOKEN\": \"The Duke of Windsor\"}, {\"X\": -0.6426783204078674, \"Y\": -14.689474105834961, \"TOKEN\": \"J Edgar Hoover\"}, {\"X\": -2.308452844619751, \"Y\": -14.421500205993652, \"TOKEN\": \"Lyndon Johnson\"}, {\"X\": -13.81699275970459, \"Y\": 16.135818481445312, \"TOKEN\": \"Otto Klemperer\"}, {\"X\": 0.9663636684417725, \"Y\": -6.817568778991699, \"TOKEN\": \"Eddie Rickenbacker\"}, {\"X\": 1.193108320236206, \"Y\": -20.33041000366211, \"TOKEN\": \"Jeanette Rankin\"}, {\"X\": 6.1103901863098145, \"Y\": 10.780670166015625, \"TOKEN\": \"Pablo Picasso\"}, {\"X\": 0.5244559049606323, \"Y\": -6.968406677246094, \"TOKEN\": \"Roberto Clemente\"}, {\"X\": 3.218234062194824, \"Y\": 6.269067764282227, \"TOKEN\": \"Nancy Mitford\"}, {\"X\": 3.557950735092163, \"Y\": -20.199440002441406, \"TOKEN\": \"Earl Warren\"}, {\"X\": 1.1160459518432617, \"Y\": 8.574992179870605, \"TOKEN\": \"Sylvia Plath\"}, {\"X\": -6.244418621063232, \"Y\": 10.41963005065918, \"TOKEN\": \"Ed Sullivan\"}, {\"X\": -6.011885166168213, \"Y\": 14.240015029907227, \"TOKEN\": \"Katharine Cornell\"}, {\"X\": 2.142029285430908, \"Y\": -6.557278156280518, \"TOKEN\": \"Charles Lindbergh\"}, {\"X\": 11.842513084411621, \"Y\": -14.831551551818848, \"TOKEN\": \"Haile Selassie\"}, {\"X\": 5.107857704162598, \"Y\": -7.8079938888549805, \"TOKEN\": \"Elijah Muhammad\"}, {\"X\": 9.465653419494629, \"Y\": -17.16649627685547, \"TOKEN\": \"Franco\"}, {\"X\": 3.864711284637451, \"Y\": 11.90059757232666, \"TOKEN\": \"Walker Evans\"}, {\"X\": 12.281484603881836, \"Y\": -16.960769653320312, \"TOKEN\": \"Chiang Kai shek\"}, {\"X\": -3.19472599029541, \"Y\": -3.142597198486328, \"TOKEN\": \"J Paul Getty\"}, {\"X\": 12.985475540161133, \"Y\": -16.64879608154297, \"TOKEN\": \"Mao Tse Tung\"}, {\"X\": 6.411359786987305, \"Y\": 10.695928573608398, \"TOKEN\": \"Max Ernst\"}, {\"X\": -1.0085030794143677, \"Y\": -18.12965202331543, \"TOKEN\": \"Richard Daley\"}, {\"X\": 13.216479301452637, \"Y\": -0.3249548673629761, \"TOKEN\": \"Jacques Monod\"}, {\"X\": -1.367120623588562, \"Y\": 17.1641902923584, \"TOKEN\": \"Adolph Zukor\"}, {\"X\": -2.0315723419189453, \"Y\": 12.687402725219727, \"TOKEN\": \"Charles Chaplin\"}, {\"X\": -5.378741264343262, \"Y\": 13.529718399047852, \"TOKEN\": \"Joan Crawford\"}, {\"X\": 9.092671394348145, \"Y\": -14.452479362487793, \"TOKEN\": \"Dash Ended\"}, {\"X\": -11.737982749938965, \"Y\": 15.359981536865234, \"TOKEN\": \"Maria Callas\"}, {\"X\": 8.854317665100098, \"Y\": -22.21473503112793, \"TOKEN\": \"Golda Meir\"}, {\"X\": 10.609031677246094, \"Y\": 1.3082987070083618, \"TOKEN\": \"Margaret Mead\"}, {\"X\": 6.118866920471191, \"Y\": -9.024884223937988, \"TOKEN\": \"Pope Paul VI\"}, {\"X\": -7.283303260803223, \"Y\": -6.654460906982422, \"TOKEN\": \"Bruce Catton\"}, {\"X\": -12.953668594360352, \"Y\": 16.32244873046875, \"TOKEN\": \"Arthur Fiedler\"}, {\"X\": -3.606452465057373, \"Y\": 12.465727806091309, \"TOKEN\": \"John Wayne\"}, {\"X\": 1.2237907648086548, \"Y\": -13.392544746398926, \"TOKEN\": \"A Philip Randolph\"}, {\"X\": -12.13571548461914, \"Y\": 19.790708541870117, \"TOKEN\": \"Stan Kenton\"}, {\"X\": -9.776265144348145, \"Y\": 16.63808250427246, \"TOKEN\": \"Richard Rodgers\"}, {\"X\": -12.791735649108887, \"Y\": 4.72695779800415, \"TOKEN\": \"Jesse Owens\"}, {\"X\": 0.1744154691696167, \"Y\": 13.81369400024414, \"TOKEN\": \"Alfred Hitchcock\"}, {\"X\": 11.404035568237305, \"Y\": 1.2002414464950562, \"TOKEN\": \"Jean Piaget\"}, {\"X\": 10.629410743713379, \"Y\": 5.37824010848999, \"TOKEN\": \"Jean Paul Sartre\"}, {\"X\": -9.51002311706543, \"Y\": 6.543694019317627, \"TOKEN\": \"Joe Louis\"}, {\"X\": -4.598476886749268, \"Y\": -5.0298991203308105, \"TOKEN\": \"Robert Moses\"}, {\"X\": 9.557087898254395, \"Y\": -22.23974609375, \"TOKEN\": \"Anwar el Sadat\"}, {\"X\": -4.135482311248779, \"Y\": 14.641820907592773, \"TOKEN\": \"Ingrid Bergman\"}, {\"X\": 11.428175926208496, \"Y\": 2.1299140453338623, \"TOKEN\": \"Anna Freud\"}, {\"X\": 11.94921875, \"Y\": -19.105857849121094, \"TOKEN\": \"Leonid Brezhnev\"}, {\"X\": -13.345328330993652, \"Y\": 17.284900665283203, \"TOKEN\": \"Arthur Rubinstein\"}, {\"X\": -11.595566749572754, \"Y\": 19.937498092651367, \"TOKEN\": \"Thelonious Monk\"}, {\"X\": -4.014244556427002, \"Y\": 16.423534393310547, \"TOKEN\": \"Lee Strasberg\"}, {\"X\": -15.019454956054688, \"Y\": 4.6996564865112305, \"TOKEN\": \"Satchel Paige\"}, {\"X\": -9.574095726013184, \"Y\": 6.669792175292969, \"TOKEN\": \"Jack Dempsey\"}, {\"X\": -11.981386184692383, \"Y\": 21.897388458251953, \"TOKEN\": \"Earl Hines\"}, {\"X\": -10.348932266235352, \"Y\": 21.249935150146484, \"TOKEN\": \"Muddy Waters\"}, {\"X\": 2.417151689529419, \"Y\": 5.5440239906311035, \"TOKEN\": \"Truman Capote\"}, {\"X\": 0.5363287925720215, \"Y\": 6.157252311706543, \"TOKEN\": \"Lillian Hellman\"}, {\"X\": -10.660518646240234, \"Y\": 7.940510272979736, \"TOKEN\": \"Johnny Weissmuller\"}, {\"X\": -7.373063564300537, \"Y\": 0.5054081082344055, \"TOKEN\": \"Ansel Adams\"}, {\"X\": -8.515951156616211, \"Y\": 15.578156471252441, \"TOKEN\": \"Ethel Merman\"}, {\"X\": 8.100703239440918, \"Y\": -20.053998947143555, \"TOKEN\": \"Indira Gandhi\"}, {\"X\": -3.5809130668640137, \"Y\": -2.1500587463378906, \"TOKEN\": \"Ray A Kroc\"}, {\"X\": -5.22397518157959, \"Y\": 15.093145370483398, \"TOKEN\": \"Richard Burton\"}, {\"X\": -11.587529182434082, \"Y\": 21.17261505126953, \"TOKEN\": \"Count Basie\"}, {\"X\": 1.3015097379684448, \"Y\": 7.229896545410156, \"TOKEN\": \"E B White\"}, {\"X\": -2.68548321723938, \"Y\": 15.73446273803711, \"TOKEN\": \"Orson Welles\"}, {\"X\": -15.570432662963867, \"Y\": 5.330615520477295, \"TOKEN\": \"Roger Maris\"}, {\"X\": -3.728980779647827, \"Y\": 13.766958236694336, \"TOKEN\": \"James Cagney\"}, {\"X\": 4.857714653015137, \"Y\": 11.632116317749023, \"TOKEN\": \"Georgia O Keeffe\"}, {\"X\": -11.422660827636719, \"Y\": 20.63717269897461, \"TOKEN\": \"Benny Goodman\"}, {\"X\": 6.449415683746338, \"Y\": 5.915811061859131, \"TOKEN\": \"Jorge Luis Borges\"}, {\"X\": 5.973481178283691, \"Y\": 5.009847164154053, \"TOKEN\": \"Bernard Malamud\"}, {\"X\": -7.632770538330078, \"Y\": 10.633408546447754, \"TOKEN\": \"Kate Smith\"}, {\"X\": -2.7195780277252197, \"Y\": -8.981995582580566, \"TOKEN\": \"The Challenger\"}, {\"X\": 8.389544486999512, \"Y\": 7.210494518280029, \"TOKEN\": \"Primo Levi\"}, {\"X\": 2.7997899055480957, \"Y\": -2.075590133666992, \"TOKEN\": \"Clare Boothe Luce\"}, {\"X\": -2.9558186531066895, \"Y\": 14.558831214904785, \"TOKEN\": \"John Huston\"}, {\"X\": -5.270463466644287, \"Y\": 12.391576766967773, \"TOKEN\": \"Rita Hayworth\"}, {\"X\": 4.1467084884643555, \"Y\": -6.818028926849365, \"TOKEN\": \"James Baldwin\"}, {\"X\": -0.20809608697891235, \"Y\": -17.895172119140625, \"TOKEN\": \"Alf Landon\"}, {\"X\": -13.740716934204102, \"Y\": 18.37775230407715, \"TOKEN\": \"Andres Segovie\"}, {\"X\": -3.0713820457458496, \"Y\": 16.13964080810547, \"TOKEN\": \"John Houseman\"}, {\"X\": 2.1412930488586426, \"Y\": 3.7850356101989746, \"TOKEN\": \"Louis L Amour\"}, {\"X\": 12.002198219299316, \"Y\": -2.0154478549957275, \"TOKEN\": \"William B Shockley\"}, {\"X\": -7.188117504119873, \"Y\": 13.554760932922363, \"TOKEN\": \"Lucille Ball\"}, {\"X\": 3.9904696941375732, \"Y\": 3.6188926696777344, \"TOKEN\": \"Robert Penn Warren\"}, {\"X\": -2.993260622024536, \"Y\": -15.6643705368042, \"TOKEN\": \"Ferdinand Marcos\"}, {\"X\": 12.461625099182129, \"Y\": -19.119094848632812, \"TOKEN\": \"Andrei Sakharov\"}, {\"X\": 11.80474853515625, \"Y\": -19.722991943359375, \"TOKEN\": \"Andrei A Gromyko\"}, {\"X\": 3.803722858428955, \"Y\": -5.2729573249816895, \"TOKEN\": \"I F Stone\"}, {\"X\": -13.136995315551758, \"Y\": 17.290422439575195, \"TOKEN\": \"Vladimir Horowitz\"}, {\"X\": 12.181300163269043, \"Y\": -14.329141616821289, \"TOKEN\": \"Hirohito\"}, {\"X\": -4.576432704925537, \"Y\": -1.7550851106643677, \"TOKEN\": \"August A Busch Jr\"}, {\"X\": 0.6167632937431335, \"Y\": -19.481542587280273, \"TOKEN\": \"Claude Pepper\"}, {\"X\": 3.112546443939209, \"Y\": 7.466588973999023, \"TOKEN\": \"Samuel Beckett\"}, {\"X\": -2.007981061935425, \"Y\": 13.82043743133545, \"TOKEN\": \"Greta Garbo\"}, {\"X\": -5.084105014801025, \"Y\": 13.166306495666504, \"TOKEN\": \"Sammy Davis Jr\"}, {\"X\": -12.992807388305664, \"Y\": 16.50141143798828, \"TOKEN\": \"Leonard Bernstein\"}, {\"X\": 5.654520034790039, \"Y\": 13.032342910766602, \"TOKEN\": \"Erte\"}, {\"X\": 0.8844993710517883, \"Y\": -12.610634803771973, \"TOKEN\": \"Ralph David Abernathy\"}, {\"X\": -6.092902660369873, \"Y\": 16.384614944458008, \"TOKEN\": \"Rex Harrison\"}, {\"X\": -2.862534999847412, \"Y\": 14.518528938293457, \"TOKEN\": \"Frank Capra\"}, {\"X\": -0.7489873766899109, \"Y\": 8.879402160644531, \"TOKEN\": \"Dr Seuss\"}, {\"X\": -12.450113296508789, \"Y\": 20.800580978393555, \"TOKEN\": \"Miles Davis\"}, {\"X\": -10.867227554321289, \"Y\": 13.873992919921875, \"TOKEN\": \"Martha Graham\"}, {\"X\": -15.863840103149414, \"Y\": 4.244723320007324, \"TOKEN\": \"Leo Durocher\"}, {\"X\": -5.181403636932373, \"Y\": 16.91637420654297, \"TOKEN\": \"Peggy Ashcroft\"}, {\"X\": 1.953492522239685, \"Y\": 1.8476524353027344, \"TOKEN\": \"Alex Haley\"}, {\"X\": -14.235946655273438, \"Y\": 17.534887313842773, \"TOKEN\": \"John Cage\"}, {\"X\": 9.157862663269043, \"Y\": -22.620685577392578, \"TOKEN\": \"Menachem Begin\"}, {\"X\": -6.655925750732422, \"Y\": 15.450852394104004, \"TOKEN\": \"Shirley Booth\"}, {\"X\": 6.403168678283691, \"Y\": 4.0633955001831055, \"TOKEN\": \"Isaac Asimov\"}, {\"X\": 2.9871246814727783, \"Y\": -0.25527191162109375, \"TOKEN\": \"William Shawn\"}, {\"X\": 3.1391007900238037, \"Y\": -7.799642562866211, \"TOKEN\": \"Marsha P Johnson\"}, {\"X\": 3.6216492652893066, \"Y\": -20.73087501525879, \"TOKEN\": \"Thurgood Marshall\"}, {\"X\": -0.025618620216846466, \"Y\": 14.099115371704102, \"TOKEN\": \"Federico Fellini\"}, {\"X\": 1.7541379928588867, \"Y\": -13.795405387878418, \"TOKEN\": \"Cesar Chavez\"}, {\"X\": -14.002293586730957, \"Y\": 19.273035049438477, \"TOKEN\": \"Carlos Montoya\"}, {\"X\": -12.635844230651855, \"Y\": 21.199996948242188, \"TOKEN\": \"Dizzy Gillespie\"}, {\"X\": -11.325723648071289, \"Y\": 4.3122029304504395, \"TOKEN\": \"Arthur Ashe\"}, {\"X\": 4.702512741088867, \"Y\": 5.378389358520508, \"TOKEN\": \"William Golding\"}, {\"X\": 11.471202850341797, \"Y\": -5.67047643661499, \"TOKEN\": \"Albert Sabin\"}, {\"X\": 0.7590458989143372, \"Y\": -18.519386291503906, \"TOKEN\": \"Thomas P O Neill Jr\"}, {\"X\": 1.6892129182815552, \"Y\": -17.274255752563477, \"TOKEN\": \"Richard Nixon\"}, {\"X\": 11.3215913772583, \"Y\": 1.8216707706451416, \"TOKEN\": \"Erik Erikson\"}, {\"X\": 12.909110069274902, \"Y\": -1.4237960577011108, \"TOKEN\": \"Linus C Pauling\"}, {\"X\": -5.725397109985352, \"Y\": 14.565417289733887, \"TOKEN\": \"Jessica Tandy\"}, {\"X\": 9.70884895324707, \"Y\": -8.380881309509277, \"TOKEN\": \"Jan Tinbergen\"}, {\"X\": 1.5300692319869995, \"Y\": -0.6255696415901184, \"TOKEN\": \"Jacqueline Kennedy\"}, {\"X\": 11.458001136779785, \"Y\": -5.619353771209717, \"TOKEN\": \"Jonas Salk\"}, {\"X\": 3.310856819152832, \"Y\": 12.234801292419434, \"TOKEN\": \"Alfred Eisenstaedt\"}, {\"X\": -7.527838230133057, \"Y\": 15.655458450317383, \"TOKEN\": \"Ginger Rogers\"}, {\"X\": -6.486595630645752, \"Y\": 14.33803939819336, \"TOKEN\": \"George Abbott\"}, {\"X\": 9.317960739135742, \"Y\": -22.759197235107422, \"TOKEN\": \"Yitzhak Rabin\"}, {\"X\": 10.1116361618042, \"Y\": -1.4016354084014893, \"TOKEN\": \"Carl Sagan\"}, {\"X\": 4.682015895843506, \"Y\": -0.24707086384296417, \"TOKEN\": \"Timothy Leary\"}, {\"X\": -7.98546838760376, \"Y\": 16.594234466552734, \"TOKEN\": \"Gene Kelly\"}, {\"X\": 4.855006217956543, \"Y\": 3.3466439247131348, \"TOKEN\": \"Allen Ginsberg\"}, {\"X\": 13.162900924682617, \"Y\": -17.003646850585938, \"TOKEN\": \"Deng Xiaoping\"}, {\"X\": -3.966529369354248, \"Y\": 13.405994415283203, \"TOKEN\": \"James Stewart\"}, {\"X\": -2.5082755088806152, \"Y\": 11.209596633911133, \"TOKEN\": \"Bob Kane\"}, {\"X\": 6.970714569091797, \"Y\": 0.5519394278526306, \"TOKEN\": \"Benjamin Spock\"}, {\"X\": -11.216814994812012, \"Y\": 5.593843460083008, \"TOKEN\": \"Helen Moody\"}, {\"X\": -5.174684524536133, \"Y\": 15.951761245727539, \"TOKEN\": \"Maureen O Sullivan\"}, {\"X\": 9.226228713989258, \"Y\": -7.758716106414795, \"TOKEN\": \"Theodore Schultz\"}, {\"X\": -6.292104244232178, \"Y\": -12.628838539123535, \"TOKEN\": \"Alan B Shepard Jr\"}, {\"X\": -11.844756126403809, \"Y\": 13.566322326660156, \"TOKEN\": \"Galina Ulanova\"}, {\"X\": -0.42799046635627747, \"Y\": -19.266468048095703, \"TOKEN\": \"Bella Abzug\"}, {\"X\": 1.4201784133911133, \"Y\": -3.4274697303771973, \"TOKEN\": \"Fred W Friendly\"}, {\"X\": -9.123132705688477, \"Y\": 18.30845832824707, \"TOKEN\": \"Frank Sinatra\"}, {\"X\": 10.336825370788574, \"Y\": -21.98391342163086, \"TOKEN\": \"Hassan II\"}, {\"X\": 5.405998706817627, \"Y\": 6.564864635467529, \"TOKEN\": \"Iris Murdoch\"}, {\"X\": 10.242327690124512, \"Y\": -22.4055118560791, \"TOKEN\": \"King Hussein\"}, {\"X\": 9.397595405578613, \"Y\": -15.240979194641113, \"TOKEN\": \"Pierre Trudeau\"}, {\"X\": 0.28496065735816956, \"Y\": -16.12995719909668, \"TOKEN\": \"Elliot Richardson\"}, {\"X\": -1.7634185552597046, \"Y\": 9.884428977966309, \"TOKEN\": \"Charles M Schulz\"}, {\"X\": 9.921907424926758, \"Y\": 0.5387915968894958, \"TOKEN\": \"Karen Sparck Jones\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='TOKEN'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c281c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
