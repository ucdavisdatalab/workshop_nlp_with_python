
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Word Embeddings &#8212; Natural Language Processing with Python</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >let toggleHintShow = 'Click to show';</script>
    <script >let toggleHintHide = 'Click to hide';</script>
    <script >let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="3. Classification and Feature Engineering" href="03_classification-and-feature-engineering.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/datalab-logo-full-color-rgb.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Natural Language Processing with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Overview
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_logistics.html">
   1. Before We Begin…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_text-annotation-with-spacy.html">
   2. Text Annotation with spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_classification-and-feature-engineering.html">
   3. Classification and Feature Engineering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Word Embeddings
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/04_word-embeddings.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ucdavisdatalab/workshop_nlp_with_python"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ucdavisdatalab/workshop_nlp_with_python/issues/new?title=Issue%20on%20page%20%2F04_word-embeddings.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ucdavisdatalab/workshop_nlp_with_python/master?urlpath=tree/04_word-embeddings.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-the-data">
   4.1. Loading the Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-an-embeddings-model">
   4.2. Using an Embeddings Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-a-model">
     4.2.1. Loading a model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#token-mappings">
     4.2.2. Token mappings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#out-of-vocabulary-tokens">
     4.2.3. Out-of-vocabulary tokens
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-relationships">
   4.3. Word relationships
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cosine-similarity">
     4.3.1. Cosine similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-the-vector-space">
     4.3.2. Visualizing the vector space
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-relationships">
     4.3.3. Other relationships
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#document-similarity">
   4.4. Document similarity
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="word-embeddings">
<h1><span class="section-number">4. </span>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h1>
<div class="section" id="loading-the-data">
<h2><span class="section-number">4.1. </span>Loading the Data<a class="headerlink" href="#loading-the-data" title="Permalink to this headline">¶</a></h2>
<p>Before we begin working with word embeddings in full, let’s load a corpus manifest file, which will help us keep track of all the obituaries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">manifest</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/session_three/manifest.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">manifest</span> <span class="o">=</span> <span class="n">manifest</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">YEAR</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;YEAR&#39;</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;%Y&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">)</span>

<span class="n">manifest</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;YEAR&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;NAME&#39;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Obituaries per Year&#39;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Num. obituaries&#39;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04_word-embeddings_2_0.png" src="_images/04_word-embeddings_2_0.png" />
</div>
</div>
<p>Here’s a sampling of the corpus:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">manifest</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">name</span><span class="p">,</span> <span class="n">date</span> <span class="o">=</span> <span class="n">manifest</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;NAME&#39;</span><span class="p">],</span> <span class="n">manifest</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;YEAR&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">date</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dean Acheson (1971)
Mohandas K Gandhi (1948)
James Cagney (1986)
Dr Seuss (1991)
Maxim Gorky (1936)
</pre></div>
</div>
</div>
</div>
<p>Now we can load the obituaries themselves. While the past two sessions have required fulltext representations of documents, word embeddings work best with bags of words, especially when it comes to doing analysis with them. Accordingly, each of the files in the corpus have already processed by a text cleaning pipeline: they represent the lowercase, stopped, and lemmatized versions of the originals.</p>
<p>No extra loading considerations are needed here either. We’ll just use <code class="docutils literal notranslate"><span class="pre">glob</span></code> to get our file paths and iterate through the list, loading each document into a <code class="docutils literal notranslate"><span class="pre">corpus</span></code> list. Note that we still must split the file contents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">glob</span>

<span class="n">paths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;data/session_three/obits/*.txt&#39;</span><span class="p">)</span>
<span class="n">paths</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">corpus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With this done, we can move on to the model.</p>
</div>
<div class="section" id="using-an-embeddings-model">
<h2><span class="section-number">4.2. </span>Using an Embeddings Model<a class="headerlink" href="#using-an-embeddings-model" title="Permalink to this headline">¶</a></h2>
<p>At this point, we are at a crossroads. On the one hand, we could train a word embeddings model using our corpus documents as is. The <code class="docutils literal notranslate"><span class="pre">gensim</span></code> library offers functionality for this, and it’s a relatively easy operation. On the other, we could use premade embeddings, which are usually trained on a more general – and much larger – set of documents. There is a tradeoff here:</p>
<ul class="simple">
<li><p>Training a corpus-specific model will more faithfully represent the token behavior of the texts we’d like to analyze, but these representations could be <em>too</em> specific, especially if the model doesn’t have enough data to train on</p></li>
<li><p>Using premade embeddings gives us the benefit of generalization: the vectors will cleave more closely to how we understand language; but such embeddings might a) miss out on certain nuances we’d like to capture, or b) introduce biases into our corpus (more on this below)</p></li>
</ul>
<p>In our case, the decision is difficult. When preparing this reader, we (Tyler and Carl) found that a model trained on the obituaries alone did not produce vectors that could fully demonstrate the capabilities of the word embedding technique. The corpus is just a little too specific, and perhaps a little too small. We could’ve used a larger corpus, but doing so would introduce slow-downs in the workshop session. Because of this, we decided to use a premade model, in this case, the Stanford <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a> embeddings (the 200-dimension version). GloVe was trained on billions of tokens, spanning Wikipedia data, newswire articles, even Twitter. More, the model’s developers offer several different dimension sizes, which are helpful for selecting embeddings with the right amount of detail.</p>
<p>That said, going with GloVe introduces its own problems. For one thing, we can’t show you how to train a word embeddings model itself – at least not live. The code to do so, however, is reproduced below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">n_dimensions</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">n_dimensions</span><span class="p">)</span> <span class="c1"># There are several other optional parameters that we won&#39;t discuss</span>
<span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_total_words</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Another problem has to do with bias. <a class="reference external" href="https://www.technologyreview.com/2016/07/27/158634/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/">Researchers have found</a> that general embeddings models reproduce gender-discriminatory language, even hate speech, by virtue of the fact that they are trained on huge amounts of text data, often without consideration of whether the content of such data is something one would endorse. GloVe is <a class="reference external" href="http://arxiv.org/abs/1607.06520">known to be biased</a> in this way. We’ll show an example later on in this chapter and will discuss this in much more detail during our live session, but for now just note that the effects of bias <em>do</em> shape how we represent our corpus, and it’s important to keep an eye out for this when working with the data.</p>
<div class="section" id="loading-a-model">
<h3><span class="section-number">4.2.1. </span>Loading a model<a class="headerlink" href="#loading-a-model" title="Permalink to this headline">¶</a></h3>
<p>With all that said, we can move on. Below, we load GloVe embeddings into our workspace using a <code class="docutils literal notranslate"><span class="pre">gensim</span></code> wrapper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;data/session_three/glove/glove-wiki-gigaword_200d.bin&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">KeyedVectors</span></code> object acts almost like a dictionary. You can do certain Python operations directly on it, like using <code class="docutils literal notranslate"><span class="pre">len()</span></code> to find the number of tokens in the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of unique tokens in the model: </span><span class="si">{</span><span class="n">n_tokens</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of unique tokens in the model: 400,000
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="token-mappings">
<h3><span class="section-number">4.2.2. </span>Token mappings<a class="headerlink" href="#token-mappings" title="Permalink to this headline">¶</a></h3>
<p>Each token in the model (what <code class="docutils literal notranslate"><span class="pre">gensim</span></code> calls a “key”) has an associated index. This mapping is accessible via the <code class="docutils literal notranslate"><span class="pre">.key_to_index</span></code> attribute:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">key_to_index</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;the&#39;: 0,
 &#39;,&#39;: 1,
 &#39;.&#39;: 2,
 &#39;of&#39;: 3,
 &#39;to&#39;: 4,
 &#39;and&#39;: 5,
 &#39;in&#39;: 6,
 &#39;a&#39;: 7,
 &#39;&quot;&#39;: 8,
 &quot;&#39;s&quot;: 9,
 &#39;for&#39;: 10,
 &#39;-&#39;: 11,
 &#39;that&#39;: 12,
 &#39;on&#39;: 13,
 &#39;is&#39;: 14,
 &#39;was&#39;: 15,
 &#39;said&#39;: 16,
 &#39;with&#39;: 17,
 &#39;he&#39;: 18,
 &#39;as&#39;: 19,
 &#39;it&#39;: 20,
 &#39;by&#39;: 21,
 &#39;at&#39;: 22,
 &#39;(&#39;: 23,
 &#39;)&#39;: 24,
 &#39;from&#39;: 25,
 &#39;his&#39;: 26,
 &quot;&#39;&#39;&quot;: 27,
 &#39;``&#39;: 28,
 &#39;an&#39;: 29,
 &#39;be&#39;: 30,
 &#39;has&#39;: 31,
 &#39;are&#39;: 32,
 &#39;have&#39;: 33,
 &#39;but&#39;: 34,
 &#39;were&#39;: 35,
 &#39;not&#39;: 36,
 &#39;this&#39;: 37,
 &#39;who&#39;: 38,
 &#39;they&#39;: 39,
 &#39;had&#39;: 40,
 &#39;i&#39;: 41,
 &#39;which&#39;: 42,
 &#39;will&#39;: 43,
 &#39;their&#39;: 44,
 &#39;:&#39;: 45,
 &#39;or&#39;: 46,
 &#39;its&#39;: 47,
 &#39;one&#39;: 48,
 &#39;after&#39;: 49,
 &#39;new&#39;: 50,
 &#39;been&#39;: 51,
 &#39;also&#39;: 52,
 &#39;we&#39;: 53,
 &#39;would&#39;: 54,
 &#39;two&#39;: 55,
 &#39;more&#39;: 56,
 &quot;&#39;&quot;: 57,
 &#39;first&#39;: 58,
 &#39;about&#39;: 59,
 &#39;up&#39;: 60,
 &#39;when&#39;: 61,
 &#39;year&#39;: 62,
 &#39;there&#39;: 63,
 &#39;all&#39;: 64,
 &#39;--&#39;: 65,
 &#39;out&#39;: 66,
 &#39;she&#39;: 67,
 &#39;other&#39;: 68,
 &#39;people&#39;: 69,
 &quot;n&#39;t&quot;: 70,
 &#39;her&#39;: 71,
 &#39;percent&#39;: 72,
 &#39;than&#39;: 73,
 &#39;over&#39;: 74,
 &#39;into&#39;: 75,
 &#39;last&#39;: 76,
 &#39;some&#39;: 77,
 &#39;government&#39;: 78,
 &#39;time&#39;: 79,
 &#39;$&#39;: 80,
 &#39;you&#39;: 81,
 &#39;years&#39;: 82,
 &#39;if&#39;: 83,
 &#39;no&#39;: 84,
 &#39;world&#39;: 85,
 &#39;can&#39;: 86,
 &#39;three&#39;: 87,
 &#39;do&#39;: 88,
 &#39;;&#39;: 89,
 &#39;president&#39;: 90,
 &#39;only&#39;: 91,
 &#39;state&#39;: 92,
 &#39;million&#39;: 93,
 &#39;could&#39;: 94,
 &#39;us&#39;: 95,
 &#39;most&#39;: 96,
 &#39;_&#39;: 97,
 &#39;against&#39;: 98,
 &#39;u.s.&#39;: 99,
 &#39;so&#39;: 100,
 &#39;them&#39;: 101,
 &#39;what&#39;: 102,
 &#39;him&#39;: 103,
 &#39;united&#39;: 104,
 &#39;during&#39;: 105,
 &#39;before&#39;: 106,
 &#39;may&#39;: 107,
 &#39;since&#39;: 108,
 &#39;many&#39;: 109,
 &#39;while&#39;: 110,
 &#39;where&#39;: 111,
 &#39;states&#39;: 112,
 &#39;because&#39;: 113,
 &#39;now&#39;: 114,
 &#39;city&#39;: 115,
 &#39;made&#39;: 116,
 &#39;like&#39;: 117,
 &#39;between&#39;: 118,
 &#39;did&#39;: 119,
 &#39;just&#39;: 120,
 &#39;national&#39;: 121,
 &#39;day&#39;: 122,
 &#39;country&#39;: 123,
 &#39;under&#39;: 124,
 &#39;such&#39;: 125,
 &#39;second&#39;: 126,
 &#39;then&#39;: 127,
 &#39;company&#39;: 128,
 &#39;group&#39;: 129,
 &#39;any&#39;: 130,
 &#39;through&#39;: 131,
 &#39;china&#39;: 132,
 &#39;four&#39;: 133,
 &#39;being&#39;: 134,
 &#39;down&#39;: 135,
 &#39;war&#39;: 136,
 &#39;back&#39;: 137,
 &#39;off&#39;: 138,
 &#39;south&#39;: 139,
 &#39;american&#39;: 140,
 &#39;minister&#39;: 141,
 &#39;police&#39;: 142,
 &#39;well&#39;: 143,
 &#39;including&#39;: 144,
 &#39;team&#39;: 145,
 &#39;international&#39;: 146,
 &#39;week&#39;: 147,
 &#39;officials&#39;: 148,
 &#39;still&#39;: 149,
 &#39;both&#39;: 150,
 &#39;even&#39;: 151,
 &#39;high&#39;: 152,
 &#39;part&#39;: 153,
 &#39;told&#39;: 154,
 &#39;those&#39;: 155,
 &#39;end&#39;: 156,
 &#39;former&#39;: 157,
 &#39;these&#39;: 158,
 &#39;make&#39;: 159,
 &#39;billion&#39;: 160,
 &#39;work&#39;: 161,
 &#39;our&#39;: 162,
 &#39;home&#39;: 163,
 &#39;school&#39;: 164,
 &#39;party&#39;: 165,
 &#39;house&#39;: 166,
 &#39;old&#39;: 167,
 &#39;later&#39;: 168,
 &#39;get&#39;: 169,
 &#39;another&#39;: 170,
 &#39;tuesday&#39;: 171,
 &#39;news&#39;: 172,
 &#39;long&#39;: 173,
 &#39;five&#39;: 174,
 &#39;called&#39;: 175,
 &#39;1&#39;: 176,
 &#39;wednesday&#39;: 177,
 &#39;military&#39;: 178,
 &#39;way&#39;: 179,
 &#39;used&#39;: 180,
 &#39;much&#39;: 181,
 &#39;next&#39;: 182,
 &#39;monday&#39;: 183,
 &#39;thursday&#39;: 184,
 &#39;friday&#39;: 185,
 &#39;game&#39;: 186,
 &#39;here&#39;: 187,
 &#39;?&#39;: 188,
 &#39;should&#39;: 189,
 &#39;take&#39;: 190,
 &#39;very&#39;: 191,
 &#39;my&#39;: 192,
 &#39;north&#39;: 193,
 &#39;security&#39;: 194,
 &#39;season&#39;: 195,
 &#39;york&#39;: 196,
 &#39;how&#39;: 197,
 &#39;public&#39;: 198,
 &#39;early&#39;: 199,
 &#39;according&#39;: 200,
 &#39;several&#39;: 201,
 &#39;court&#39;: 202,
 &#39;say&#39;: 203,
 &#39;around&#39;: 204,
 &#39;foreign&#39;: 205,
 &#39;10&#39;: 206,
 &#39;until&#39;: 207,
 &#39;set&#39;: 208,
 &#39;political&#39;: 209,
 &#39;says&#39;: 210,
 &#39;market&#39;: 211,
 &#39;however&#39;: 212,
 &#39;family&#39;: 213,
 &#39;life&#39;: 214,
 &#39;same&#39;: 215,
 &#39;general&#39;: 216,
 &#39;–&#39;: 217,
 &#39;left&#39;: 218,
 &#39;good&#39;: 219,
 &#39;top&#39;: 220,
 &#39;university&#39;: 221,
 &#39;going&#39;: 222,
 &#39;number&#39;: 223,
 &#39;major&#39;: 224,
 &#39;known&#39;: 225,
 &#39;points&#39;: 226,
 &#39;won&#39;: 227,
 &#39;six&#39;: 228,
 &#39;month&#39;: 229,
 &#39;dollars&#39;: 230,
 &#39;bank&#39;: 231,
 &#39;2&#39;: 232,
 &#39;iraq&#39;: 233,
 &#39;use&#39;: 234,
 &#39;members&#39;: 235,
 &#39;each&#39;: 236,
 &#39;area&#39;: 237,
 &#39;found&#39;: 238,
 &#39;official&#39;: 239,
 &#39;sunday&#39;: 240,
 &#39;place&#39;: 241,
 &#39;go&#39;: 242,
 &#39;based&#39;: 243,
 &#39;among&#39;: 244,
 &#39;third&#39;: 245,
 &#39;times&#39;: 246,
 &#39;took&#39;: 247,
 &#39;right&#39;: 248,
 &#39;days&#39;: 249,
 &#39;local&#39;: 250,
 &#39;economic&#39;: 251,
 &#39;countries&#39;: 252,
 &#39;see&#39;: 253,
 &#39;best&#39;: 254,
 &#39;report&#39;: 255,
 &#39;killed&#39;: 256,
 &#39;held&#39;: 257,
 &#39;business&#39;: 258,
 &#39;west&#39;: 259,
 &#39;does&#39;: 260,
 &#39;own&#39;: 261,
 &#39;%&#39;: 262,
 &#39;came&#39;: 263,
 &#39;law&#39;: 264,
 &#39;months&#39;: 265,
 &#39;women&#39;: 266,
 &quot;&#39;re&quot;: 267,
 &#39;power&#39;: 268,
 &#39;think&#39;: 269,
 &#39;service&#39;: 270,
 &#39;children&#39;: 271,
 &#39;bush&#39;: 272,
 &#39;show&#39;: 273,
 &#39;/&#39;: 274,
 &#39;help&#39;: 275,
 &#39;chief&#39;: 276,
 &#39;saturday&#39;: 277,
 &#39;system&#39;: 278,
 &#39;john&#39;: 279,
 &#39;support&#39;: 280,
 &#39;series&#39;: 281,
 &#39;play&#39;: 282,
 &#39;office&#39;: 283,
 &#39;following&#39;: 284,
 &#39;me&#39;: 285,
 &#39;meeting&#39;: 286,
 &#39;expected&#39;: 287,
 &#39;late&#39;: 288,
 &#39;washington&#39;: 289,
 &#39;games&#39;: 290,
 &#39;european&#39;: 291,
 &#39;league&#39;: 292,
 &#39;reported&#39;: 293,
 &#39;final&#39;: 294,
 &#39;added&#39;: 295,
 &#39;without&#39;: 296,
 &#39;british&#39;: 297,
 &#39;white&#39;: 298,
 &#39;history&#39;: 299,
 &#39;man&#39;: 300,
 &#39;men&#39;: 301,
 &#39;became&#39;: 302,
 &#39;want&#39;: 303,
 &#39;march&#39;: 304,
 &#39;case&#39;: 305,
 &#39;few&#39;: 306,
 &#39;run&#39;: 307,
 &#39;money&#39;: 308,
 &#39;began&#39;: 309,
 &#39;open&#39;: 310,
 &#39;name&#39;: 311,
 &#39;trade&#39;: 312,
 &#39;center&#39;: 313,
 &#39;3&#39;: 314,
 &#39;israel&#39;: 315,
 &#39;oil&#39;: 316,
 &#39;too&#39;: 317,
 &#39;al&#39;: 318,
 &#39;film&#39;: 319,
 &#39;win&#39;: 320,
 &#39;led&#39;: 321,
 &#39;east&#39;: 322,
 &#39;central&#39;: 323,
 &#39;20&#39;: 324,
 &#39;air&#39;: 325,
 &#39;come&#39;: 326,
 &#39;chinese&#39;: 327,
 &#39;town&#39;: 328,
 &#39;leader&#39;: 329,
 &#39;army&#39;: 330,
 &#39;line&#39;: 331,
 &#39;never&#39;: 332,
 &#39;little&#39;: 333,
 &#39;played&#39;: 334,
 &#39;prime&#39;: 335,
 &#39;death&#39;: 336,
 &#39;companies&#39;: 337,
 &#39;least&#39;: 338,
 &#39;put&#39;: 339,
 &#39;forces&#39;: 340,
 &#39;past&#39;: 341,
 &#39;de&#39;: 342,
 &#39;half&#39;: 343,
 &#39;june&#39;: 344,
 &#39;saying&#39;: 345,
 &#39;know&#39;: 346,
 &#39;federal&#39;: 347,
 &#39;french&#39;: 348,
 &#39;peace&#39;: 349,
 &#39;earlier&#39;: 350,
 &#39;capital&#39;: 351,
 &#39;force&#39;: 352,
 &#39;great&#39;: 353,
 &#39;union&#39;: 354,
 &#39;near&#39;: 355,
 &#39;released&#39;: 356,
 &#39;small&#39;: 357,
 &#39;department&#39;: 358,
 &#39;every&#39;: 359,
 &#39;health&#39;: 360,
 &#39;japan&#39;: 361,
 &#39;head&#39;: 362,
 &#39;ago&#39;: 363,
 &#39;night&#39;: 364,
 &#39;big&#39;: 365,
 &#39;cup&#39;: 366,
 &#39;election&#39;: 367,
 &#39;region&#39;: 368,
 &#39;director&#39;: 369,
 &#39;talks&#39;: 370,
 &#39;program&#39;: 371,
 &#39;far&#39;: 372,
 &#39;today&#39;: 373,
 &#39;statement&#39;: 374,
 &#39;july&#39;: 375,
 &#39;although&#39;: 376,
 &#39;district&#39;: 377,
 &#39;again&#39;: 378,
 &#39;born&#39;: 379,
 &#39;development&#39;: 380,
 &#39;leaders&#39;: 381,
 &#39;council&#39;: 382,
 &#39;close&#39;: 383,
 &#39;record&#39;: 384,
 &#39;along&#39;: 385,
 &#39;county&#39;: 386,
 &#39;france&#39;: 387,
 &#39;went&#39;: 388,
 &#39;point&#39;: 389,
 &#39;must&#39;: 390,
 &#39;spokesman&#39;: 391,
 &#39;your&#39;: 392,
 &#39;member&#39;: 393,
 &#39;plan&#39;: 394,
 &#39;financial&#39;: 395,
 &#39;april&#39;: 396,
 &#39;recent&#39;: 397,
 &#39;campaign&#39;: 398,
 &#39;become&#39;: 399,
 &#39;troops&#39;: 400,
 &#39;whether&#39;: 401,
 &#39;lost&#39;: 402,
 &#39;music&#39;: 403,
 &#39;15&#39;: 404,
 &#39;got&#39;: 405,
 &#39;israeli&#39;: 406,
 &#39;30&#39;: 407,
 &#39;need&#39;: 408,
 &#39;4&#39;: 409,
 &#39;lead&#39;: 410,
 &#39;already&#39;: 411,
 &#39;russia&#39;: 412,
 &#39;though&#39;: 413,
 &#39;might&#39;: 414,
 &#39;free&#39;: 415,
 &#39;hit&#39;: 416,
 &#39;rights&#39;: 417,
 &#39;11&#39;: 418,
 &#39;information&#39;: 419,
 &#39;away&#39;: 420,
 &#39;12&#39;: 421,
 &#39;5&#39;: 422,
 &#39;others&#39;: 423,
 &#39;control&#39;: 424,
 &#39;within&#39;: 425,
 &#39;large&#39;: 426,
 &#39;economy&#39;: 427,
 &#39;press&#39;: 428,
 &#39;agency&#39;: 429,
 &#39;water&#39;: 430,
 &#39;died&#39;: 431,
 &#39;career&#39;: 432,
 &#39;making&#39;: 433,
 &#39;...&#39;: 434,
 &#39;deal&#39;: 435,
 &#39;attack&#39;: 436,
 &#39;side&#39;: 437,
 &#39;seven&#39;: 438,
 &#39;better&#39;: 439,
 &#39;less&#39;: 440,
 &#39;september&#39;: 441,
 &#39;once&#39;: 442,
 &#39;clinton&#39;: 443,
 &#39;main&#39;: 444,
 &#39;due&#39;: 445,
 &#39;committee&#39;: 446,
 &#39;building&#39;: 447,
 &#39;conference&#39;: 448,
 &#39;club&#39;: 449,
 &#39;january&#39;: 450,
 &#39;decision&#39;: 451,
 &#39;stock&#39;: 452,
 &#39;america&#39;: 453,
 &#39;given&#39;: 454,
 &#39;give&#39;: 455,
 &#39;often&#39;: 456,
 &#39;announced&#39;: 457,
 &#39;television&#39;: 458,
 &#39;industry&#39;: 459,
 &#39;order&#39;: 460,
 &#39;young&#39;: 461,
 &quot;&#39;ve&quot;: 462,
 &#39;palestinian&#39;: 463,
 &#39;age&#39;: 464,
 &#39;start&#39;: 465,
 &#39;administration&#39;: 466,
 &#39;russian&#39;: 467,
 &#39;prices&#39;: 468,
 &#39;round&#39;: 469,
 &#39;december&#39;: 470,
 &#39;nations&#39;: 471,
 &quot;&#39;m&quot;: 472,
 &#39;human&#39;: 473,
 &#39;india&#39;: 474,
 &#39;defense&#39;: 475,
 &#39;asked&#39;: 476,
 &#39;total&#39;: 477,
 &#39;october&#39;: 478,
 &#39;players&#39;: 479,
 &#39;bill&#39;: 480,
 &#39;important&#39;: 481,
 &#39;southern&#39;: 482,
 &#39;move&#39;: 483,
 &#39;fire&#39;: 484,
 &#39;population&#39;: 485,
 &#39;rose&#39;: 486,
 &#39;november&#39;: 487,
 &#39;include&#39;: 488,
 &#39;further&#39;: 489,
 &#39;nuclear&#39;: 490,
 &#39;street&#39;: 491,
 &#39;taken&#39;: 492,
 &#39;media&#39;: 493,
 &#39;different&#39;: 494,
 &#39;issue&#39;: 495,
 &#39;received&#39;: 496,
 &#39;secretary&#39;: 497,
 &#39;return&#39;: 498,
 &#39;college&#39;: 499,
 &#39;working&#39;: 500,
 &#39;community&#39;: 501,
 &#39;eight&#39;: 502,
 &#39;groups&#39;: 503,
 &#39;despite&#39;: 504,
 &#39;level&#39;: 505,
 &#39;largest&#39;: 506,
 &#39;whose&#39;: 507,
 &#39;attacks&#39;: 508,
 &#39;germany&#39;: 509,
 &#39;august&#39;: 510,
 &#39;change&#39;: 511,
 &#39;church&#39;: 512,
 &#39;nation&#39;: 513,
 &#39;german&#39;: 514,
 &#39;station&#39;: 515,
 &#39;london&#39;: 516,
 &#39;weeks&#39;: 517,
 &#39;having&#39;: 518,
 &#39;18&#39;: 519,
 &#39;research&#39;: 520,
 &#39;black&#39;: 521,
 &#39;services&#39;: 522,
 &#39;story&#39;: 523,
 &#39;6&#39;: 524,
 &#39;europe&#39;: 525,
 &#39;sales&#39;: 526,
 &#39;policy&#39;: 527,
 &#39;visit&#39;: 528,
 &#39;northern&#39;: 529,
 &#39;lot&#39;: 530,
 &#39;across&#39;: 531,
 &#39;per&#39;: 532,
 &#39;current&#39;: 533,
 &#39;board&#39;: 534,
 &#39;football&#39;: 535,
 &#39;ministry&#39;: 536,
 &#39;workers&#39;: 537,
 &#39;vote&#39;: 538,
 &#39;book&#39;: 539,
 &#39;fell&#39;: 540,
 &#39;seen&#39;: 541,
 &#39;role&#39;: 542,
 &#39;students&#39;: 543,
 &#39;shares&#39;: 544,
 &#39;iran&#39;: 545,
 &#39;process&#39;: 546,
 &#39;agreement&#39;: 547,
 &#39;quarter&#39;: 548,
 &#39;full&#39;: 549,
 &#39;match&#39;: 550,
 &#39;started&#39;: 551,
 &#39;growth&#39;: 552,
 &#39;yet&#39;: 553,
 &#39;moved&#39;: 554,
 &#39;possible&#39;: 555,
 &#39;western&#39;: 556,
 &#39;special&#39;: 557,
 &#39;100&#39;: 558,
 &#39;plans&#39;: 559,
 &#39;interest&#39;: 560,
 &#39;behind&#39;: 561,
 &#39;strong&#39;: 562,
 &#39;england&#39;: 563,
 &#39;named&#39;: 564,
 &#39;food&#39;: 565,
 &#39;period&#39;: 566,
 &#39;real&#39;: 567,
 &#39;authorities&#39;: 568,
 &#39;car&#39;: 569,
 &#39;term&#39;: 570,
 &#39;rate&#39;: 571,
 &#39;race&#39;: 572,
 &#39;nearly&#39;: 573,
 &#39;korea&#39;: 574,
 &#39;enough&#39;: 575,
 &#39;site&#39;: 576,
 &#39;opposition&#39;: 577,
 &#39;keep&#39;: 578,
 &#39;25&#39;: 579,
 &#39;call&#39;: 580,
 &#39;future&#39;: 581,
 &#39;taking&#39;: 582,
 &#39;island&#39;: 583,
 &#39;2008&#39;: 584,
 &#39;2006&#39;: 585,
 &#39;road&#39;: 586,
 &#39;outside&#39;: 587,
 &#39;really&#39;: 588,
 &#39;century&#39;: 589,
 &#39;democratic&#39;: 590,
 &#39;almost&#39;: 591,
 &#39;single&#39;: 592,
 &#39;share&#39;: 593,
 &#39;leading&#39;: 594,
 &#39;trying&#39;: 595,
 &#39;find&#39;: 596,
 &#39;album&#39;: 597,
 &#39;senior&#39;: 598,
 &#39;minutes&#39;: 599,
 &#39;together&#39;: 600,
 &#39;congress&#39;: 601,
 &#39;index&#39;: 602,
 &#39;australia&#39;: 603,
 &#39;results&#39;: 604,
 &#39;hard&#39;: 605,
 &#39;hours&#39;: 606,
 &#39;land&#39;: 607,
 &#39;action&#39;: 608,
 &#39;higher&#39;: 609,
 &#39;field&#39;: 610,
 &#39;cut&#39;: 611,
 &#39;coach&#39;: 612,
 &#39;elections&#39;: 613,
 &#39;san&#39;: 614,
 &#39;issues&#39;: 615,
 &#39;executive&#39;: 616,
 &#39;february&#39;: 617,
 &#39;production&#39;: 618,
 &#39;areas&#39;: 619,
 &#39;river&#39;: 620,
 &#39;face&#39;: 621,
 &#39;using&#39;: 622,
 &#39;japanese&#39;: 623,
 &#39;province&#39;: 624,
 &#39;park&#39;: 625,
 &#39;price&#39;: 626,
 &#39;commission&#39;: 627,
 &#39;california&#39;: 628,
 &#39;father&#39;: 629,
 &#39;son&#39;: 630,
 &#39;education&#39;: 631,
 &#39;7&#39;: 632,
 &#39;village&#39;: 633,
 &#39;energy&#39;: 634,
 &#39;shot&#39;: 635,
 &#39;short&#39;: 636,
 &#39;africa&#39;: 637,
 &#39;key&#39;: 638,
 &#39;red&#39;: 639,
 &#39;association&#39;: 640,
 &#39;average&#39;: 641,
 &#39;pay&#39;: 642,
 &#39;exchange&#39;: 643,
 &#39;eu&#39;: 644,
 &#39;something&#39;: 645,
 &#39;gave&#39;: 646,
 &#39;likely&#39;: 647,
 &#39;player&#39;: 648,
 &#39;george&#39;: 649,
 &#39;2007&#39;: 650,
 &#39;victory&#39;: 651,
 &#39;8&#39;: 652,
 &#39;low&#39;: 653,
 &#39;things&#39;: 654,
 &#39;2010&#39;: 655,
 &#39;pakistan&#39;: 656,
 &#39;14&#39;: 657,
 &#39;post&#39;: 658,
 &#39;social&#39;: 659,
 &#39;continue&#39;: 660,
 &#39;ever&#39;: 661,
 &#39;look&#39;: 662,
 &#39;chairman&#39;: 663,
 &#39;job&#39;: 664,
 &#39;2000&#39;: 665,
 &#39;soldiers&#39;: 666,
 &#39;able&#39;: 667,
 &#39;parliament&#39;: 668,
 &#39;front&#39;: 669,
 &#39;himself&#39;: 670,
 &#39;problems&#39;: 671,
 &#39;private&#39;: 672,
 &#39;lower&#39;: 673,
 &#39;list&#39;: 674,
 &#39;built&#39;: 675,
 &#39;13&#39;: 676,
 &#39;efforts&#39;: 677,
 &#39;dollar&#39;: 678,
 &#39;miles&#39;: 679,
 &#39;included&#39;: 680,
 &#39;radio&#39;: 681,
 &#39;live&#39;: 682,
 &#39;form&#39;: 683,
 &#39;david&#39;: 684,
 &#39;african&#39;: 685,
 &#39;increase&#39;: 686,
 &#39;reports&#39;: 687,
 &#39;sent&#39;: 688,
 &#39;fourth&#39;: 689,
 &#39;always&#39;: 690,
 &#39;king&#39;: 691,
 &#39;50&#39;: 692,
 &#39;tax&#39;: 693,
 &#39;taiwan&#39;: 694,
 &#39;britain&#39;: 695,
 &#39;16&#39;: 696,
 &#39;playing&#39;: 697,
 &#39;title&#39;: 698,
 &#39;middle&#39;: 699,
 &#39;meet&#39;: 700,
 &#39;global&#39;: 701,
 &#39;wife&#39;: 702,
 &#39;2009&#39;: 703,
 &#39;position&#39;: 704,
 &#39;located&#39;: 705,
 &#39;clear&#39;: 706,
 &#39;ahead&#39;: 707,
 &#39;2004&#39;: 708,
 &#39;2005&#39;: 709,
 &#39;iraqi&#39;: 710,
 &#39;english&#39;: 711,
 &#39;result&#39;: 712,
 &#39;release&#39;: 713,
 &#39;violence&#39;: 714,
 &#39;goal&#39;: 715,
 &#39;project&#39;: 716,
 &#39;closed&#39;: 717,
 &#39;border&#39;: 718,
 &#39;body&#39;: 719,
 &#39;soon&#39;: 720,
 &#39;crisis&#39;: 721,
 &#39;division&#39;: 722,
 &#39;&amp;amp;&#39;: 723,
 &#39;served&#39;: 724,
 &#39;tour&#39;: 725,
 &#39;hospital&#39;: 726,
 &#39;kong&#39;: 727,
 &#39;test&#39;: 728,
 &#39;hong&#39;: 729,
 &#39;u.n.&#39;: 730,
 &#39;inc.&#39;: 731,
 &#39;technology&#39;: 732,
 &#39;believe&#39;: 733,
 &#39;organization&#39;: 734,
 &#39;published&#39;: 735,
 &#39;weapons&#39;: 736,
 &#39;agreed&#39;: 737,
 &#39;why&#39;: 738,
 &#39;nine&#39;: 739,
 &#39;summer&#39;: 740,
 &#39;wanted&#39;: 741,
 &#39;republican&#39;: 742,
 &#39;act&#39;: 743,
 &#39;recently&#39;: 744,
 &#39;texas&#39;: 745,
 &#39;course&#39;: 746,
 &#39;problem&#39;: 747,
 &#39;senate&#39;: 748,
 &#39;medical&#39;: 749,
 &#39;un&#39;: 750,
 &#39;done&#39;: 751,
 &#39;reached&#39;: 752,
 &#39;star&#39;: 753,
 &#39;continued&#39;: 754,
 &#39;investors&#39;: 755,
 &#39;living&#39;: 756,
 &#39;care&#39;: 757,
 &#39;signed&#39;: 758,
 &#39;17&#39;: 759,
 &#39;art&#39;: 760,
 &#39;provide&#39;: 761,
 &#39;worked&#39;: 762,
 &#39;presidential&#39;: 763,
 &#39;gold&#39;: 764,
 &#39;obama&#39;: 765,
 &#39;morning&#39;: 766,
 &#39;dead&#39;: 767,
 &#39;opened&#39;: 768,
 &quot;&#39;ll&quot;: 769,
 &#39;event&#39;: 770,
 &#39;previous&#39;: 771,
 &#39;cost&#39;: 772,
 &#39;instead&#39;: 773,
 &#39;canada&#39;: 774,
 &#39;band&#39;: 775,
 &#39;teams&#39;: 776,
 &#39;daily&#39;: 777,
 &#39;2001&#39;: 778,
 &#39;available&#39;: 779,
 &#39;drug&#39;: 780,
 &#39;coming&#39;: 781,
 &#39;2003&#39;: 782,
 &#39;investment&#39;: 783,
 &#39;’s&#39;: 784,
 &#39;michael&#39;: 785,
 &#39;civil&#39;: 786,
 &#39;woman&#39;: 787,
 &#39;training&#39;: 788,
 &#39;appeared&#39;: 789,
 &#39;9&#39;: 790,
 &#39;involved&#39;: 791,
 &#39;indian&#39;: 792,
 &#39;similar&#39;: 793,
 &#39;situation&#39;: 794,
 &#39;24&#39;: 795,
 &#39;los&#39;: 796,
 &#39;running&#39;: 797,
 &#39;fighting&#39;: 798,
 &#39;mark&#39;: 799,
 &#39;40&#39;: 800,
 &#39;trial&#39;: 801,
 &#39;hold&#39;: 802,
 &#39;australian&#39;: 803,
 &#39;thought&#39;: 804,
 &#39;!&#39;: 805,
 &#39;study&#39;: 806,
 &#39;fall&#39;: 807,
 &#39;mother&#39;: 808,
 &#39;met&#39;: 809,
 &#39;relations&#39;: 810,
 &#39;anti&#39;: 811,
 &#39;2002&#39;: 812,
 &#39;song&#39;: 813,
 &#39;popular&#39;: 814,
 &#39;base&#39;: 815,
 &#39;tv&#39;: 816,
 &#39;ground&#39;: 817,
 &#39;markets&#39;: 818,
 &#39;ii&#39;: 819,
 &#39;newspaper&#39;: 820,
 &#39;staff&#39;: 821,
 &#39;saw&#39;: 822,
 &#39;hand&#39;: 823,
 &#39;hope&#39;: 824,
 &#39;operations&#39;: 825,
 &#39;pressure&#39;: 826,
 &#39;americans&#39;: 827,
 &#39;eastern&#39;: 828,
 &#39;st.&#39;: 829,
 &#39;legal&#39;: 830,
 &#39;asia&#39;: 831,
 &#39;budget&#39;: 832,
 &#39;returned&#39;: 833,
 &#39;considered&#39;: 834,
 &#39;love&#39;: 835,
 &#39;wrote&#39;: 836,
 &#39;stop&#39;: 837,
 &#39;fight&#39;: 838,
 &#39;currently&#39;: 839,
 &#39;charges&#39;: 840,
 &#39;try&#39;: 841,
 &#39;aid&#39;: 842,
 &#39;ended&#39;: 843,
 &#39;management&#39;: 844,
 &#39;brought&#39;: 845,
 &#39;cases&#39;: 846,
 &#39;decided&#39;: 847,
 &#39;failed&#39;: 848,
 &#39;network&#39;: 849,
 &#39;works&#39;: 850,
 &#39;gas&#39;: 851,
 &#39;turned&#39;: 852,
 &#39;fact&#39;: 853,
 &#39;vice&#39;: 854,
 &#39;ca&#39;: 855,
 &#39;mexico&#39;: 856,
 &#39;trading&#39;: 857,
 &#39;especially&#39;: 858,
 &#39;reporters&#39;: 859,
 &#39;afghanistan&#39;: 860,
 &#39;common&#39;: 861,
 &#39;looking&#39;: 862,
 &#39;space&#39;: 863,
 &#39;rates&#39;: 864,
 &#39;manager&#39;: 865,
 &#39;loss&#39;: 866,
 &#39;2011&#39;: 867,
 &#39;justice&#39;: 868,
 &#39;thousands&#39;: 869,
 &#39;james&#39;: 870,
 &#39;rather&#39;: 871,
 &#39;fund&#39;: 872,
 &#39;thing&#39;: 873,
 &#39;republic&#39;: 874,
 &#39;opening&#39;: 875,
 &#39;accused&#39;: 876,
 &#39;winning&#39;: 877,
 &#39;scored&#39;: 878,
 &#39;championship&#39;: 879,
 &#39;example&#39;: 880,
 &#39;getting&#39;: 881,
 &#39;biggest&#39;: 882,
 &#39;performance&#39;: 883,
 &#39;sports&#39;: 884,
 &#39;1998&#39;: 885,
 &#39;let&#39;: 886,
 &#39;allowed&#39;: 887,
 &#39;schools&#39;: 888,
 &#39;means&#39;: 889,
 &#39;turn&#39;: 890,
 &#39;leave&#39;: 891,
 &#39;no.&#39;: 892,
 &#39;robert&#39;: 893,
 &#39;personal&#39;: 894,
 &#39;stocks&#39;: 895,
 &#39;showed&#39;: 896,
 &#39;light&#39;: 897,
 &#39;arrested&#39;: 898,
 &#39;person&#39;: 899,
 &#39;either&#39;: 900,
 &#39;offer&#39;: 901,
 &#39;majority&#39;: 902,
 &#39;battle&#39;: 903,
 &#39;19&#39;: 904,
 &#39;class&#39;: 905,
 &#39;evidence&#39;: 906,
 &#39;makes&#39;: 907,
 &#39;society&#39;: 908,
 &#39;products&#39;: 909,
 &#39;regional&#39;: 910,
 &#39;needed&#39;: 911,
 &#39;stage&#39;: 912,
 &#39;am&#39;: 913,
 &#39;doing&#39;: 914,
 &#39;families&#39;: 915,
 &#39;construction&#39;: 916,
 &#39;various&#39;: 917,
 &#39;1996&#39;: 918,
 &#39;sold&#39;: 919,
 &#39;independent&#39;: 920,
 &#39;kind&#39;: 921,
 &#39;airport&#39;: 922,
 &#39;paul&#39;: 923,
 &#39;judge&#39;: 924,
 &#39;internet&#39;: 925,
 &#39;movement&#39;: 926,
 &#39;room&#39;: 927,
 &#39;followed&#39;: 928,
 &#39;original&#39;: 929,
 &#39;angeles&#39;: 930,
 &#39;italy&#39;: 931,
 &#39;`&#39;: 932,
 &#39;data&#39;: 933,
 &#39;comes&#39;: 934,
 &#39;parties&#39;: 935,
 &#39;nothing&#39;: 936,
 &#39;sea&#39;: 937,
 &#39;bring&#39;: 938,
 &#39;2012&#39;: 939,
 &#39;annual&#39;: 940,
 &#39;officer&#39;: 941,
 &#39;beijing&#39;: 942,
 &#39;present&#39;: 943,
 &#39;remain&#39;: 944,
 &#39;nato&#39;: 945,
 &#39;1999&#39;: 946,
 &#39;22&#39;: 947,
 &#39;remains&#39;: 948,
 &#39;allow&#39;: 949,
 &#39;florida&#39;: 950,
 &#39;computer&#39;: 951,
 &#39;21&#39;: 952,
 &#39;contract&#39;: 953,
 &#39;coast&#39;: 954,
 &#39;created&#39;: 955,
 &#39;demand&#39;: 956,
 &#39;operation&#39;: 957,
 &#39;events&#39;: 958,
 &#39;islamic&#39;: 959,
 &#39;beat&#39;: 960,
 &#39;analysts&#39;: 961,
 &#39;interview&#39;: 962,
 &#39;helped&#39;: 963,
 &#39;child&#39;: 964,
 &#39;probably&#39;: 965,
 &#39;spent&#39;: 966,
 &#39;asian&#39;: 967,
 &#39;effort&#39;: 968,
 &#39;cooperation&#39;: 969,
 &#39;shows&#39;: 970,
 &#39;calls&#39;: 971,
 &#39;investigation&#39;: 972,
 &#39;lives&#39;: 973,
 &#39;video&#39;: 974,
 &#39;yen&#39;: 975,
 &#39;runs&#39;: 976,
 &#39;tried&#39;: 977,
 &#39;bad&#39;: 978,
 &#39;described&#39;: 979,
 &#39;1994&#39;: 980,
 &#39;toward&#39;: 981,
 &#39;written&#39;: 982,
 &#39;throughout&#39;: 983,
 &#39;established&#39;: 984,
 &#39;mission&#39;: 985,
 &#39;associated&#39;: 986,
 &#39;buy&#39;: 987,
 &#39;growing&#39;: 988,
 &#39;green&#39;: 989,
 &#39;forward&#39;: 990,
 &#39;competition&#39;: 991,
 &#39;poor&#39;: 992,
 &#39;latest&#39;: 993,
 &#39;banks&#39;: 994,
 &#39;question&#39;: 995,
 &#39;1997&#39;: 996,
 &#39;prison&#39;: 997,
 &#39;feel&#39;: 998,
 &#39;attention&#39;: 999,
 ...}
</pre></div>
</div>
</div>
</div>
<p>If you want to get the vector representation for a token, you can use either the key or the index. The syntax is just like a Python <code class="docutils literal notranslate"><span class="pre">dict</span></code>. Below, we randomly select a single token from the model vocabulary’s <code class="docutils literal notranslate"><span class="pre">.index_to_key</span></code> attribute and find the index associated with it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="n">rand_token</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">rand_idx</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">[</span><span class="n">rand_token</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The index position for &#39;</span><span class="si">{</span><span class="n">rand_token</span><span class="si">}</span><span class="s2">&#39; is </span><span class="si">{</span><span class="n">rand_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The index position for &#39;haslip&#39; is 214697
</pre></div>
</div>
</div>
</div>
<p>Here’s its vector:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.096381 , -0.058649 ,  0.089544 , -0.020958 ,  0.10898  ,
        0.0041636, -0.050918 ,  0.495    , -0.41323  , -0.35049  ,
       -0.22446  , -0.10963  ,  0.15537  , -0.19424  , -0.576    ,
        0.14337  ,  0.54146  , -0.14105  , -0.36195  , -0.023676 ,
        0.28087  , -0.39426  ,  0.29047  , -0.69867  , -0.058583 ,
        0.61096  ,  0.46914  ,  0.62078  , -0.1078   , -0.17606  ,
       -0.067678 ,  0.0053767, -0.22373  ,  0.28312  , -0.17976  ,
       -0.0079747,  0.54096  ,  0.652    ,  0.11734  ,  0.41843  ,
        0.24632  ,  0.26559  ,  0.089592 ,  0.15844  ,  0.42791  ,
       -0.0099089,  0.49841  ,  0.40165  , -0.0033434,  0.032795 ,
       -0.12126  , -0.020271 ,  0.18126  , -0.41415  ,  0.42187  ,
       -0.41819  , -0.33533  , -0.17432  , -0.40626  ,  0.21796  ,
        0.65378  ,  0.086075 ,  0.072884 , -0.37545  , -0.57231  ,
        0.095432 ,  0.071249 , -0.15231  , -0.2737   ,  0.50722  ,
       -0.19941  ,  0.015184 ,  0.13118  , -0.47095  ,  0.09147  ,
       -0.98408  ,  0.091484 , -0.68946  ,  0.61726  ,  0.0093509,
        0.17992  ,  0.0017879,  0.46805  ,  0.079823 ,  0.054726 ,
       -0.6748   ,  0.50645  , -0.10011  , -0.49538  ,  0.4857   ,
        0.19077  , -0.0041027, -0.44744  , -0.31291  , -0.27479  ,
        0.12217  , -0.71155  ,  0.96313  , -0.326    , -0.4231   ,
       -0.57008  , -0.13388  , -0.77206  ,  0.050246 , -0.50174  ,
        0.19938  , -0.72926  , -0.89034  ,  0.12708  , -0.25219  ,
       -0.1521   ,  1.1889   , -0.29362  , -0.64131  ,  0.42514  ,
       -0.32324  ,  0.16517  , -0.040818 ,  0.19145  ,  0.2401   ,
       -0.054054 , -0.078657 ,  0.16185  ,  0.50085  , -0.83182  ,
        0.22139  ,  0.29634  , -0.3098   ,  0.12307  ,  0.1399   ,
       -0.15118  , -0.15959  ,  0.25989  , -0.32824  ,  0.092874 ,
       -0.35002  ,  0.49248  ,  0.82703  , -0.41857  ,  0.080048 ,
        0.15268  ,  0.096924 , -0.50438  ,  0.3962   , -0.015521 ,
       -0.28388  ,  0.14572  , -0.09923  , -0.099976 , -0.11506  ,
        0.32182  , -0.093168 ,  0.41793  ,  0.30836  , -0.16459  ,
       -0.0046936,  0.44663  , -0.29152  ,  0.41165  ,  0.11666  ,
        0.27818  ,  0.73854  ,  0.3754   ,  0.091994 , -0.11484  ,
       -0.021938 ,  0.19324  , -0.058357 ,  0.76173  , -0.68727  ,
        0.050728 ,  0.18188  , -0.30568  , -0.20248  , -0.27157  ,
       -0.037877 , -0.43287  ,  0.36589  ,  0.030838 , -0.20101  ,
       -0.73809  ,  0.12684  ,  0.21765  ,  0.071618 ,  0.44379  ,
       -0.35164  ,  0.19259  , -0.33926  , -0.39215  , -0.049196 ,
       -0.23929  , -0.32415  , -0.1033   , -0.12032  , -0.18609  ,
       -0.22615  , -0.55928  , -0.23131  , -0.097507 , -0.72758  ],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>And here we show that accessing this vector with either the index or key produces the same thing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="n">rand_token</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Finally, we can store the entire model vocabulary in a <code class="docutils literal notranslate"><span class="pre">set</span></code> and show a few examples of the tokens therein.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">model_vocab</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cavies
melnik
lucky
130kg
panglossian
arien
weiers
fighters
catchiness
economical
</pre></div>
</div>
</div>
</div>
<p>You may find some unexpected tokens in this output. Though it has been ostensibly trained on an English corpus, GloVe contains multilingual text. It also contains lots of noisy tokens, which range from erroneous segmentations (“drummer/percussionist” is one token, for example) to password-like strings and even HTML markup. Depending on your task, you may not notice these tokens, but they do in fact influence the overall shape of the model, and sometimes you’ll find them cropping up when you’re hunting around for similar terms and the like (more on this soon).</p>
</div>
<div class="section" id="out-of-vocabulary-tokens">
<h3><span class="section-number">4.2.3. </span>Out-of-vocabulary tokens<a class="headerlink" href="#out-of-vocabulary-tokens" title="Permalink to this headline">¶</a></h3>
<p>While GloVe’s vocabulary sometimes seems <em>too</em> expansive, there are other instances where it’s too restricted.</p>
<div class="cell tag_raises-exception docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="s1">&#39;unshaped&#39;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;Not in vocabulary!&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AssertionError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">h7</span><span class="o">/</span><span class="n">tzxfms7d2z7gwlgtbvw15msc0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_99511</span><span class="o">/</span><span class="mf">744621433.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="k">assert</span> <span class="s1">&#39;unshaped&#39;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;Not in vocabulary!&quot;</span>

<span class="ne">AssertionError</span>: Not in vocabulary!
</pre></div>
</div>
</div>
</div>
<p>If the model wasn’t trained on a particular word, it won’t have a corresponding vector for that word either. This is crucial. Because models like GloVe only know what they’ve been trained on, you need to be aware of any potential discrepancies between their vocabularies and your corpus data. If you don’t keep this in mind, sending unseen, or <strong>out-of-vocabulary</strong> tokens to GloVe will throw errors in your code:</p>
<div class="cell tag_raises-exception docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;unshaped&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">h7</span><span class="o">/</span><span class="n">tzxfms7d2z7gwlgtbvw15msc0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_99511</span><span class="o">/</span><span class="mf">3553718512.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">model</span><span class="p">[</span><span class="s1">&#39;unshaped&#39;</span><span class="p">]</span>

<span class="nn">~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">__getitem__</span><span class="nt">(self, key_or_keys)</span>
<span class="g g-Whitespace">    </span><span class="mi">377</span>         <span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">378</span><span class="sd">         if isinstance(key_or_keys, KEY_TYPES):</span>
<span class="ne">--&gt; </span><span class="mi">379</span><span class="sd">             return self.get_vector(key_or_keys)</span>
<span class="g g-Whitespace">    </span><span class="mi">380</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">381</span><span class="sd">         return vstack([self.get_vector(key) for key in key_or_keys])</span>

<span class="nn">~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">get_vector</span><span class="nt">(self, key, norm)</span>
<span class="g g-Whitespace">    </span><span class="mi">420</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">421</span><span class="sd">         &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">422</span>         <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_index</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">423</span>         <span class="k">if</span> <span class="n">norm</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">424</span>             <span class="bp">self</span><span class="o">.</span><span class="n">fill_norms</span><span class="p">()</span>

<span class="nn">~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">get_index</span><span class="nt">(self, key, default)</span>
<span class="g g-Whitespace">    </span><span class="mi">394</span>             <span class="k">return</span> <span class="n">default</span>
<span class="g g-Whitespace">    </span><span class="mi">395</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">396</span>             <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Key &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; not present&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">397</span> 
<span class="g g-Whitespace">    </span><span class="mi">398</span>     <span class="k">def</span> <span class="nf">get_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

<span class="ne">KeyError</span>: &quot;Key &#39;unshaped&#39; not present&quot;
</pre></div>
</div>
</div>
</div>
<p>There are a few ways to handle this problem. The most common is to simply <em>not encode</em> tokens in your corpus that don’t have a corresponding vector in GloVe. Below, we construct three dictionaries for our corpus data. The first contains all tokens, while the second and third are comprised of tokens that are and are not in Glove, respectively. We identify whether the model has a token using its <code class="docutils literal notranslate"><span class="pre">.has_index_for()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">token</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
<span class="n">in_glove</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">corpus_vocab</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">has_index_for</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
<span class="n">no_glove</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">corpus_vocab</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">has_index_for</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Total words in the corpus vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus_vocab</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Number of corpus words in GloVe: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">in_glove</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Number of corpus words not in GloVe: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">no_glove</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total words in the corpus vocabulary: 29,330 
Number of corpus words in GloVe: 27,488 
Number of corpus words not in GloVe: 1,842
</pre></div>
</div>
</div>
</div>
<p>Any subsequent code we write will need to reference these dictionaries to determine whether it should encode a token.</p>
<p>While this is what we’ll indeed do below, obviously it isn’t an ideal situation. But it’s one of the consequences of using premade models. There are, however, a few other ways to handle out-of-vocabulary terms. Some models offer special “UNK” tokens, which you could associate with all of your problem tokens. This, at the very least, enables you to have <em>some</em> representation of your data. A more complex approach involves taking the mean embedding of the word vectors surrounding an unknown token; and depending on the model, you can also train it further, adding extra tokens from your domain-specific text. Instructions for this last option are available <a class="reference external" href="https://radimrehurek.com/gensim/models/word2vec.html#usage-examples">here</a> in the <code class="docutils literal notranslate"><span class="pre">gensim</span></code> documentation.</p>
</div>
</div>
<div class="section" id="word-relationships">
<h2><span class="section-number">4.3. </span>Word relationships<a class="headerlink" href="#word-relationships" title="Permalink to this headline">¶</a></h2>
<p>Later on we’ll use GloVe to encode our corpus texts. But before we do, it’s worth demonstrating more generally some of the properties of word vectors. Vector representations of text allow us to perform various mathematical operations on our corpus that approximate (though maybe <em>only</em> approximate) semantics. The most common among these operations is finding the <strong>cosine similarity</strong> between two vectors. Our Getting Started with Textual Data series has a whole <a class="reference external" href="https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/05_clustering-and-classification.html">chapter</a> on this measure, so if you haven’t encountered it before, we recommend you read that. But in short: cosine similarity measures the difference between vectors’ orientation in a feature space; the closer two vectors are, the more likely they are to share semantic similarities.</p>
<div class="section" id="cosine-similarity">
<h3><span class="section-number">4.3.1. </span>Cosine similarity<a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">gensim</span></code> provides easy access to this measure and other such vector space operations, and we can use this functionality to explore relationships between words in a model. To find the cosine similarity between the vectors for two words in GloVe, simply use the model’s <code class="docutils literal notranslate"><span class="pre">.similarity()</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="s1">&#39;calculate&#39;</span><span class="p">,</span> <span class="s1">&#39;compute&#39;</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Consine similarity score for &#39;</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Consine similarity score for &#39;calculate&#39; and &#39;compute&#39;: 0.6991
</pre></div>
</div>
</div>
</div>
<p>The only difference between the score above and the one that you might produce, say, with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s cosine similarity implementation is that <code class="docutils literal notranslate"><span class="pre">gensim</span></code> bounds its values from <code class="docutils literal notranslate"><span class="pre">[-1,1]</span></code>, whereas the latter uses a <code class="docutils literal notranslate"><span class="pre">[0,1]</span></code> scale. While in <code class="docutils literal notranslate"><span class="pre">gensim</span></code> it’s still the case that similar words score closer to <code class="docutils literal notranslate"><span class="pre">1</span></code>, highly dissimilar words will be closer to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p>
<p>At any rate, we can get the top <em>n</em> most similar words for a word using <code class="docutils literal notranslate"><span class="pre">.most_similar()</span></code> (it defaults to 10).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">targets</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">in_glove</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens most similar to &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
    <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;carriage&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>carriages</td>
      <td>0.749064</td>
    </tr>
    <tr>
      <th>1</th>
      <td>horse-drawn</td>
      <td>0.662327</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wagon</td>
      <td>0.615561</td>
    </tr>
    <tr>
      <th>3</th>
      <td>wagons</td>
      <td>0.606641</td>
    </tr>
    <tr>
      <th>4</th>
      <td>locomotive</td>
      <td>0.551395</td>
    </tr>
    <tr>
      <th>5</th>
      <td>horseless</td>
      <td>0.550725</td>
    </tr>
    <tr>
      <th>6</th>
      <td>carts</td>
      <td>0.525195</td>
    </tr>
    <tr>
      <th>7</th>
      <td>rails</td>
      <td>0.522622</td>
    </tr>
    <tr>
      <th>8</th>
      <td>horse</td>
      <td>0.515362</td>
    </tr>
    <tr>
      <th>9</th>
      <td>freight</td>
      <td>0.513062</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;prodigal&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>first-born</td>
      <td>0.491209</td>
    </tr>
    <tr>
      <th>1</th>
      <td>parable</td>
      <td>0.469259</td>
    </tr>
    <tr>
      <th>2</th>
      <td>five-year-old</td>
      <td>0.453422</td>
    </tr>
    <tr>
      <th>3</th>
      <td>sista</td>
      <td>0.431001</td>
    </tr>
    <tr>
      <th>4</th>
      <td>kingsolver</td>
      <td>0.422675</td>
    </tr>
    <tr>
      <th>5</th>
      <td>rambunctious</td>
      <td>0.419960</td>
    </tr>
    <tr>
      <th>6</th>
      <td>six-year-old</td>
      <td>0.415938</td>
    </tr>
    <tr>
      <th>7</th>
      <td>new-born</td>
      <td>0.415167</td>
    </tr>
    <tr>
      <th>8</th>
      <td>seven-year-old</td>
      <td>0.410743</td>
    </tr>
    <tr>
      <th>9</th>
      <td>birthright</td>
      <td>0.410475</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;assertion&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>assertions</td>
      <td>0.865127</td>
    </tr>
    <tr>
      <th>1</th>
      <td>asserted</td>
      <td>0.730507</td>
    </tr>
    <tr>
      <th>2</th>
      <td>asserting</td>
      <td>0.725012</td>
    </tr>
    <tr>
      <th>3</th>
      <td>contradicted</td>
      <td>0.710288</td>
    </tr>
    <tr>
      <th>4</th>
      <td>accusation</td>
      <td>0.687947</td>
    </tr>
    <tr>
      <th>5</th>
      <td>argument</td>
      <td>0.658653</td>
    </tr>
    <tr>
      <th>6</th>
      <td>claim</td>
      <td>0.655640</td>
    </tr>
    <tr>
      <th>7</th>
      <td>allegation</td>
      <td>0.654061</td>
    </tr>
    <tr>
      <th>8</th>
      <td>asserts</td>
      <td>0.643998</td>
    </tr>
    <tr>
      <th>9</th>
      <td>contradicts</td>
      <td>0.637502</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;colombes&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>yves-du-manoir</td>
      <td>0.663367</td>
    </tr>
    <tr>
      <th>1</th>
      <td>nogent-sur-marne</td>
      <td>0.520546</td>
    </tr>
    <tr>
      <th>2</th>
      <td>louveciennes</td>
      <td>0.514641</td>
    </tr>
    <tr>
      <th>3</th>
      <td>suresnes</td>
      <td>0.513956</td>
    </tr>
    <tr>
      <th>4</th>
      <td>neuilly-sur-seine</td>
      <td>0.505479</td>
    </tr>
    <tr>
      <th>5</th>
      <td>beaujoire</td>
      <td>0.497105</td>
    </tr>
    <tr>
      <th>6</th>
      <td>bercy</td>
      <td>0.493436</td>
    </tr>
    <tr>
      <th>7</th>
      <td>gennevilliers</td>
      <td>0.481897</td>
    </tr>
    <tr>
      <th>8</th>
      <td>paume</td>
      <td>0.480164</td>
    </tr>
    <tr>
      <th>9</th>
      <td>kuip</td>
      <td>0.479602</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;tellingly&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>pertinently</td>
      <td>0.600636</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsurprisingly</td>
      <td>0.577011</td>
    </tr>
    <tr>
      <th>2</th>
      <td>influentially</td>
      <td>0.576146</td>
    </tr>
    <tr>
      <th>3</th>
      <td>intriguingly</td>
      <td>0.568034</td>
    </tr>
    <tr>
      <th>4</th>
      <td>poignantly</td>
      <td>0.561211</td>
    </tr>
    <tr>
      <th>5</th>
      <td>problematically</td>
      <td>0.542586</td>
    </tr>
    <tr>
      <th>6</th>
      <td>damagingly</td>
      <td>0.541995</td>
    </tr>
    <tr>
      <th>7</th>
      <td>poetically</td>
      <td>0.541322</td>
    </tr>
    <tr>
      <th>8</th>
      <td>assuredly</td>
      <td>0.531001</td>
    </tr>
    <tr>
      <th>9</th>
      <td>paradoxically</td>
      <td>0.522612</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can also find the <em>least</em> similar word. This is useful to show, because it pressures our idea of what counts as similarity. Mathematical similarity does not always align with concepts like synonyms and antonyms. For example, it’s probably safe to say that the semantic opposite of “good” – that is, its antonym – is “evil.” But in the world of vector spaces, the least similar word to “good” is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;good&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">))[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;cw96&#39;, -0.6553234457969666)
</pre></div>
</div>
</div>
</div>
<p>Just noise! Relatively speaking, “good” and “evil” are actually quite similar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="s1">&#39;good&#39;</span><span class="p">,</span> <span class="s1">&#39;evil&#39;</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Consine similarity score for </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Consine similarity score for good and evil: 0.3378
</pre></div>
</div>
</div>
</div>
<p>How do we make sense of this? Well, it has to do with the way the word embeddings are created. Since embeddings models are ultimately trained on co-occurrence data, words that tend to appear in similar kinds of contexts will be more similar in a mathematical sense than those that don’t.</p>
<p>Keeping this in mind is also important for considerations of bias. Since, in one sense, embeddings reflect the interchangeability between tokens, they will reinforce negative, even harmful patterns in the data (which is to say in culture at large). For example, consider the most similar words for “doctor” and “nurse.” The latter is locked up within gendered language: a nurse is like a midwife is like a mother.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bias_example</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;nurse&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">bias_example</span><span class="p">:</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens most similar to &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
    <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;doctor&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>physician</td>
      <td>0.736021</td>
    </tr>
    <tr>
      <th>1</th>
      <td>doctors</td>
      <td>0.672406</td>
    </tr>
    <tr>
      <th>2</th>
      <td>surgeon</td>
      <td>0.655147</td>
    </tr>
    <tr>
      <th>3</th>
      <td>dr.</td>
      <td>0.652498</td>
    </tr>
    <tr>
      <th>4</th>
      <td>nurse</td>
      <td>0.651449</td>
    </tr>
    <tr>
      <th>5</th>
      <td>medical</td>
      <td>0.648189</td>
    </tr>
    <tr>
      <th>6</th>
      <td>hospital</td>
      <td>0.636380</td>
    </tr>
    <tr>
      <th>7</th>
      <td>patient</td>
      <td>0.619159</td>
    </tr>
    <tr>
      <th>8</th>
      <td>dentist</td>
      <td>0.584747</td>
    </tr>
    <tr>
      <th>9</th>
      <td>psychiatrist</td>
      <td>0.568571</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens most similar to &#39;nurse&#39;:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>nurses</td>
      <td>0.714051</td>
    </tr>
    <tr>
      <th>1</th>
      <td>doctor</td>
      <td>0.651449</td>
    </tr>
    <tr>
      <th>2</th>
      <td>nursing</td>
      <td>0.626937</td>
    </tr>
    <tr>
      <th>3</th>
      <td>midwife</td>
      <td>0.614592</td>
    </tr>
    <tr>
      <th>4</th>
      <td>anesthetist</td>
      <td>0.610603</td>
    </tr>
    <tr>
      <th>5</th>
      <td>physician</td>
      <td>0.610359</td>
    </tr>
    <tr>
      <th>6</th>
      <td>hospital</td>
      <td>0.609222</td>
    </tr>
    <tr>
      <th>7</th>
      <td>mother</td>
      <td>0.586503</td>
    </tr>
    <tr>
      <th>8</th>
      <td>therapist</td>
      <td>0.580488</td>
    </tr>
    <tr>
      <th>9</th>
      <td>dentist</td>
      <td>0.573556</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="visualizing-the-vector-space">
<h3><span class="section-number">4.3.2. </span>Visualizing the vector space<a class="headerlink" href="#visualizing-the-vector-space" title="Permalink to this headline">¶</a></h3>
<p>One way to start getting a feel for all this is to visualize the word vectors. We do so below by sampling a portion of the GloVe vectors and then reducing them into two-dimensional data, which we can plot.</p>
<div class="margin sidebar">
<p class="sidebar-title">How we create the visualization data</p>
<p><code class="docutils literal notranslate"><span class="pre">sample_embeddings()</span></code> takes a sample from GloVe:</p>
<ol class="simple">
<li><p>First it randomly selects indices in the model</p></li>
<li><p>Then it uses these to subset the vectors</p></li>
<li><p>Finally it associates the tokens with their respective indices to produce a set of labels</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">prepare_vis_data()</span></code> takes the sampled vectors and their labels and reduces them with a t-SNE embedder</p>
<ol class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">TSNE()</span></code> portion of the code does the work of reducing our 200-dimension vectors into only two dimensions</p></li>
<li><p>Then the function converts the two-dimensional data into a dataframe and associates the labels</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">sample_embeddings</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">samp</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">n_vectors</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_vectors</span><span class="p">),</span> <span class="n">samp</span><span class="p">)</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectors</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">mask</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">vocab</span>

<span class="k">def</span> <span class="nf">prepare_vis_data</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">reduced</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span>
        <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
        <span class="n">angle</span><span class="o">=</span><span class="mf">0.65</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">357</span>
    <span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    
    <span class="n">vis_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">])</span>
    <span class="n">vis_data</span><span class="p">[</span><span class="s1">&#39;TOKEN&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    
    <span class="k">return</span> <span class="n">vis_data</span>

<span class="n">all_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">model</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">])</span>
<span class="n">sampled</span><span class="p">,</span> <span class="n">sampled_vocab</span> <span class="o">=</span> <span class="n">sample_embeddings</span><span class="p">(</span><span class="n">all_vectors</span><span class="p">)</span>
<span class="n">vis_data</span> <span class="o">=</span> <span class="n">prepare_vis_data</span><span class="p">(</span><span class="n">sampled</span><span class="p">,</span> <span class="n">sampled_vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With the reduced embeddings made, it’s time to plot them. Have a look around at the results. What seems right to you? What surprises you?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">vis_data</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
    <span class="n">tooltip</span><span class="o">=</span><span class="s1">&#39;TOKEN&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">650</span>
<span class="p">)</span><span class="o">.</span><span class="n">interactive</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-aacc5394a25a41609dd8807eaf0c1f5f"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-aacc5394a25a41609dd8807eaf0c1f5f") {
      outputDiv = document.getElementById("altair-viz-aacc5394a25a41609dd8807eaf0c1f5f");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "4.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "data": {"name": "data-553aa71b29138e3d171332d544e96807"}, "mark": {"type": "circle", "size": 30}, "encoding": {"tooltip": {"field": "TOKEN", "type": "nominal"}, "x": {"field": "X", "type": "quantitative"}, "y": {"field": "Y", "type": "quantitative"}}, "height": 650, "selection": {"selector001": {"type": "interval", "bind": "scales", "encodings": ["x", "y"]}}, "width": 650, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-553aa71b29138e3d171332d544e96807": [{"X": -1.4740819931030273, "Y": -2.3825206756591797, "TOKEN": "ventilators"}, {"X": 2.539323329925537, "Y": -0.27344051003456116, "TOKEN": "glassport"}, {"X": -0.7119340896606445, "Y": -1.376646637916565, "TOKEN": "hasp"}, {"X": -1.4097557067871094, "Y": 2.7686545848846436, "TOKEN": "shkidchenko"}, {"X": -1.8086059093475342, "Y": 1.8115317821502686, "TOKEN": "afaf"}, {"X": -1.5592777729034424, "Y": 4.26688289642334, "TOKEN": "boldrini"}, {"X": 3.4078733921051025, "Y": -0.1313047707080841, "TOKEN": "lianga"}, {"X": -3.9097111225128174, "Y": -0.5376776456832886, "TOKEN": "nepad"}, {"X": 0.5699324011802673, "Y": 3.8286898136138916, "TOKEN": "michiel"}, {"X": -0.5479013323783875, "Y": 1.9978498220443726, "TOKEN": "fiocco"}, {"X": -0.6716096997261047, "Y": -1.3641191720962524, "TOKEN": "misnomers"}, {"X": 0.37411776185035706, "Y": 0.7933511734008789, "TOKEN": "helenae"}, {"X": 2.2954049110412598, "Y": -1.7813420295715332, "TOKEN": "d.b.h."}, {"X": -1.2245150804519653, "Y": -1.014292597770691, "TOKEN": "triazolam"}, {"X": -1.9490091800689697, "Y": 4.6588897705078125, "TOKEN": "besoes"}, {"X": -1.8900905847549438, "Y": -1.302649974822998, "TOKEN": "exubera"}, {"X": 1.9065213203430176, "Y": -0.11520148813724518, "TOKEN": "clifty"}, {"X": 3.24039626121521, "Y": 5.788583755493164, "TOKEN": "apace"}, {"X": -1.3713912963867188, "Y": -1.6160557270050049, "TOKEN": "lactose"}, {"X": -2.6893303394317627, "Y": 6.284940242767334, "TOKEN": "anshuman"}, {"X": 2.9541730880737305, "Y": -4.128842830657959, "TOKEN": "43.36"}, {"X": 2.819119453430176, "Y": -0.8094100952148438, "TOKEN": "frankfurt-am-main"}, {"X": -4.216585636138916, "Y": -0.8802628517150879, "TOKEN": "8-9"}, {"X": 0.3990091383457184, "Y": 0.8704105615615845, "TOKEN": "grisea"}, {"X": -1.7056668996810913, "Y": -4.390084743499756, "TOKEN": "prioritising"}, {"X": 5.587216854095459, "Y": -0.5160926580429077, "TOKEN": "hebbian"}, {"X": 3.340003728866577, "Y": -1.3319377899169922, "TOKEN": "northolt"}, {"X": 3.855742931365967, "Y": -2.0658040046691895, "TOKEN": "ch\u00e2teau"}, {"X": 0.08296652883291245, "Y": -3.118088483810425, "TOKEN": "stilts"}, {"X": 2.987562656402588, "Y": -1.210697054862976, "TOKEN": "bruntsfield"}, {"X": 1.8094555139541626, "Y": 3.3379223346710205, "TOKEN": "jenoptik"}, {"X": -3.7304368019104004, "Y": 5.03922176361084, "TOKEN": "exane"}, {"X": 2.047483444213867, "Y": 2.227689027786255, "TOKEN": "essene"}, {"X": -0.5840518474578857, "Y": -0.5513261556625366, "TOKEN": "to-night"}, {"X": -1.9293924570083618, "Y": 1.8623321056365967, "TOKEN": "heckstall-smith"}, {"X": -1.8437153100967407, "Y": -2.454421281814575, "TOKEN": "switchgear"}, {"X": 1.836958885192871, "Y": -4.487987041473389, "TOKEN": "nett"}, {"X": -3.71435284614563, "Y": 1.9702144861221313, "TOKEN": "juzdado"}, {"X": -0.8938693404197693, "Y": 3.478992223739624, "TOKEN": "dilweg"}, {"X": 1.5721889734268188, "Y": -1.5335931777954102, "TOKEN": "ptolemais"}, {"X": 0.9837422370910645, "Y": 2.453423023223877, "TOKEN": "z-boy"}, {"X": -2.5843353271484375, "Y": -4.974385738372803, "TOKEN": "crowing"}, {"X": -1.7591191530227661, "Y": 3.065626621246338, "TOKEN": "miskine"}, {"X": -1.6419014930725098, "Y": 2.631091594696045, "TOKEN": "mison"}, {"X": 0.3621511161327362, "Y": 5.173644065856934, "TOKEN": "pigott"}, {"X": -0.4723818302154541, "Y": 3.39797306060791, "TOKEN": "cannito"}, {"X": -1.1072362661361694, "Y": 0.8235310316085815, "TOKEN": "asso"}, {"X": 1.3495252132415771, "Y": -2.9269707202911377, "TOKEN": "ffi"}, {"X": -0.022696778178215027, "Y": 1.3082071542739868, "TOKEN": "suketu"}, {"X": 3.2904961109161377, "Y": 0.9010786414146423, "TOKEN": "iwaki"}, {"X": 0.10693833976984024, "Y": -1.6954972743988037, "TOKEN": "mathml"}, {"X": -4.929640293121338, "Y": -0.9725319147109985, "TOKEN": "apnewsbreak"}, {"X": -0.3127559423446655, "Y": 2.437299966812134, "TOKEN": "fuselier"}, {"X": -0.21823956072330475, "Y": -5.187806129455566, "TOKEN": "four-wheeled"}, {"X": 1.0854058265686035, "Y": -5.081707000732422, "TOKEN": "4-gigabyte"}, {"X": -0.5953555703163147, "Y": 0.3624289929866791, "TOKEN": "oroonoko"}, {"X": -3.6703667640686035, "Y": -5.8358683586120605, "TOKEN": "vendors"}, {"X": -0.8563324809074402, "Y": -4.106234073638916, "TOKEN": "attires"}, {"X": 2.4042415618896484, "Y": -3.9092214107513428, "TOKEN": "1.238"}, {"X": -0.6663496494293213, "Y": -7.152177810668945, "TOKEN": "ipod"}, {"X": -4.067355155944824, "Y": -1.812752604484558, "TOKEN": "reappraisal"}, {"X": -0.21628792583942413, "Y": 1.2616623640060425, "TOKEN": "tiriel"}, {"X": -1.6032384634017944, "Y": -0.4950762093067169, "TOKEN": "hungaroton"}, {"X": -0.03993244096636772, "Y": 4.7117204666137695, "TOKEN": "shermer"}, {"X": -0.6325821876525879, "Y": -2.0534582138061523, "TOKEN": "disharmonious"}, {"X": -3.143270254135132, "Y": -3.171765089035034, "TOKEN": "characterise"}, {"X": -0.5495846271514893, "Y": -0.6837527751922607, "TOKEN": "peaceforce"}, {"X": -1.166593074798584, "Y": -1.7190707921981812, "TOKEN": "furfural"}, {"X": 1.243682861328125, "Y": 4.741348743438721, "TOKEN": "murkowski"}, {"X": -1.1847422122955322, "Y": 2.11633038520813, "TOKEN": "yankey"}, {"X": -0.05630792677402496, "Y": -1.1371984481811523, "TOKEN": "muralidharan"}, {"X": -3.111284017562866, "Y": -0.3752305209636688, "TOKEN": "payer"}, {"X": -5.675839424133301, "Y": 4.0823283195495605, "TOKEN": "tsung"}, {"X": -3.0190987586975098, "Y": 5.0500946044921875, "TOKEN": "eamon"}, {"X": 5.3780517578125, "Y": -2.079031229019165, "TOKEN": "weaponisation"}, {"X": 0.4968571066856384, "Y": 0.3219345510005951, "TOKEN": "hispanica"}, {"X": -2.20863938331604, "Y": 5.633877277374268, "TOKEN": "dabbaransi"}, {"X": 1.6973235607147217, "Y": 2.0741193294525146, "TOKEN": "tertyshny"}, {"X": -2.176776647567749, "Y": -5.611693382263184, "TOKEN": "cubes"}, {"X": 4.461055755615234, "Y": -1.428071141242981, "TOKEN": "anmc21"}, {"X": 1.285526156425476, "Y": -2.7520153522491455, "TOKEN": "3,564"}, {"X": -0.3727191984653473, "Y": 0.36389511823654175, "TOKEN": "ooma"}, {"X": 0.06493178009986877, "Y": 2.9334702491760254, "TOKEN": "hargon"}, {"X": 3.788365364074707, "Y": -5.00250768661499, "TOKEN": "5-3-1"}, {"X": 4.077769756317139, "Y": -0.7791034579277039, "TOKEN": "cooksville"}, {"X": -3.0254712104797363, "Y": -1.5530765056610107, "TOKEN": "pocketbook"}, {"X": -5.605373382568359, "Y": -2.9129018783569336, "TOKEN": "acknowledging"}, {"X": -0.2903939187526703, "Y": 2.384039878845215, "TOKEN": "melsby"}, {"X": 1.362209677696228, "Y": -5.634096145629883, "TOKEN": "enviro500"}, {"X": 3.9063055515289307, "Y": 1.8803675174713135, "TOKEN": "xiaotian"}, {"X": 6.361299514770508, "Y": -2.6566336154937744, "TOKEN": "harped"}, {"X": 0.31132298707962036, "Y": 2.801318407058716, "TOKEN": "perre"}, {"X": 1.0688740015029907, "Y": 4.219442844390869, "TOKEN": "rozhdestvensky"}, {"X": 0.22142702341079712, "Y": 0.3279697000980377, "TOKEN": "obscura"}, {"X": 3.451167106628418, "Y": -1.902500033378601, "TOKEN": "philiphaugh"}, {"X": -2.1726772785186768, "Y": -1.5356887578964233, "TOKEN": "betaseron"}, {"X": -2.32273006439209, "Y": 5.1324076652526855, "TOKEN": "pahimi"}, {"X": 3.23172664642334, "Y": 2.7394118309020996, "TOKEN": "arecanut"}, {"X": -0.9473609328269958, "Y": 4.027863502502441, "TOKEN": "gravante"}, {"X": -2.131044387817383, "Y": -3.20021915435791, "TOKEN": "caress"}, {"X": -1.9109238386154175, "Y": 1.4962433576583862, "TOKEN": "autobiographer"}, {"X": 5.339200019836426, "Y": -4.370607852935791, "TOKEN": "istiqlal"}, {"X": -3.5847272872924805, "Y": 3.887451171875, "TOKEN": "hussam"}, {"X": -0.20559053122997284, "Y": -3.201793670654297, "TOKEN": "well-adjusted"}, {"X": 2.1796061992645264, "Y": 3.299710988998413, "TOKEN": "dhirubhai"}, {"X": 4.272793292999268, "Y": -0.1907457560300827, "TOKEN": "polonnaruwa"}, {"X": 3.894817352294922, "Y": -0.1889965981245041, "TOKEN": "mutur"}, {"X": -1.1011096239089966, "Y": 1.885332465171814, "TOKEN": "fercu"}, {"X": -0.1780427247285843, "Y": -1.6135249137878418, "TOKEN": "multi-service"}, {"X": -0.5314499735832214, "Y": 2.4538090229034424, "TOKEN": "vanderlaan"}, {"X": -0.7556816935539246, "Y": 3.119180917739868, "TOKEN": "galardi"}, {"X": -3.473130702972412, "Y": 2.964059829711914, "TOKEN": "asiedu"}, {"X": -0.1902996301651001, "Y": 3.706932544708252, "TOKEN": "brinton"}, {"X": 4.811267852783203, "Y": -0.9633121490478516, "TOKEN": "enclosure"}, {"X": 0.002774687483906746, "Y": -0.5425926446914673, "TOKEN": "periode"}, {"X": -0.19263510406017303, "Y": 4.644006252288818, "TOKEN": "nobuaki"}, {"X": -1.4522907733917236, "Y": -2.642733097076416, "TOKEN": "wellwishers"}, {"X": -1.9776250123977661, "Y": 5.343936920166016, "TOKEN": "sudibyo"}, {"X": -4.4388508796691895, "Y": 4.059108734130859, "TOKEN": "eur2004-eng"}, {"X": 2.8655266761779785, "Y": -0.22168859839439392, "TOKEN": "fontanka"}, {"X": -2.5279428958892822, "Y": 2.5529720783233643, "TOKEN": "abdille"}, {"X": -3.5113675594329834, "Y": 0.4388543367385864, "TOKEN": "aph"}, {"X": -6.942800521850586, "Y": 0.060520920902490616, "TOKEN": "14-run"}, {"X": -1.351650595664978, "Y": 1.8804866075515747, "TOKEN": "lippy"}, {"X": -6.364508628845215, "Y": 2.538832902908325, "TOKEN": "nicaean"}, {"X": 3.5727171897888184, "Y": -1.1506446599960327, "TOKEN": "chhatrapati"}, {"X": 0.33753547072410583, "Y": 2.3822202682495117, "TOKEN": "zalla"}, {"X": 1.5272893905639648, "Y": -0.908283531665802, "TOKEN": "branik"}, {"X": -1.8200746774673462, "Y": 1.4015904664993286, "TOKEN": "locy"}, {"X": 0.1300409585237503, "Y": -4.430993556976318, "TOKEN": "r10000"}, {"X": -3.2448787689208984, "Y": -1.9207720756530762, "TOKEN": "epistolary"}, {"X": 1.4898000955581665, "Y": 3.382638931274414, "TOKEN": "bellegarde"}, {"X": 0.03505469858646393, "Y": 2.8050951957702637, "TOKEN": "beckstrom"}, {"X": 3.0891613960266113, "Y": -0.11629194766283035, "TOKEN": "wallkill"}, {"X": -1.1133290529251099, "Y": -1.4208749532699585, "TOKEN": "simulacra"}, {"X": -5.39387845993042, "Y": -2.9027352333068848, "TOKEN": "glitch"}, {"X": -3.5346221923828125, "Y": -1.2353463172912598, "TOKEN": "plainsong"}, {"X": 4.442059516906738, "Y": -0.7583795189857483, "TOKEN": "millpond"}, {"X": -4.976461887359619, "Y": 1.2908648252487183, "TOKEN": "toledo"}, {"X": 2.59747052192688, "Y": 0.20451687276363373, "TOKEN": "kamenice"}, {"X": -1.6736034154891968, "Y": 4.456933975219727, "TOKEN": "evangelist"}, {"X": 1.6844465732574463, "Y": -2.7121410369873047, "TOKEN": "1074"}, {"X": -0.9903956055641174, "Y": 4.008310794830322, "TOKEN": "westerling"}, {"X": -0.24644118547439575, "Y": 3.7350263595581055, "TOKEN": "taubert"}, {"X": -3.831713914871216, "Y": 2.699092388153076, "TOKEN": "muliaina"}, {"X": -2.968665599822998, "Y": 0.945982813835144, "TOKEN": "zhongbo"}, {"X": -5.3109211921691895, "Y": 0.4354104697704315, "TOKEN": "bonferroni"}, {"X": 2.076697826385498, "Y": 5.250495433807373, "TOKEN": "carranza"}, {"X": 0.27093490958213806, "Y": -2.5743460655212402, "TOKEN": "weatherstripping"}, {"X": -2.525247573852539, "Y": 3.935178756713867, "TOKEN": "haniff"}, {"X": 2.9557912349700928, "Y": 0.5897364616394043, "TOKEN": "yeongju"}, {"X": 1.6032100915908813, "Y": 4.2924981117248535, "TOKEN": "majengo"}, {"X": 2.2001700401306152, "Y": -3.6678595542907715, "TOKEN": "15,240"}, {"X": -2.6204657554626465, "Y": -2.9878318309783936, "TOKEN": "gauged"}, {"X": 2.4358434677124023, "Y": -1.8912209272384644, "TOKEN": "wnbl"}, {"X": -1.8488569259643555, "Y": -0.3483849763870239, "TOKEN": "daewoosa"}, {"X": -4.573500156402588, "Y": 2.6365041732788086, "TOKEN": "kutty"}, {"X": -1.2431312799453735, "Y": 2.2920994758605957, "TOKEN": "olzon"}, {"X": 0.9373063445091248, "Y": 0.9225083589553833, "TOKEN": "parupalli"}, {"X": 0.8819901347160339, "Y": -1.4581818580627441, "TOKEN": "vsm"}, {"X": -2.162527084350586, "Y": 0.2448706328868866, "TOKEN": "hrf"}, {"X": 3.8814616203308105, "Y": -5.002679824829102, "TOKEN": "8-1-1"}, {"X": -5.874255180358887, "Y": -3.432929515838623, "TOKEN": "extreme"}, {"X": -4.622040748596191, "Y": 4.695132255554199, "TOKEN": "schmirler"}, {"X": 5.352616786956787, "Y": -4.5389790534973145, "TOKEN": "bharatiya"}, {"X": -0.8619553446769714, "Y": -4.580296993255615, "TOKEN": "tuxedoes"}, {"X": 0.011636333540081978, "Y": 1.9625697135925293, "TOKEN": "mehrjui"}, {"X": -3.1868085861206055, "Y": -3.36741042137146, "TOKEN": "fluctuate"}, {"X": 2.715125560760498, "Y": 4.120132923126221, "TOKEN": "alvarion"}, {"X": -2.0373213291168213, "Y": 3.839756965637207, "TOKEN": "mahmut"}, {"X": 5.882650375366211, "Y": 0.7534213066101074, "TOKEN": "westmorland"}, {"X": -1.0774643421173096, "Y": -1.6676846742630005, "TOKEN": "non-invasive"}, {"X": -0.4853777289390564, "Y": -2.150542974472046, "TOKEN": "weaklings"}, {"X": 0.6403583884239197, "Y": -3.0818188190460205, "TOKEN": "necropsies"}, {"X": -0.8654313683509827, "Y": 2.041677236557007, "TOKEN": "herzig"}, {"X": 3.7475194931030273, "Y": -2.737074136734009, "TOKEN": "terrey"}, {"X": -3.4742746353149414, "Y": -0.9759350419044495, "TOKEN": "grandfathers"}, {"X": 2.8872907161712646, "Y": 0.004795713350176811, "TOKEN": "kivalliq"}, {"X": 1.6543294191360474, "Y": -5.438046932220459, "TOKEN": "59-foot"}, {"X": -1.0126289129257202, "Y": 5.214019298553467, "TOKEN": "yerger"}, {"X": 2.262708902359009, "Y": 1.0443695783615112, "TOKEN": "ili"}, {"X": -1.8066685199737549, "Y": 2.6433799266815186, "TOKEN": "holborow"}, {"X": -1.0315216779708862, "Y": 5.296455383300781, "TOKEN": "stuhlinger"}, {"X": 2.3645448684692383, "Y": -0.5739672183990479, "TOKEN": "kumaris"}, {"X": -1.5713458061218262, "Y": -4.654413223266602, "TOKEN": "counterterror"}, {"X": -3.8234643936157227, "Y": 1.0982012748718262, "TOKEN": "smither"}, {"X": -0.5024693608283997, "Y": 0.07268930971622467, "TOKEN": "hornpipe"}, {"X": -3.305850028991699, "Y": 1.9705629348754883, "TOKEN": "tuncay"}, {"X": 3.4826836585998535, "Y": -1.4743378162384033, "TOKEN": "lytham"}, {"X": 0.9679208993911743, "Y": -1.4986224174499512, "TOKEN": "oae"}, {"X": -5.541245460510254, "Y": -3.7947301864624023, "TOKEN": "hate"}, {"X": 0.48737069964408875, "Y": 0.5576289296150208, "TOKEN": "bauri"}, {"X": -2.222872495651245, "Y": 1.4579510688781738, "TOKEN": "flo"}, {"X": -3.4735782146453857, "Y": -4.5853190422058105, "TOKEN": "irrationality"}, {"X": 1.9988009929656982, "Y": 0.08635176718235016, "TOKEN": "californias"}, {"X": 5.593459129333496, "Y": 2.982895612716675, "TOKEN": "deutschlandradio"}, {"X": 5.8557562828063965, "Y": 0.7299953699111938, "TOKEN": "lonsdale"}, {"X": 0.7658917903900146, "Y": 0.9891980886459351, "TOKEN": "passio"}, {"X": 0.8804107904434204, "Y": 4.7472968101501465, "TOKEN": "runnicles"}, {"X": -0.9000968337059021, "Y": 2.878647804260254, "TOKEN": "kouznetsov"}, {"X": -1.0297542810440063, "Y": 1.7057406902313232, "TOKEN": "fornaciari"}, {"X": 3.465240716934204, "Y": -3.385221242904663, "TOKEN": "100.95"}, {"X": 3.1912105083465576, "Y": -6.289119243621826, "TOKEN": "kw"}, {"X": 0.09579065442085266, "Y": -2.013593912124634, "TOKEN": "type-ii"}, {"X": 1.5709079504013062, "Y": 0.8138598203659058, "TOKEN": "tamachi"}, {"X": 2.3476901054382324, "Y": 0.9596177935600281, "TOKEN": "signoria"}, {"X": 1.1364684104919434, "Y": -4.561669826507568, "TOKEN": "billionth"}, {"X": 0.4518682360649109, "Y": -0.5092313289642334, "TOKEN": "auran"}, {"X": -0.732485294342041, "Y": 2.1753146648406982, "TOKEN": "pribina"}, {"X": -1.0771737098693848, "Y": 3.710981607437134, "TOKEN": "alevras"}, {"X": -0.08948315680027008, "Y": -1.7276239395141602, "TOKEN": "trysting"}, {"X": -0.9122494459152222, "Y": 4.455461025238037, "TOKEN": "whitcomb"}, {"X": 0.03287038579583168, "Y": -1.5111312866210938, "TOKEN": "dwelling-house"}, {"X": -1.2691901922225952, "Y": 3.1658341884613037, "TOKEN": "stamatis"}, {"X": -0.7930132746696472, "Y": -1.1054959297180176, "TOKEN": "topographies"}, {"X": -2.2486910820007324, "Y": -1.6834876537322998, "TOKEN": "muezzin"}, {"X": 0.22054597735404968, "Y": 3.021421194076538, "TOKEN": "merel"}, {"X": -1.9830832481384277, "Y": 5.394672393798828, "TOKEN": "messan"}, {"X": 4.028114318847656, "Y": 3.749109983444214, "TOKEN": "sentido"}, {"X": 1.422594428062439, "Y": -2.1981990337371826, "TOKEN": "3in"}, {"X": -0.25053444504737854, "Y": -5.166909217834473, "TOKEN": "heavy-duty"}, {"X": -0.51695317029953, "Y": -5.449703693389893, "TOKEN": "corvettes"}, {"X": -5.1740193367004395, "Y": -3.9091808795928955, "TOKEN": "nazism"}, {"X": 2.5564427375793457, "Y": 0.6889424920082092, "TOKEN": "odernheim"}, {"X": -0.2690254747867584, "Y": -0.08201169222593307, "TOKEN": "pagri"}, {"X": -1.2587727308273315, "Y": 6.223748683929443, "TOKEN": "laaksonen"}, {"X": 4.28346586227417, "Y": 1.8410345315933228, "TOKEN": "myx"}, {"X": -1.0707896947860718, "Y": 0.28158169984817505, "TOKEN": "ekaterini"}, {"X": 2.52240252494812, "Y": -4.082561016082764, "TOKEN": "12,085"}, {"X": -6.075375080108643, "Y": -3.4765820503234863, "TOKEN": "remaining"}, {"X": -1.379718542098999, "Y": -2.5486884117126465, "TOKEN": "offputting"}, {"X": 4.045485496520996, "Y": 3.6298696994781494, "TOKEN": "europeo"}, {"X": 2.1044774055480957, "Y": -4.6098504066467285, "TOKEN": "euro730"}, {"X": -3.7209115028381348, "Y": 2.139669179916382, "TOKEN": "trevorrow"}, {"X": -1.796862006187439, "Y": 2.0260696411132812, "TOKEN": "lonigan"}, {"X": 4.232663154602051, "Y": -1.9971349239349365, "TOKEN": "amiata"}, {"X": -0.16256293654441833, "Y": 2.2325146198272705, "TOKEN": "guadagnino"}, {"X": -2.4342432022094727, "Y": 4.204888820648193, "TOKEN": "ohlsson"}, {"X": 3.4718971252441406, "Y": 1.0288395881652832, "TOKEN": "taiyuan"}, {"X": -0.23112881183624268, "Y": 5.556695461273193, "TOKEN": "saji"}, {"X": 0.204925075173378, "Y": 6.319996356964111, "TOKEN": "masuda"}, {"X": -1.4513845443725586, "Y": 4.5142621994018555, "TOKEN": "jertz"}, {"X": 1.4539660215377808, "Y": 3.0260508060455322, "TOKEN": "faberg\u00e9"}, {"X": -5.559822082519531, "Y": -2.953131675720215, "TOKEN": "gaps"}, {"X": 0.30032259225845337, "Y": 4.9972310066223145, "TOKEN": "hornstein"}, {"X": 2.9960975646972656, "Y": -4.208270072937012, "TOKEN": "45.67"}, {"X": -1.2116703987121582, "Y": 0.7174493670463562, "TOKEN": "sayaka"}, {"X": 3.7709732055664062, "Y": 5.01836633682251, "TOKEN": "dressmaker"}, {"X": -2.2276556491851807, "Y": 1.670046329498291, "TOKEN": "cicely"}, {"X": 3.1046998500823975, "Y": 0.06875205785036087, "TOKEN": "aliceville"}, {"X": 2.7677078247070312, "Y": -3.9120590686798096, "TOKEN": "71.49"}, {"X": -2.279715061187744, "Y": 2.602442741394043, "TOKEN": "olivero"}, {"X": -5.387943267822266, "Y": -1.0780918598175049, "TOKEN": "artic"}, {"X": -2.352419137954712, "Y": 3.8084394931793213, "TOKEN": "amund"}, {"X": 2.5661606788635254, "Y": -0.8127549290657043, "TOKEN": "a33"}, {"X": -0.5483072996139526, "Y": -2.7661032676696777, "TOKEN": "prioritises"}, {"X": -4.10971736907959, "Y": -1.2668499946594238, "TOKEN": "milonga"}, {"X": 1.7171651124954224, "Y": -3.635314464569092, "TOKEN": "974,000"}, {"X": 1.1659934520721436, "Y": 5.62261962890625, "TOKEN": "auffray"}, {"X": 5.5256733894348145, "Y": -3.0899012088775635, "TOKEN": "php"}, {"X": 1.4836194515228271, "Y": 1.2547589540481567, "TOKEN": "teepencolumn"}, {"X": -1.5907056331634521, "Y": -2.95894455909729, "TOKEN": "tractable"}, {"X": -1.1109641790390015, "Y": 2.3635151386260986, "TOKEN": "c\u0103lin"}, {"X": 4.948692798614502, "Y": 1.0311052799224854, "TOKEN": "bia\u0142ystok"}, {"X": -0.5847738981246948, "Y": 1.176300048828125, "TOKEN": "fleance"}, {"X": 0.35628366470336914, "Y": 3.300353527069092, "TOKEN": "florine"}, {"X": -2.4319074153900146, "Y": 2.6450753211975098, "TOKEN": "bizimungu"}, {"X": -1.6428767442703247, "Y": 2.4568073749542236, "TOKEN": "lawrason"}, {"X": -1.7230792045593262, "Y": -4.534492492675781, "TOKEN": "reallocating"}, {"X": -4.39867639541626, "Y": 1.5598564147949219, "TOKEN": "forsyth"}, {"X": 0.7695196270942688, "Y": 0.12642870843410492, "TOKEN": "debout"}, {"X": 1.05030357837677, "Y": -3.112774610519409, "TOKEN": "quick-firing"}, {"X": -2.5443127155303955, "Y": -2.5302650928497314, "TOKEN": "graeco-roman"}, {"X": -4.192838668823242, "Y": 0.8290651440620422, "TOKEN": "gettys"}, {"X": -2.481128215789795, "Y": 4.742999076843262, "TOKEN": "buchalter"}, {"X": 1.0331909656524658, "Y": -1.7130844593048096, "TOKEN": "c65"}, {"X": -1.4684669971466064, "Y": 1.724593997001648, "TOKEN": "utai"}, {"X": 0.8559288382530212, "Y": -2.4836742877960205, "TOKEN": "post-2012"}, {"X": 1.56231689453125, "Y": 0.2378627210855484, "TOKEN": "kranti"}, {"X": -0.1727360039949417, "Y": -5.06728982925415, "TOKEN": "late-model"}, {"X": -0.30470359325408936, "Y": 1.1979336738586426, "TOKEN": "lonar"}, {"X": -5.063669681549072, "Y": -3.028507947921753, "TOKEN": "eradicate"}, {"X": -1.490734577178955, "Y": -1.6976441144943237, "TOKEN": "drakh"}, {"X": -3.6288273334503174, "Y": -5.848777770996094, "TOKEN": "dealerships"}, {"X": -3.8245482444763184, "Y": -3.7148923873901367, "TOKEN": "gulch"}, {"X": -5.405037879943848, "Y": -2.0605452060699463, "TOKEN": "praises"}, {"X": 1.5218658447265625, "Y": -2.5649056434631348, "TOKEN": "scholar-in-residence"}, {"X": -3.3201539516448975, "Y": -3.306138515472412, "TOKEN": "constrict"}, {"X": 3.6640570163726807, "Y": 0.4367755651473999, "TOKEN": "choc\u00f3"}, {"X": 0.4364018142223358, "Y": 0.5257223844528198, "TOKEN": "elaeagnus"}, {"X": 2.8962960243225098, "Y": -4.143298149108887, "TOKEN": "45.02"}, {"X": -5.389662265777588, "Y": -3.8928306102752686, "TOKEN": "bigotry"}, {"X": -2.342288017272949, "Y": 3.660080909729004, "TOKEN": "septi"}, {"X": 0.5909144878387451, "Y": 4.95119047164917, "TOKEN": "selcuk"}, {"X": 0.4687480926513672, "Y": -5.695087432861328, "TOKEN": "scaled-up"}, {"X": -2.6950442790985107, "Y": 5.3209547996521, "TOKEN": "perben"}, {"X": -0.31401073932647705, "Y": 0.903344452381134, "TOKEN": "arulpragasam"}, {"X": -1.2695541381835938, "Y": -0.45246633887290955, "TOKEN": "redesignations"}, {"X": 2.9119575023651123, "Y": 4.118902206420898, "TOKEN": "vinasat"}, {"X": 2.984811544418335, "Y": -3.5924019813537598, "TOKEN": "72.87"}, {"X": -2.804403781890869, "Y": 3.47823429107666, "TOKEN": "p\u00e9tur"}, {"X": 0.28265732526779175, "Y": 0.8850948214530945, "TOKEN": "hemionus"}, {"X": 4.208653450012207, "Y": 0.13123157620429993, "TOKEN": "bojonegoro"}, {"X": -2.014662981033325, "Y": -1.6189360618591309, "TOKEN": "carpools"}, {"X": -0.4093990921974182, "Y": 2.0946178436279297, "TOKEN": "reirden"}, {"X": 6.3982062339782715, "Y": 1.6519638299942017, "TOKEN": "hamleys"}, {"X": -2.2178196907043457, "Y": -5.03744649887085, "TOKEN": "chipping"}, {"X": 2.5485353469848633, "Y": -3.784128189086914, "TOKEN": "64.31"}, {"X": -0.4087604284286499, "Y": 2.598313093185425, "TOKEN": "loane"}, {"X": -3.629302501678467, "Y": 4.057591915130615, "TOKEN": "nunu"}, {"X": 2.6316497325897217, "Y": -5.440423011779785, "TOKEN": "18,400"}, {"X": -1.4288100004196167, "Y": 4.960320949554443, "TOKEN": "harkins"}, {"X": 4.040222644805908, "Y": -1.3209168910980225, "TOKEN": "snowfields"}, {"X": -0.005165495444089174, "Y": 0.1758166402578354, "TOKEN": "defamer"}, {"X": -5.935876369476318, "Y": 1.580306887626648, "TOKEN": "adal"}, {"X": 0.7930986881256104, "Y": 0.012443621642887592, "TOKEN": "hubu"}, {"X": -0.19814006984233856, "Y": 1.1912962198257446, "TOKEN": "malen"}, {"X": 4.439844131469727, "Y": 2.676302909851074, "TOKEN": "hydrographic"}, {"X": 2.8161721229553223, "Y": -0.9621338248252869, "TOKEN": "moorebank"}, {"X": -2.670135259628296, "Y": 1.8109245300292969, "TOKEN": "offishall"}, {"X": 2.2069761753082275, "Y": 2.794538736343384, "TOKEN": "barasa"}, {"X": 0.2758801281452179, "Y": 0.2857034504413605, "TOKEN": "xperiment"}, {"X": 3.2890419960021973, "Y": -0.9929473996162415, "TOKEN": "middlewood"}, {"X": 6.275542736053467, "Y": -0.7830561399459839, "TOKEN": "dealmaker"}, {"X": -6.123561859130859, "Y": -3.237525224685669, "TOKEN": "mind"}, {"X": -1.4089597463607788, "Y": 4.4376606941223145, "TOKEN": "listin"}, {"X": -3.0943989753723145, "Y": 2.140441656112671, "TOKEN": "semira"}, {"X": 0.36966028809547424, "Y": -5.832399845123291, "TOKEN": "rerecorded"}, {"X": -0.9932948350906372, "Y": 1.2531037330627441, "TOKEN": "margetic"}, {"X": -0.5558658838272095, "Y": 0.9263777732849121, "TOKEN": "jenis"}, {"X": 0.38180863857269287, "Y": 0.04933185875415802, "TOKEN": "aliis"}, {"X": 0.4770422577857971, "Y": -5.794703006744385, "TOKEN": "long-wheelbase"}, {"X": -6.231944561004639, "Y": -3.186506748199463, "TOKEN": "cinema"}, {"X": -2.8029329776763916, "Y": 1.6730479001998901, "TOKEN": "lowed"}, {"X": 0.21847154200077057, "Y": 4.346381187438965, "TOKEN": "manasse"}, {"X": -2.009425640106201, "Y": -3.6998353004455566, "TOKEN": "thoughout"}, {"X": 0.5115712285041809, "Y": -2.1428565979003906, "TOKEN": "do-or-die"}, {"X": 2.6490602493286133, "Y": -3.8620028495788574, "TOKEN": "66.11"}, {"X": -1.030930519104004, "Y": 0.3273904323577881, "TOKEN": "chebi"}, {"X": 3.3459737300872803, "Y": -6.236547470092773, "TOKEN": "454"}, {"X": -2.687742233276367, "Y": -2.2857043743133545, "TOKEN": "hallucinatory"}, {"X": 6.312904357910156, "Y": -0.7721364498138428, "TOKEN": "hothead"}, {"X": 2.3038430213928223, "Y": 1.9445539712905884, "TOKEN": "crescendoed"}, {"X": 6.231532096862793, "Y": -0.19796213507652283, "TOKEN": "hundertwasser"}, {"X": 4.865779876708984, "Y": 0.992033064365387, "TOKEN": "szczeci\u0144ski"}, {"X": -1.3310580253601074, "Y": -3.623289108276367, "TOKEN": "quirks"}, {"X": 2.833767890930176, "Y": -0.07861720025539398, "TOKEN": "ptolemaida"}, {"X": 1.6960722208023071, "Y": 0.5958938002586365, "TOKEN": "hakerem"}, {"X": -0.22053177654743195, "Y": 2.515516519546509, "TOKEN": "kebo"}, {"X": -4.135652542114258, "Y": -0.4920024573802948, "TOKEN": "175-word"}, {"X": 2.5565757751464844, "Y": 0.09325636923313141, "TOKEN": "meelick"}, {"X": -3.9635956287384033, "Y": 2.1152405738830566, "TOKEN": "jarno"}, {"X": -6.246262073516846, "Y": 4.748015403747559, "TOKEN": "chung"}, {"X": 1.7499909400939941, "Y": -3.4449892044067383, "TOKEN": "6,180"}, {"X": 0.13028490543365479, "Y": 1.7068229913711548, "TOKEN": "chappellet"}, {"X": -0.5831658244132996, "Y": -0.48499491810798645, "TOKEN": "e-type"}, {"X": 3.039206027984619, "Y": -2.5344736576080322, "TOKEN": "chongju"}, {"X": -1.4923681020736694, "Y": 4.021053314208984, "TOKEN": "kusumah"}, {"X": -1.779391884803772, "Y": 2.2486190795898438, "TOKEN": "tostig"}, {"X": -5.7009124755859375, "Y": 3.823683261871338, "TOKEN": "deanship"}, {"X": 0.6132247447967529, "Y": 4.147651672363281, "TOKEN": "southey"}, {"X": -0.1334451586008072, "Y": -1.8662985563278198, "TOKEN": "drug-smuggling"}, {"X": -2.843531370162964, "Y": 2.79909086227417, "TOKEN": "angarita"}, {"X": -3.699707508087158, "Y": 3.9855992794036865, "TOKEN": "yamaoka"}, {"X": 0.10870981961488724, "Y": 3.2333920001983643, "TOKEN": "tannehill"}, {"X": 3.2776036262512207, "Y": -2.6775035858154297, "TOKEN": "roborough"}, {"X": -1.443134069442749, "Y": 0.9613194465637207, "TOKEN": "malesia"}, {"X": -1.0685851573944092, "Y": 1.2221871614456177, "TOKEN": "kirvesniemi"}, {"X": 1.3972115516662598, "Y": -2.870706081390381, "TOKEN": "75-mile"}, {"X": -3.1782562732696533, "Y": -3.4496994018554688, "TOKEN": "buoy"}, {"X": -1.9771946668624878, "Y": -4.521629810333252, "TOKEN": "scooping"}, {"X": 1.9442362785339355, "Y": -2.8410472869873047, "TOKEN": "2213"}, {"X": -1.273131012916565, "Y": -2.1205620765686035, "TOKEN": "admixtures"}, {"X": 1.090136170387268, "Y": -3.858818292617798, "TOKEN": "dearer"}, {"X": 1.2089741230010986, "Y": -1.9267452955245972, "TOKEN": "tmdl"}, {"X": -4.397533893585205, "Y": 3.1451947689056396, "TOKEN": "whitall"}, {"X": 1.0407594442367554, "Y": 1.4488309621810913, "TOKEN": "ittefaq"}, {"X": 3.0511796474456787, "Y": -0.3404979407787323, "TOKEN": "basavanagudi"}, {"X": -0.8354786038398743, "Y": -2.579047918319702, "TOKEN": "erogenous"}, {"X": 1.2033313512802124, "Y": -2.8752636909484863, "TOKEN": "juli\u00e1"}, {"X": -0.8130099773406982, "Y": 6.521300792694092, "TOKEN": "mcclane"}, {"X": -1.3080509901046753, "Y": 5.336254119873047, "TOKEN": "lazar"}, {"X": 2.7866663932800293, "Y": -0.1835983693599701, "TOKEN": "eiteljorg"}, {"X": -1.0928255319595337, "Y": -1.947931170463562, "TOKEN": "well-managed"}, {"X": 0.9862383604049683, "Y": 2.782508611679077, "TOKEN": "maruska"}, {"X": -3.447571277618408, "Y": 1.5185348987579346, "TOKEN": "outrighted"}, {"X": 2.2945024967193604, "Y": -0.7813913226127625, "TOKEN": "byelorussia"}, {"X": -1.4273276329040527, "Y": -0.9171908497810364, "TOKEN": "stavudine"}, {"X": -1.5523746013641357, "Y": -2.822678565979004, "TOKEN": "houseplants"}, {"X": 0.8908947110176086, "Y": 2.73714017868042, "TOKEN": "wieringen"}, {"X": -1.0754550695419312, "Y": -1.9085516929626465, "TOKEN": "voice-mail"}, {"X": 2.901801347732544, "Y": -4.111526966094971, "TOKEN": "48.01"}, {"X": -2.212432384490967, "Y": -0.15755334496498108, "TOKEN": "dnv"}, {"X": -6.262380123138428, "Y": 4.773246765136719, "TOKEN": "mong"}, {"X": -0.44856536388397217, "Y": 4.013690948486328, "TOKEN": "morwood"}, {"X": 3.000732183456421, "Y": -4.074461460113525, "TOKEN": "45.51"}, {"X": -0.8257893323898315, "Y": 4.0948486328125, "TOKEN": "kotoku"}, {"X": -0.8394668102264404, "Y": 3.415677309036255, "TOKEN": "malon"}, {"X": -4.769233226776123, "Y": -1.8131104707717896, "TOKEN": "plunge"}, {"X": -0.41713476181030273, "Y": -1.9224528074264526, "TOKEN": "middle-range"}, {"X": 0.1443779617547989, "Y": 2.633758544921875, "TOKEN": "aderca"}, {"X": 2.9754700660705566, "Y": 5.687536716461182, "TOKEN": "mourner"}, {"X": 0.8483167290687561, "Y": -2.151315450668335, "TOKEN": "uwb"}, {"X": -2.2921061515808105, "Y": 3.9047999382019043, "TOKEN": "jebsen"}, {"X": 2.324084758758545, "Y": 4.814062118530273, "TOKEN": "bizinsidersfgate.com"}, {"X": -3.6019465923309326, "Y": -2.257072925567627, "TOKEN": "tewahedo"}, {"X": -2.491086721420288, "Y": 1.788278341293335, "TOKEN": "voltio"}, {"X": 3.144092559814453, "Y": 0.5670773983001709, "TOKEN": "eschweiler"}, {"X": 3.153143882751465, "Y": 0.3964516520500183, "TOKEN": "huddinge"}, {"X": 0.03769350424408913, "Y": -0.22495095431804657, "TOKEN": "prabandha"}, {"X": -2.7877445220947266, "Y": -1.1479378938674927, "TOKEN": "motc"}, {"X": -0.7463422417640686, "Y": 3.1103806495666504, "TOKEN": "momoh"}, {"X": -0.48455166816711426, "Y": -6.7001872062683105, "TOKEN": "turntable"}, {"X": -0.903971254825592, "Y": -1.5624581575393677, "TOKEN": "mmds"}, {"X": -1.727726697921753, "Y": 2.8815360069274902, "TOKEN": "ss-obersturmbannf\u00fchrer"}, {"X": 1.0828008651733398, "Y": -1.2557307481765747, "TOKEN": "aphc"}, {"X": -1.4569129943847656, "Y": -3.375661849975586, "TOKEN": "buds"}, {"X": 3.0534915924072266, "Y": -4.27729606628418, "TOKEN": "25.07"}, {"X": 1.6946454048156738, "Y": 1.7633353471755981, "TOKEN": "somone"}, {"X": -1.420081377029419, "Y": -5.417290687561035, "TOKEN": "aubergines"}, {"X": -0.5265129208564758, "Y": 6.185872554779053, "TOKEN": "bangash"}, {"X": 3.3211007118225098, "Y": -2.399897575378418, "TOKEN": "mayoralty"}, {"X": -2.0083677768707275, "Y": 5.687067031860352, "TOKEN": "salomonsson"}, {"X": 2.2565016746520996, "Y": 4.433899879455566, "TOKEN": "novopharm"}, {"X": 1.3654499053955078, "Y": 3.5990803241729736, "TOKEN": "merode"}, {"X": 2.3443644046783447, "Y": -1.7984883785247803, "TOKEN": "1922-1924"}, {"X": 0.799523651599884, "Y": 0.36821919679641724, "TOKEN": "kesavananda"}, {"X": -1.32834792137146, "Y": -0.8030815720558167, "TOKEN": "neumega"}, {"X": -3.4158687591552734, "Y": -0.39240774512290955, "TOKEN": "m&m"}, {"X": -1.56446373462677, "Y": 0.22329500317573547, "TOKEN": "vassaras"}, {"X": -0.6908623576164246, "Y": -2.800992012023926, "TOKEN": "pockmarks"}, {"X": -2.202483654022217, "Y": -1.2532968521118164, "TOKEN": "malagasy"}, {"X": 0.17091797292232513, "Y": 0.07586494088172913, "TOKEN": "ar\u0101js"}, {"X": -0.5372744798660278, "Y": 3.6191678047180176, "TOKEN": "mallin"}, {"X": -1.348728895187378, "Y": -2.309483766555786, "TOKEN": "multi-layered"}, {"X": 0.5181519389152527, "Y": 3.7832894325256348, "TOKEN": "ovalle"}, {"X": -0.2869735360145569, "Y": -0.30735743045806885, "TOKEN": "ketupat"}, {"X": -0.8231161236763, "Y": 0.5483035445213318, "TOKEN": "xanthopoulos"}, {"X": -3.8063347339630127, "Y": -2.891016960144043, "TOKEN": "resuscitate"}, {"X": -3.6941134929656982, "Y": 5.253408432006836, "TOKEN": "lins"}, {"X": 0.4645596742630005, "Y": -2.8998966217041016, "TOKEN": "hibakusha"}, {"X": 5.031955242156982, "Y": -4.777305603027344, "TOKEN": "pp"}, {"X": -3.5294573307037354, "Y": 3.7016189098358154, "TOKEN": "qassab"}, {"X": -1.2360172271728516, "Y": 4.927376747131348, "TOKEN": "anthes"}, {"X": -0.5607608556747437, "Y": 2.881772041320801, "TOKEN": "d'agata"}, {"X": -3.46036958694458, "Y": 4.230465888977051, "TOKEN": "zeid"}, {"X": -2.9801459312438965, "Y": -0.7129165530204773, "TOKEN": "niger"}, {"X": 4.758917808532715, "Y": -3.4368488788604736, "TOKEN": "mahamid"}, {"X": 0.9133273363113403, "Y": -1.1090892553329468, "TOKEN": "acja"}, {"X": -3.8122241497039795, "Y": 6.017901420593262, "TOKEN": "garriques"}, {"X": 0.7490769624710083, "Y": 1.7226459980010986, "TOKEN": "heathman"}, {"X": -0.771364152431488, "Y": 0.5155519247055054, "TOKEN": "bat-mite"}, {"X": -4.413086414337158, "Y": -1.1570159196853638, "TOKEN": "aerobics"}, {"X": -2.7299578189849854, "Y": 0.7024499773979187, "TOKEN": "topco"}, {"X": 4.722146034240723, "Y": 0.9996522068977356, "TOKEN": "d\u0142ugo\u0142\u0119ka"}, {"X": 0.8067545890808105, "Y": 5.437992095947266, "TOKEN": "sensenbrenner"}, {"X": 2.3719193935394287, "Y": 2.382183313369751, "TOKEN": "84-84"}, {"X": -0.41258522868156433, "Y": 4.017794609069824, "TOKEN": "shkolnikova"}, {"X": -3.4841201305389404, "Y": -4.731363296508789, "TOKEN": "numerator"}, {"X": 3.3685896396636963, "Y": 0.251798540353775, "TOKEN": "kerak"}, {"X": -0.9540887475013733, "Y": 1.6020480394363403, "TOKEN": "acon"}, {"X": -0.46011289954185486, "Y": 1.1963489055633545, "TOKEN": "gattai"}, {"X": 0.628092885017395, "Y": 3.4837746620178223, "TOKEN": "bucy"}, {"X": 1.216480016708374, "Y": 0.16427305340766907, "TOKEN": "lerici"}, {"X": -2.8389251232147217, "Y": 2.642366409301758, "TOKEN": "parvin"}, {"X": -0.4232117235660553, "Y": 0.8892337083816528, "TOKEN": "ravenal"}, {"X": -0.7132864594459534, "Y": 0.21665546298027039, "TOKEN": "otryad"}, {"X": -5.430671691894531, "Y": -3.9169058799743652, "TOKEN": "intimidation"}, {"X": 3.2679436206817627, "Y": -6.243717670440674, "TOKEN": "230"}, {"X": -1.444638967514038, "Y": -6.487732887268066, "TOKEN": "three-card"}, {"X": -2.7996227741241455, "Y": 0.4230603873729706, "TOKEN": "ovie"}, {"X": -1.7405561208724976, "Y": -0.20572569966316223, "TOKEN": "shater"}, {"X": 2.0926027297973633, "Y": 1.7507528066635132, "TOKEN": "yoed"}, {"X": 4.242556095123291, "Y": 2.6044318675994873, "TOKEN": "mcso"}, {"X": -0.9124161601066589, "Y": 2.9371554851531982, "TOKEN": "avrin"}, {"X": 2.673888921737671, "Y": -4.478856086730957, "TOKEN": "69.8"}, {"X": -2.2616004943847656, "Y": -3.872823715209961, "TOKEN": "fe2"}, {"X": -0.030582835897803307, "Y": -1.135754942893982, "TOKEN": "one-touch"}, {"X": 3.5797529220581055, "Y": 0.08950261026620865, "TOKEN": "wasilla"}, {"X": -1.3286151885986328, "Y": -4.126594543457031, "TOKEN": "pizzazz"}, {"X": -2.733762502670288, "Y": 1.8881702423095703, "TOKEN": "bessa"}, {"X": -5.54456901550293, "Y": -2.909639835357666, "TOKEN": "facto"}, {"X": 3.0667953491210938, "Y": 5.7139129638671875, "TOKEN": "mournfully"}, {"X": 1.2105481624603271, "Y": -0.14565575122833252, "TOKEN": "lh1"}, {"X": 1.6778604984283447, "Y": -6.659700393676758, "TOKEN": ".411"}, {"X": -2.948936939239502, "Y": 2.534842014312744, "TOKEN": "krasovsky"}, {"X": -1.8654195070266724, "Y": -3.7086050510406494, "TOKEN": "turfing"}, {"X": -1.6054701805114746, "Y": 0.5756731033325195, "TOKEN": "txeroki"}, {"X": -1.995492696762085, "Y": 3.3534297943115234, "TOKEN": "msiri"}, {"X": 3.015451669692993, "Y": 4.200768947601318, "TOKEN": "outbid"}, {"X": 0.33698323369026184, "Y": 3.9263458251953125, "TOKEN": "serber"}, {"X": -4.42377233505249, "Y": 5.250954627990723, "TOKEN": "angelino"}, {"X": -2.559248685836792, "Y": -2.6618545055389404, "TOKEN": "surmising"}, {"X": 1.2554728984832764, "Y": 2.3229804039001465, "TOKEN": "half-heavyweight"}, {"X": -1.6946537494659424, "Y": -4.9679999351501465, "TOKEN": "zest"}, {"X": 2.311068296432495, "Y": 2.871680498123169, "TOKEN": "idiakez"}, {"X": -0.552251398563385, "Y": 0.5574705600738525, "TOKEN": "mombi"}, {"X": -2.0115954875946045, "Y": -1.4919308423995972, "TOKEN": "heartlessly"}, {"X": 2.5971217155456543, "Y": -5.2880330085754395, "TOKEN": "1,880"}, {"X": -1.7407234907150269, "Y": -1.150786280632019, "TOKEN": "phytonutrients"}, {"X": -0.6072757244110107, "Y": 4.153895854949951, "TOKEN": "blackmore"}, {"X": 1.9928852319717407, "Y": 3.530076742172241, "TOKEN": "kallmann"}, {"X": -0.5558777451515198, "Y": 2.148878812789917, "TOKEN": "michelini"}, {"X": -3.857008934020996, "Y": -4.120787143707275, "TOKEN": "pushforward"}, {"X": 1.7150421142578125, "Y": 4.126972675323486, "TOKEN": "tradicionales"}, {"X": 2.76539945602417, "Y": 0.1547895222902298, "TOKEN": "g\u00e4strikland"}, {"X": -0.24130979180335999, "Y": 2.050004243850708, "TOKEN": "alisan"}, {"X": -4.867617607116699, "Y": -0.20177684724330902, "TOKEN": "rafter"}, {"X": -0.6496727466583252, "Y": -2.47658109664917, "TOKEN": "caretaking"}, {"X": 1.5045760869979858, "Y": -3.4925425052642822, "TOKEN": "1,843"}, {"X": -4.18618631362915, "Y": -2.41899037361145, "TOKEN": "monographs"}, {"X": -3.910351514816284, "Y": 6.08687162399292, "TOKEN": "maaouya"}, {"X": 2.1252129077911377, "Y": -1.0785696506500244, "TOKEN": "karamu"}, {"X": -2.33431339263916, "Y": 2.6693220138549805, "TOKEN": "sudarat"}, {"X": 0.944638192653656, "Y": -0.10416176915168762, "TOKEN": "taip"}, {"X": 0.4825173020362854, "Y": -1.157397985458374, "TOKEN": "poesy"}, {"X": -2.264789581298828, "Y": -2.726024627685547, "TOKEN": "interoperate"}, {"X": 0.5446896553039551, "Y": -5.875086784362793, "TOKEN": "fuel-injected"}, {"X": -2.5282363891601562, "Y": 0.42020556330680847, "TOKEN": "smaug"}, {"X": 2.3919026851654053, "Y": -4.101582050323486, "TOKEN": "130.7"}, {"X": -0.9268332123756409, "Y": -3.5126497745513916, "TOKEN": "balmier"}, {"X": 2.147714376449585, "Y": -2.3593199253082275, "TOKEN": "2555"}, {"X": 0.14030589163303375, "Y": 1.8758304119110107, "TOKEN": "boieldieu"}, {"X": 1.802382230758667, "Y": -2.4621195793151855, "TOKEN": "2460"}, {"X": -2.30991792678833, "Y": -2.201096296310425, "TOKEN": "dehydrated"}, {"X": 0.7223762273788452, "Y": -3.3801698684692383, "TOKEN": "live/work"}, {"X": -1.6958075761795044, "Y": 0.04398522526025772, "TOKEN": "180-gram"}, {"X": -2.3751769065856934, "Y": 7.215742588043213, "TOKEN": "lisagor"}, {"X": -0.33751529455184937, "Y": -3.6717867851257324, "TOKEN": "heterosexuals"}, {"X": -1.2834818363189697, "Y": 5.403528690338135, "TOKEN": "kingson"}, {"X": -0.7289449572563171, "Y": -2.1745545864105225, "TOKEN": "clangor"}, {"X": -1.8861511945724487, "Y": 1.5005325078964233, "TOKEN": "fayrouz"}, {"X": 3.2596845626831055, "Y": -0.7236020565032959, "TOKEN": "chapeltown"}, {"X": 0.8048467636108398, "Y": -1.6302217245101929, "TOKEN": "qsa"}, {"X": -0.8372861742973328, "Y": -3.1495957374572754, "TOKEN": "turmoils"}, {"X": -1.8162744045257568, "Y": 4.104310989379883, "TOKEN": "haggans"}, {"X": 1.789596438407898, "Y": 4.010588645935059, "TOKEN": "alagich"}, {"X": 0.17460565268993378, "Y": -4.6578497886657715, "TOKEN": "300-megahertz"}, {"X": -1.0489133596420288, "Y": -3.823683023452759, "TOKEN": "18-man"}, {"X": -0.5443968772888184, "Y": 1.6536108255386353, "TOKEN": "amaia"}, {"X": -5.625808238983154, "Y": -2.581062078475952, "TOKEN": "barroso"}, {"X": 3.587738037109375, "Y": 2.7575342655181885, "TOKEN": "nazarenko"}, {"X": -6.829739570617676, "Y": 2.7048912048339844, "TOKEN": "gupta"}, {"X": 1.2781182527542114, "Y": -4.810114860534668, "TOKEN": "vertex"}, {"X": 3.323991060256958, "Y": -6.2316083908081055, "TOKEN": "409"}, {"X": 3.5882575511932373, "Y": -1.8958337306976318, "TOKEN": "8,235.81"}, {"X": 0.8245859742164612, "Y": -0.7379465699195862, "TOKEN": "khaplang"}, {"X": 0.9402991533279419, "Y": -2.551217794418335, "TOKEN": "closed-door"}, {"X": 1.7714029550552368, "Y": -4.342824459075928, "TOKEN": "rm55"}, {"X": 0.43380680680274963, "Y": 7.740967273712158, "TOKEN": "friesz"}, {"X": 3.540675640106201, "Y": -0.029922965914011, "TOKEN": "queensferry"}, {"X": 0.277007520198822, "Y": 2.1172256469726562, "TOKEN": "woo-ping"}, {"X": -6.8906660079956055, "Y": 0.03684132918715477, "TOKEN": "48-28"}, {"X": 0.026134643703699112, "Y": -4.6172776222229, "TOKEN": "diebold"}, {"X": -6.8467206954956055, "Y": 0.021413125097751617, "TOKEN": "44-32"}, {"X": 2.2399230003356934, "Y": -3.5455574989318848, "TOKEN": "immunise"}, {"X": 0.5950833559036255, "Y": 3.306748628616333, "TOKEN": "j\u00e1uregui"}, {"X": 0.0932656079530716, "Y": 3.613133430480957, "TOKEN": "obenshain"}, {"X": -1.6019079685211182, "Y": -1.9395787715911865, "TOKEN": "trimmers"}, {"X": -1.6362708806991577, "Y": 0.7921961545944214, "TOKEN": "henny"}, {"X": 5.031163692474365, "Y": 0.3258114755153656, "TOKEN": "barguna"}, {"X": -2.891390323638916, "Y": 4.161311149597168, "TOKEN": "ramadier"}, {"X": 3.763122797012329, "Y": 0.23735234141349792, "TOKEN": "bethlehem"}, {"X": 0.8493447303771973, "Y": 0.14427174627780914, "TOKEN": "prosti"}, {"X": 0.7631403803825378, "Y": 3.250994920730591, "TOKEN": "deusen"}, {"X": 1.5940780639648438, "Y": 0.9231545329093933, "TOKEN": "woodforest"}, {"X": 1.1627699136734009, "Y": -1.594456672668457, "TOKEN": "photoelectron"}, {"X": 1.3060050010681152, "Y": 4.651974201202393, "TOKEN": "hiro"}, {"X": 1.7301889657974243, "Y": -1.064252257347107, "TOKEN": "apnts"}, {"X": 2.310858726501465, "Y": 1.336440086364746, "TOKEN": "meurthe"}, {"X": -1.9835724830627441, "Y": -6.003596782684326, "TOKEN": "meats"}, {"X": 5.279904365539551, "Y": 1.8136391639709473, "TOKEN": "1991-2001"}, {"X": -0.301535964012146, "Y": 5.360853672027588, "TOKEN": "nikonov"}, {"X": -0.9077666401863098, "Y": -1.562127709388733, "TOKEN": "amitraz"}, {"X": 0.10000906884670258, "Y": -1.0859719514846802, "TOKEN": "dpx"}, {"X": 3.077629327774048, "Y": -4.144982814788818, "TOKEN": "50.77"}, {"X": -0.2563353180885315, "Y": 4.730294227600098, "TOKEN": "daykin"}, {"X": -2.685286521911621, "Y": 0.09848714619874954, "TOKEN": "merci"}, {"X": -1.736698031425476, "Y": 2.3468194007873535, "TOKEN": "barnabe"}, {"X": -0.38101786375045776, "Y": -2.735818862915039, "TOKEN": "gundeck"}, {"X": -2.5619635581970215, "Y": 5.440972805023193, "TOKEN": "jodar"}, {"X": -1.0468815565109253, "Y": 2.777017831802368, "TOKEN": "jiahui"}, {"X": -1.2598575353622437, "Y": 3.737807273864746, "TOKEN": "desper"}, {"X": -0.4651343524456024, "Y": -5.954855442047119, "TOKEN": "f-18"}, {"X": -2.86796236038208, "Y": -2.2337894439697266, "TOKEN": "homelessness"}, {"X": 4.030767440795898, "Y": 1.0647165775299072, "TOKEN": "tongxin"}, {"X": -3.501293659210205, "Y": -0.6202322244644165, "TOKEN": "sage"}, {"X": 6.063899040222168, "Y": -1.522916555404663, "TOKEN": "formar"}, {"X": -3.281686544418335, "Y": -0.7203750014305115, "TOKEN": "072"}, {"X": 3.8628346920013428, "Y": -0.4096645414829254, "TOKEN": "bandirma"}, {"X": -3.5183944702148438, "Y": 3.466223955154419, "TOKEN": "sein"}, {"X": -0.6745136380195618, "Y": 2.801403760910034, "TOKEN": "tresca"}, {"X": -6.787745952606201, "Y": -3.1564598083496094, "TOKEN": "berkeley"}, {"X": 5.4551496505737305, "Y": -3.1442160606384277, "TOKEN": "dlls"}, {"X": 2.9656999111175537, "Y": -1.4553500413894653, "TOKEN": "racquet"}, {"X": 1.2596074342727661, "Y": 2.5828700065612793, "TOKEN": "sward"}, {"X": -0.03672480210661888, "Y": -2.283900499343872, "TOKEN": "nuwaubian"}, {"X": 2.97969651222229, "Y": -4.116636276245117, "TOKEN": "40.17"}, {"X": 1.775259256362915, "Y": -4.15617561340332, "TOKEN": "euro305"}, {"X": -2.118114948272705, "Y": 0.23395636677742004, "TOKEN": "anr"}, {"X": 0.26079538464546204, "Y": 0.766375720500946, "TOKEN": "gordonii"}, {"X": 3.4439427852630615, "Y": -3.335881233215332, "TOKEN": "105.50"}, {"X": -6.056367874145508, "Y": 2.4051995277404785, "TOKEN": "arcadius"}, {"X": 0.8045846223831177, "Y": 3.9502179622650146, "TOKEN": "fuglesang"}, {"X": 1.6569157838821411, "Y": 2.8926634788513184, "TOKEN": "grau"}, {"X": -0.659976601600647, "Y": -7.167619705200195, "TOKEN": "usb"}, {"X": -4.099419116973877, "Y": 2.7267470359802246, "TOKEN": "hohler"}, {"X": 4.258230686187744, "Y": 2.597344398498535, "TOKEN": "undersheriff"}, {"X": -2.866900682449341, "Y": 0.99996417760849, "TOKEN": "nijssen"}, {"X": 0.17916063964366913, "Y": -0.8519234657287598, "TOKEN": "reids"}, {"X": 0.4536197781562805, "Y": 1.2691938877105713, "TOKEN": "maili"}, {"X": -1.1744325160980225, "Y": 3.6909117698669434, "TOKEN": "vallier"}, {"X": 4.6265716552734375, "Y": -3.791254997253418, "TOKEN": "pedicures"}, {"X": 0.14175069332122803, "Y": 0.8893805742263794, "TOKEN": "confessio"}, {"X": -1.8060857057571411, "Y": -2.6185054779052734, "TOKEN": "non-experts"}, {"X": -1.1321502923965454, "Y": 4.087284088134766, "TOKEN": "agoglia"}, {"X": 3.34623122215271, "Y": -1.110701560974121, "TOKEN": "wapping"}, {"X": -1.8397220373153687, "Y": 1.1440010070800781, "TOKEN": "finanz"}, {"X": -0.12819918990135193, "Y": -1.362632155418396, "TOKEN": "boundedness"}, {"X": 2.1908676624298096, "Y": 5.939661502838135, "TOKEN": "prat"}, {"X": -2.757066011428833, "Y": 3.5076680183410645, "TOKEN": "pendergest"}, {"X": -3.416491985321045, "Y": -2.131699800491333, "TOKEN": "vajray\u0101na"}, {"X": 2.165764331817627, "Y": 0.48095306754112244, "TOKEN": "kaeng"}, {"X": -1.7268637418746948, "Y": 6.400439262390137, "TOKEN": "hunzinger"}, {"X": -0.6627355217933655, "Y": -2.7562737464904785, "TOKEN": "windfalls"}, {"X": -1.4805792570114136, "Y": 6.920926094055176, "TOKEN": "runzheimer"}, {"X": -6.2248148918151855, "Y": -2.322042465209961, "TOKEN": "giuliani"}, {"X": 1.150233268737793, "Y": -3.135582447052002, "TOKEN": "3,136"}, {"X": 2.2219250202178955, "Y": -2.764832019805908, "TOKEN": "14:45"}, {"X": 5.467353343963623, "Y": 2.7638328075408936, "TOKEN": "wtae-tv"}, {"X": -0.6142559051513672, "Y": 3.83943510055542, "TOKEN": "rosaleen"}, {"X": 2.5873024463653564, "Y": 2.1385347843170166, "TOKEN": "tiptoed"}, {"X": 3.0686912536621094, "Y": -3.7603986263275146, "TOKEN": "386.00"}, {"X": 0.2585228681564331, "Y": -0.29376181960105896, "TOKEN": "sele\u00e7\u00e3o"}, {"X": -0.47394317388534546, "Y": -5.277638912200928, "TOKEN": "saddle-tank"}, {"X": -4.260490417480469, "Y": 0.7921804785728455, "TOKEN": "therien"}, {"X": 0.9505285620689392, "Y": -4.921911239624023, "TOKEN": "2,500-dollar"}, {"X": -1.6921477317810059, "Y": -3.16182017326355, "TOKEN": "recline"}, {"X": -0.5682699084281921, "Y": 4.569321632385254, "TOKEN": "donohue"}, {"X": 0.7353286743164062, "Y": 0.605478823184967, "TOKEN": "yasht"}, {"X": -6.015819072723389, "Y": -2.347379684448242, "TOKEN": "shanghai"}, {"X": -0.5629487633705139, "Y": 1.8346596956253052, "TOKEN": "\u0111\u1eb7ng"}, {"X": 0.6742351651191711, "Y": -0.5576913356781006, "TOKEN": "strella"}, {"X": 0.5763474702835083, "Y": 1.212536334991455, "TOKEN": "laide"}, {"X": -1.2121269702911377, "Y": -0.14459675550460815, "TOKEN": "double-crested"}, {"X": 3.3813304901123047, "Y": -3.3298771381378174, "TOKEN": "107.80"}, {"X": -3.2382304668426514, "Y": -0.21173036098480225, "TOKEN": "usurped"}, {"X": 0.9679193496704102, "Y": 5.884157657623291, "TOKEN": "koppelman"}, {"X": -1.4125267267227173, "Y": 2.75284481048584, "TOKEN": "enami"}, {"X": 1.2158303260803223, "Y": 1.7395424842834473, "TOKEN": "clavecin"}, {"X": -1.1146162748336792, "Y": 2.211482524871826, "TOKEN": "pelat"}, {"X": -1.881773591041565, "Y": -4.23121976852417, "TOKEN": "somersaults"}, {"X": 0.45045405626296997, "Y": -3.772766351699829, "TOKEN": "haploid"}, {"X": -2.9537835121154785, "Y": -3.0877797603607178, "TOKEN": "afflict"}, {"X": -4.345194339752197, "Y": -5.045164585113525, "TOKEN": "voluntad"}, {"X": 3.0306811332702637, "Y": 0.7299676537513733, "TOKEN": "princesa"}, {"X": -1.1383956670761108, "Y": 3.0242061614990234, "TOKEN": "thiele"}, {"X": -5.502238750457764, "Y": 0.10466086864471436, "TOKEN": "hunkpapa"}, {"X": -3.5502240657806396, "Y": 0.9610216021537781, "TOKEN": "kreusch"}, {"X": 3.3735923767089844, "Y": 4.622497081756592, "TOKEN": "anti-reflective"}, {"X": -0.03518887236714363, "Y": 1.3599787950515747, "TOKEN": "tpca"}, {"X": 4.089001655578613, "Y": -0.3000109791755676, "TOKEN": "valachchenai"}, {"X": -3.9561896324157715, "Y": -2.2728309631347656, "TOKEN": "koans"}, {"X": 0.4145191013813019, "Y": 2.9442410469055176, "TOKEN": "brumidi"}, {"X": -0.3035939931869507, "Y": -2.2344892024993896, "TOKEN": "disclaim"}, {"X": -0.5363736152648926, "Y": 2.9875059127807617, "TOKEN": "aaberg"}, {"X": 0.2588661313056946, "Y": -4.380708694458008, "TOKEN": "128-member"}, {"X": 1.5219908952713013, "Y": 5.838411808013916, "TOKEN": "orly"}, {"X": -2.6519384384155273, "Y": 3.228546619415283, "TOKEN": "yussuf"}, {"X": 2.1229894161224365, "Y": -6.216242790222168, "TOKEN": "granita"}, {"X": -1.5926198959350586, "Y": 2.0182993412017822, "TOKEN": "gissur"}, {"X": -1.0534061193466187, "Y": 4.0437211990356445, "TOKEN": "rousmaniere"}, {"X": -6.808102607727051, "Y": 0.001257234369404614, "TOKEN": "71-57"}, {"X": 2.612405300140381, "Y": -1.8932969570159912, "TOKEN": "under-19s"}, {"X": -2.1793739795684814, "Y": 2.8544669151306152, "TOKEN": "shourie"}, {"X": 4.441745281219482, "Y": 1.0403063297271729, "TOKEN": "varzaqan"}, {"X": -0.7653360366821289, "Y": -0.9992982149124146, "TOKEN": "vendange"}, {"X": 0.8890417814254761, "Y": 0.7208360433578491, "TOKEN": "californica"}, {"X": 4.559155464172363, "Y": 4.492811679840088, "TOKEN": "rosewood"}, {"X": 1.6504594087600708, "Y": -1.3405697345733643, "TOKEN": "chsaa"}, {"X": -0.7914655804634094, "Y": -0.10106704384088516, "TOKEN": "melas"}, {"X": -0.03718988969922066, "Y": -0.15803495049476624, "TOKEN": "vizion"}, {"X": 4.185320854187012, "Y": 1.2687350511550903, "TOKEN": "alut"}, {"X": -5.088326930999756, "Y": 0.8864673376083374, "TOKEN": "enameling"}, {"X": 2.826253652572632, "Y": -3.8812243938446045, "TOKEN": "70.55"}, {"X": -4.98637056350708, "Y": 2.7055439949035645, "TOKEN": "jex-blake"}, {"X": 4.981750965118408, "Y": -0.48499536514282227, "TOKEN": "braises"}, {"X": -2.9388983249664307, "Y": 5.804079055786133, "TOKEN": "rugare"}, {"X": 4.60456657409668, "Y": -2.7430026531219482, "TOKEN": "sculpted"}, {"X": -1.4120491743087769, "Y": -2.1133666038513184, "TOKEN": "over-use"}, {"X": -0.40578117966651917, "Y": 0.02088896371424198, "TOKEN": "velva"}, {"X": 0.05824452266097069, "Y": -4.427164077758789, "TOKEN": "nacelles"}, {"X": 0.5220704078674316, "Y": -0.8142294883728027, "TOKEN": "atgms"}, {"X": -0.7475338578224182, "Y": 0.3537520170211792, "TOKEN": "jugni"}, {"X": 2.8284006118774414, "Y": 1.8051890134811401, "TOKEN": "juts"}, {"X": -5.329840660095215, "Y": -1.1619925498962402, "TOKEN": "4x400m"}, {"X": -3.599818706512451, "Y": -0.3232240378856659, "TOKEN": "burros"}, {"X": 0.4123293459415436, "Y": 1.840820550918579, "TOKEN": "imtiyaz"}, {"X": -0.31475943326950073, "Y": 0.4370901584625244, "TOKEN": "mattu"}, {"X": 0.23460617661476135, "Y": -0.27636978030204773, "TOKEN": "disneys"}, {"X": 4.970452308654785, "Y": -2.348855972290039, "TOKEN": "plastinated"}, {"X": -0.053057584911584854, "Y": -5.1717023849487305, "TOKEN": "m-84"}, {"X": -3.5155012607574463, "Y": -4.719793319702148, "TOKEN": "quadratic"}, {"X": -1.3333847522735596, "Y": 2.877598524093628, "TOKEN": "beechler"}, {"X": -6.752291679382324, "Y": -0.0020262699108570814, "TOKEN": "58-44"}, {"X": 1.5109524726867676, "Y": -0.59121173620224, "TOKEN": "marmaray"}, {"X": 0.3925894796848297, "Y": -6.8598737716674805, "TOKEN": "trashes"}, {"X": 2.6330013275146484, "Y": -3.987694263458252, "TOKEN": "177.47"}, {"X": -0.7946184873580933, "Y": 3.2177531719207764, "TOKEN": "yukinaga"}, {"X": -0.06910783797502518, "Y": 0.6912758946418762, "TOKEN": "autochloris"}, {"X": 2.6656148433685303, "Y": 3.2488527297973633, "TOKEN": "idus"}, {"X": 1.0929139852523804, "Y": 3.3143272399902344, "TOKEN": "auken"}, {"X": 0.41675060987472534, "Y": -0.8881430625915527, "TOKEN": "il-86"}, {"X": 0.4712318181991577, "Y": 1.0327112674713135, "TOKEN": "chinta"}, {"X": 0.49473023414611816, "Y": 0.679741382598877, "TOKEN": "catostomus"}, {"X": 2.6100656986236572, "Y": 1.2950589656829834, "TOKEN": "brze\u017ano"}, {"X": -3.3948774337768555, "Y": 2.694823741912842, "TOKEN": "aradippou"}, {"X": 1.550447702407837, "Y": 4.925170421600342, "TOKEN": "oben"}, {"X": 3.0102217197418213, "Y": -1.157036304473877, "TOKEN": "bordighera"}, {"X": 3.221470832824707, "Y": -1.2496219873428345, "TOKEN": "castelgandolfo"}, {"X": 0.929902970790863, "Y": 2.7660281658172607, "TOKEN": "sternheim"}, {"X": -1.8070322275161743, "Y": 3.1205127239227295, "TOKEN": "malan"}, {"X": -1.021909236907959, "Y": -3.497471809387207, "TOKEN": "waspish"}, {"X": 1.6665174961090088, "Y": -3.291442394256592, "TOKEN": "4/10"}, {"X": -3.3310155868530273, "Y": 1.4199286699295044, "TOKEN": "marianne"}, {"X": 4.789556503295898, "Y": 0.29814645648002625, "TOKEN": "begusarai"}, {"X": -3.232576370239258, "Y": 3.8580968379974365, "TOKEN": "allam"}, {"X": -0.6854195594787598, "Y": -2.623337507247925, "TOKEN": "tatoos"}, {"X": -3.1486761569976807, "Y": -7.139735221862793, "TOKEN": "h3"}, {"X": 3.759352207183838, "Y": -4.167550563812256, "TOKEN": "adv25"}, {"X": -0.5703602433204651, "Y": -3.202364444732666, "TOKEN": "leading-edge"}, {"X": 2.105229139328003, "Y": 0.2327788919210434, "TOKEN": "senneterre"}, {"X": -2.343212842941284, "Y": -1.9509979486465454, "TOKEN": "polyarthritis"}, {"X": -1.706209659576416, "Y": -1.4838148355484009, "TOKEN": "ususally"}, {"X": 2.4127821922302246, "Y": -4.58909273147583, "TOKEN": ".86"}, {"X": 2.4011030197143555, "Y": -1.9980688095092773, "TOKEN": "1949-53"}, {"X": -2.1756410598754883, "Y": 5.991199970245361, "TOKEN": "nikica"}, {"X": 2.919226884841919, "Y": 0.3904760181903839, "TOKEN": "suir"}, {"X": -3.3749163150787354, "Y": 3.8722918033599854, "TOKEN": "mohammadawi"}, {"X": 1.240769863128662, "Y": -1.628686547279358, "TOKEN": "ucu"}, {"X": -0.047739483416080475, "Y": 0.574146032333374, "TOKEN": "chancer"}, {"X": 0.4830852150917053, "Y": -0.2638256251811981, "TOKEN": "drummania"}, {"X": 1.7983962297439575, "Y": 0.14332321286201477, "TOKEN": "narbonensis"}, {"X": -2.123858690261841, "Y": -3.9558968544006348, "TOKEN": "wrenches"}, {"X": 4.52018404006958, "Y": 4.435153484344482, "TOKEN": "senorita"}, {"X": 3.6346657276153564, "Y": 0.04950781166553497, "TOKEN": "lunglei"}, {"X": 1.3734097480773926, "Y": 0.9504534006118774, "TOKEN": "jamma"}, {"X": -4.172755241394043, "Y": 0.2105589509010315, "TOKEN": "hicks"}, {"X": 2.8289623260498047, "Y": -4.6773200035095215, "TOKEN": "14.2"}, {"X": 2.396629810333252, "Y": -1.9670636653900146, "TOKEN": "2002-3"}, {"X": 2.7697949409484863, "Y": -0.6324173212051392, "TOKEN": "232nd"}, {"X": 2.8548200130462646, "Y": -2.7212986946105957, "TOKEN": "issoudun"}, {"X": -1.8199132680892944, "Y": 3.497117519378662, "TOKEN": "shevchuk"}, {"X": 1.6408835649490356, "Y": -3.8033812046051025, "TOKEN": "ps3s"}, {"X": 1.1999492645263672, "Y": 6.371418476104736, "TOKEN": "sandwell"}, {"X": 1.3996849060058594, "Y": 1.9716347455978394, "TOKEN": "benziger"}, {"X": -3.310457944869995, "Y": 3.986060619354248, "TOKEN": "faraj"}, {"X": 0.8831310272216797, "Y": 1.2650840282440186, "TOKEN": "pelea"}, {"X": 1.0268193483352661, "Y": 3.546065330505371, "TOKEN": "lattre"}, {"X": 2.003812789916992, "Y": -4.935161113739014, "TOKEN": "675,000"}, {"X": -1.9457377195358276, "Y": 0.732126772403717, "TOKEN": "khalifatul"}, {"X": -4.189296245574951, "Y": -1.8280739784240723, "TOKEN": "painstaking"}, {"X": 0.2513154149055481, "Y": 0.12757572531700134, "TOKEN": "gurren"}, {"X": -1.9172216653823853, "Y": 1.8606547117233276, "TOKEN": "dietrick"}, {"X": -0.2027626484632492, "Y": -3.3141908645629883, "TOKEN": "deviants"}, {"X": -2.76324462890625, "Y": -2.8960318565368652, "TOKEN": "anecdotal"}, {"X": -1.3238860368728638, "Y": 0.9692322015762329, "TOKEN": "helgenberger"}, {"X": 3.031873941421509, "Y": 3.01979923248291, "TOKEN": "shipholding"}, {"X": -0.497012734413147, "Y": 3.135657548904419, "TOKEN": "mahasaya"}, {"X": -1.8795520067214966, "Y": -3.9994938373565674, "TOKEN": "ratting"}, {"X": -0.8943201899528503, "Y": -3.3649725914001465, "TOKEN": "posher"}, {"X": -3.309995174407959, "Y": -0.23124083876609802, "TOKEN": "woodiwiss"}, {"X": 3.2909634113311768, "Y": -6.28445291519165, "TOKEN": "93"}, {"X": -0.9246943593025208, "Y": 2.699774980545044, "TOKEN": "buczynski"}, {"X": 3.2938754558563232, "Y": -6.205495357513428, "TOKEN": "965"}, {"X": 3.2454075813293457, "Y": -3.4647560119628906, "TOKEN": "91.45"}, {"X": -1.9688748121261597, "Y": -3.122163772583008, "TOKEN": "evacuees"}, {"X": -1.3918832540512085, "Y": 1.8410978317260742, "TOKEN": "boyington"}, {"X": 0.13676224648952484, "Y": 0.569284200668335, "TOKEN": "b\u00e9b\u00e9"}, {"X": 2.1921613216400146, "Y": -0.46533262729644775, "TOKEN": "skelmorlie"}, {"X": 3.7951600551605225, "Y": 0.7709807753562927, "TOKEN": "kola"}, {"X": 0.6973099112510681, "Y": -2.250746250152588, "TOKEN": "upstroke"}, {"X": 2.680802822113037, "Y": -5.508998394012451, "TOKEN": "7,300"}, {"X": 1.9367904663085938, "Y": -3.255481243133545, "TOKEN": "141.0"}, {"X": -0.910668134689331, "Y": 1.0814721584320068, "TOKEN": "autio"}, {"X": -2.523472309112549, "Y": -4.843729496002197, "TOKEN": "fulminating"}, {"X": -0.652175784111023, "Y": 1.8352702856063843, "TOKEN": "blagojevi\u0107"}, {"X": 2.8042871952056885, "Y": 4.295703411102295, "TOKEN": "seacrest"}, {"X": -4.652350425720215, "Y": 0.12127985805273056, "TOKEN": "ojo"}, {"X": -1.7125005722045898, "Y": -2.021918773651123, "TOKEN": "amphibolite"}, {"X": -2.2749860286712646, "Y": -2.3950612545013428, "TOKEN": "surcharges"}, {"X": -2.610264539718628, "Y": 1.1865977048873901, "TOKEN": "borowitz"}, {"X": 0.5320931673049927, "Y": 0.3990064263343811, "TOKEN": "\u014dji"}, {"X": -0.23649835586547852, "Y": 4.16666841506958, "TOKEN": "kirst"}, {"X": -4.27803897857666, "Y": 0.11426587402820587, "TOKEN": "ugandan"}, {"X": -6.1468186378479, "Y": -3.3447935581207275, "TOKEN": "their"}, {"X": 4.171870231628418, "Y": 0.3133343458175659, "TOKEN": "sar\u0131yer"}, {"X": 4.907862663269043, "Y": 0.3078199028968811, "TOKEN": "mahbubnagar"}, {"X": 4.627552509307861, "Y": 0.28402039408683777, "TOKEN": "puri"}, {"X": -2.32667875289917, "Y": 0.6029675006866455, "TOKEN": "electrabel"}, {"X": -5.3643388748168945, "Y": 1.8339709043502808, "TOKEN": "rcarr"}, {"X": 5.531094074249268, "Y": 2.921949625015259, "TOKEN": "hessischer"}, {"X": 0.4444912075996399, "Y": 7.752206802368164, "TOKEN": "griese"}, {"X": 0.06977568566799164, "Y": 0.6437925696372986, "TOKEN": "kairo"}, {"X": 1.0384681224822998, "Y": 0.712478518486023, "TOKEN": "bahnbetriebswerk"}, {"X": 4.11575984954834, "Y": -1.9751060009002686, "TOKEN": "skylit"}, {"X": 1.8466975688934326, "Y": 6.9863128662109375, "TOKEN": "26-yard"}, {"X": -1.629215121269226, "Y": 5.164325714111328, "TOKEN": "erastus"}, {"X": -0.047741279006004333, "Y": 0.2492506057024002, "TOKEN": "kyodan"}, {"X": 5.190881729125977, "Y": -1.9787254333496094, "TOKEN": "xenograft"}, {"X": -4.213862419128418, "Y": -3.218945264816284, "TOKEN": "encompassed"}, {"X": -0.902303159236908, "Y": 4.52507209777832, "TOKEN": "meckstroth"}, {"X": -0.9281708598136902, "Y": 1.865843415260315, "TOKEN": "serifovic"}, {"X": 2.9069390296936035, "Y": 2.285416841506958, "TOKEN": "lambro"}, {"X": 3.117877244949341, "Y": -0.8551384210586548, "TOKEN": "walmley"}, {"X": -3.3707213401794434, "Y": -3.4514341354370117, "TOKEN": "estuarine"}, {"X": -5.051145076751709, "Y": 2.7221381664276123, "TOKEN": "kokosalaki"}, {"X": 0.033664803951978683, "Y": 0.5261523127555847, "TOKEN": "destoroyah"}, {"X": -3.497988700866699, "Y": 4.052121162414551, "TOKEN": "jawdat"}, {"X": -0.7007628679275513, "Y": 1.6158668994903564, "TOKEN": "cersei"}, {"X": 2.222219705581665, "Y": -2.862352132797241, "TOKEN": "\u02dac"}, {"X": 3.2595181465148926, "Y": 3.385321617126465, "TOKEN": "rhoshii"}, {"X": -3.036191463470459, "Y": 4.930931568145752, "TOKEN": "monterroso"}, {"X": -1.300661563873291, "Y": 0.17232970893383026, "TOKEN": "sranan"}, {"X": 1.0366504192352295, "Y": -4.1028852462768555, "TOKEN": "u.s.-north"}, {"X": -0.5176072716712952, "Y": -1.2961021661758423, "TOKEN": "satans"}, {"X": -0.1167353019118309, "Y": -3.362811326980591, "TOKEN": "newsflash"}, {"X": -2.142746686935425, "Y": -0.8095759749412537, "TOKEN": "kirundi"}, {"X": -3.0795629024505615, "Y": 3.7175631523132324, "TOKEN": "howeish"}, {"X": -1.516010046005249, "Y": -1.3776774406433105, "TOKEN": "anthropoids"}, {"X": 2.5373547077178955, "Y": -4.224574565887451, "TOKEN": "174.5"}, {"X": -0.37306755781173706, "Y": 5.92426061630249, "TOKEN": "dastgir"}, {"X": -5.122848033905029, "Y": 3.518911600112915, "TOKEN": "fogler"}, {"X": 3.369586944580078, "Y": 1.5350841283798218, "TOKEN": "camaleon"}, {"X": -0.35194534063339233, "Y": 5.804253578186035, "TOKEN": "jufeng"}, {"X": -3.2911324501037598, "Y": -1.8652315139770508, "TOKEN": "treacly"}, {"X": -2.638465166091919, "Y": -4.324077606201172, "TOKEN": "fertilizes"}, {"X": -2.2442352771759033, "Y": 3.192337989807129, "TOKEN": "razziq"}, {"X": 2.5332510471343994, "Y": -3.7235617637634277, "TOKEN": "69.44"}, {"X": 1.9353663921356201, "Y": 2.578087568283081, "TOKEN": "rhdp"}, {"X": 5.163637161254883, "Y": -0.9950976371765137, "TOKEN": "hanborough"}, {"X": -3.7676234245300293, "Y": 0.0474214069545269, "TOKEN": "hilfiger"}, {"X": 1.4775464534759521, "Y": -6.393705368041992, "TOKEN": "19-foot"}, {"X": -0.7075648903846741, "Y": 1.3855669498443604, "TOKEN": "bo\u017eena"}, {"X": -6.299725532531738, "Y": -3.4917685985565186, "TOKEN": "bite"}, {"X": 3.0632081031799316, "Y": 1.0414592027664185, "TOKEN": "wltw"}, {"X": -6.381618022918701, "Y": -3.2240190505981445, "TOKEN": "teaching"}, {"X": -3.431117534637451, "Y": 1.5582398176193237, "TOKEN": "fireman"}, {"X": -0.6116020083427429, "Y": 3.8554954528808594, "TOKEN": "daughton"}, {"X": 5.231817245483398, "Y": -1.4801993370056152, "TOKEN": "tapao"}, {"X": 2.112527370452881, "Y": 1.560782551765442, "TOKEN": "tortricidae"}, {"X": 0.5935952067375183, "Y": 0.14419059455394745, "TOKEN": "guaiacum"}, {"X": -0.17475666105747223, "Y": 1.573980689048767, "TOKEN": "grafenauer"}, {"X": -2.9393608570098877, "Y": 1.7852368354797363, "TOKEN": "35-year-old"}, {"X": 3.341660261154175, "Y": 0.21750831604003906, "TOKEN": "doncello"}, {"X": -5.142721652984619, "Y": -1.2465842962265015, "TOKEN": "1-marcos"}, {"X": 6.473453044891357, "Y": 1.6723741292953491, "TOKEN": "zavvi"}, {"X": 0.19634675979614258, "Y": 4.376881122589111, "TOKEN": "clower"}, {"X": 0.7359952330589294, "Y": 6.2893524169921875, "TOKEN": "yumi"}, {"X": 4.849889278411865, "Y": 1.0083873271942139, "TOKEN": "bochnia"}, {"X": -1.6503684520721436, "Y": 3.3634488582611084, "TOKEN": "brunious"}, {"X": -0.770328938961029, "Y": 5.536957263946533, "TOKEN": "sudjana"}, {"X": -1.296342134475708, "Y": -1.8369114398956299, "TOKEN": "serpentinite"}, {"X": 0.9665226936340332, "Y": -0.8895454406738281, "TOKEN": "centauros"}, {"X": 0.8703761696815491, "Y": 1.7917909622192383, "TOKEN": "rott"}, {"X": 0.3978448808193207, "Y": -1.0368666648864746, "TOKEN": "yibna"}, {"X": 0.6391910910606384, "Y": 0.756290853023529, "TOKEN": "d\u014d"}, {"X": 3.273577928543091, "Y": 1.8549410104751587, "TOKEN": "piquette"}, {"X": 0.2835421562194824, "Y": 4.681025981903076, "TOKEN": "wilhite"}, {"X": -3.956402540206909, "Y": 2.0738024711608887, "TOKEN": "ishii"}, {"X": -3.299757480621338, "Y": -0.6466087102890015, "TOKEN": "isn"}, {"X": -2.0683724880218506, "Y": 1.4292718172073364, "TOKEN": "mudford"}, {"X": -3.437708616256714, "Y": -0.7555591464042664, "TOKEN": "movin"}, {"X": 2.8567636013031006, "Y": -0.28204506635665894, "TOKEN": "rinteln"}, {"X": -0.01219396386295557, "Y": -1.8809726238250732, "TOKEN": "lorentzian"}, {"X": 0.12580369412899017, "Y": -4.073676109313965, "TOKEN": "25-seat"}, {"X": -0.4834960401058197, "Y": -1.7614530324935913, "TOKEN": "bigwigs"}, {"X": -1.2711141109466553, "Y": 3.613245725631714, "TOKEN": "eberts"}, {"X": 0.3784184157848358, "Y": 1.8078712224960327, "TOKEN": "syrie"}, {"X": -0.4795849621295929, "Y": 4.057555198669434, "TOKEN": "munsterman"}, {"X": -0.6372260451316833, "Y": 2.5201756954193115, "TOKEN": "tamburrino"}, {"X": -0.5347198843955994, "Y": 2.9210755825042725, "TOKEN": "dimitrije"}, {"X": 1.5293159484863281, "Y": -4.5185322761535645, "TOKEN": "rm0"}, {"X": -0.861787736415863, "Y": -1.441931962966919, "TOKEN": "ldap"}, {"X": -0.5136367678642273, "Y": -6.143360614776611, "TOKEN": "737-600"}, {"X": -0.7616261839866638, "Y": -2.4556455612182617, "TOKEN": "luxuriousness"}, {"X": 0.5348389744758606, "Y": 2.339507818222046, "TOKEN": "maty"}, {"X": 2.8079476356506348, "Y": -3.3293557167053223, "TOKEN": "85.20"}, {"X": -6.798669338226318, "Y": 2.681922674179077, "TOKEN": "maurya"}, {"X": -0.15566763281822205, "Y": -3.1892905235290527, "TOKEN": "mi-8s"}, {"X": -0.6854631900787354, "Y": -1.3218330144882202, "TOKEN": "case-insensitive"}, {"X": 3.0101377964019775, "Y": -0.1980898231267929, "TOKEN": "watonga"}, {"X": -2.3921620845794678, "Y": -5.765665054321289, "TOKEN": "brie"}, {"X": -3.510413646697998, "Y": -0.9746548533439636, "TOKEN": "damned"}, {"X": 0.8632700443267822, "Y": -0.9907852411270142, "TOKEN": "weedeater"}, {"X": -6.9245500564575195, "Y": -0.06309468299150467, "TOKEN": "76-73"}, {"X": 1.471376657485962, "Y": -1.1618661880493164, "TOKEN": "ilh"}, {"X": 4.245445728302002, "Y": -2.4413681030273438, "TOKEN": "toccatas"}, {"X": -2.415955066680908, "Y": 7.321242332458496, "TOKEN": "logie"}, {"X": -1.9850012063980103, "Y": -1.408779263496399, "TOKEN": "90-hour"}, {"X": 1.2091065645217896, "Y": 1.1041548252105713, "TOKEN": "hnlms"}, {"X": -0.40755313634872437, "Y": 0.6433344483375549, "TOKEN": "amai"}, {"X": -1.2923790216445923, "Y": -0.880963921546936, "TOKEN": "hydralazine"}, {"X": 2.6045081615448, "Y": 0.43777355551719666, "TOKEN": "oxus"}, {"X": -0.16137640178203583, "Y": -2.989367961883545, "TOKEN": "fairy-like"}, {"X": -3.4170663356781006, "Y": -2.5367603302001953, "TOKEN": "theosophists"}, {"X": 0.1218673586845398, "Y": 0.1712789237499237, "TOKEN": "vulgaria"}, {"X": -5.518921852111816, "Y": -2.9700841903686523, "TOKEN": "dividing"}, {"X": -0.3197990357875824, "Y": 2.984489917755127, "TOKEN": "bezanson"}, {"X": 1.017146348953247, "Y": -0.5111675262451172, "TOKEN": "aeria"}, {"X": -0.8605577349662781, "Y": 0.061127159744501114, "TOKEN": "amrapali"}, {"X": -0.15564243495464325, "Y": 4.013552188873291, "TOKEN": "kilbourne"}, {"X": -4.745069980621338, "Y": 2.052943229675293, "TOKEN": "breyten"}, {"X": -1.8807718753814697, "Y": -2.1033966541290283, "TOKEN": "logbooks"}, {"X": -2.3457930088043213, "Y": -0.8294879198074341, "TOKEN": "phosphatases"}, {"X": -0.41007447242736816, "Y": -0.33424150943756104, "TOKEN": "freakum"}, {"X": 1.0922086238861084, "Y": 1.3210678100585938, "TOKEN": "jamais"}, {"X": 1.7097421884536743, "Y": -1.69334077835083, "TOKEN": "sphl"}, {"X": -0.4000631272792816, "Y": 6.999017715454102, "TOKEN": "povetkin"}, {"X": 0.8392562866210938, "Y": 6.037268161773682, "TOKEN": "avers"}, {"X": 3.421748161315918, "Y": -3.3191535472869873, "TOKEN": "109.21"}, {"X": -1.0210572481155396, "Y": 0.8567553758621216, "TOKEN": "snagglepuss"}, {"X": -3.835038661956787, "Y": 0.24262750148773193, "TOKEN": "yankeenets"}, {"X": -3.6007096767425537, "Y": -1.9108800888061523, "TOKEN": "spirituals"}, {"X": 5.506478786468506, "Y": 2.755401611328125, "TOKEN": "galavisi\u00f3n"}, {"X": 0.8466315865516663, "Y": 6.335511684417725, "TOKEN": "bagger"}, {"X": 1.9857983589172363, "Y": -0.8218477368354797, "TOKEN": "noja"}, {"X": -3.822972059249878, "Y": -2.0709681510925293, "TOKEN": "bibles"}, {"X": -1.6248725652694702, "Y": 2.944216728210449, "TOKEN": "tabea"}, {"X": -0.5189255475997925, "Y": 3.6316635608673096, "TOKEN": "bernsdorff"}, {"X": -0.4236864447593689, "Y": 4.151956081390381, "TOKEN": "wemmer"}, {"X": -4.595667362213135, "Y": 1.4147306680679321, "TOKEN": "b-12"}, {"X": -2.0978147983551025, "Y": 0.4518289268016815, "TOKEN": "babka"}, {"X": -0.4241254925727844, "Y": 2.314905881881714, "TOKEN": "simonne"}, {"X": -3.3065688610076904, "Y": -1.1664315462112427, "TOKEN": "heroines"}, {"X": 1.3980995416641235, "Y": 1.6632177829742432, "TOKEN": "auxiliar"}, {"X": -0.5937578678131104, "Y": -1.4662047624588013, "TOKEN": "subspecific"}, {"X": -2.5231337547302246, "Y": -0.021280353888869286, "TOKEN": "xyy"}, {"X": -3.24580979347229, "Y": -3.3276259899139404, "TOKEN": "behooves"}, {"X": 1.0924557447433472, "Y": -1.0823665857315063, "TOKEN": "ralliart"}, {"X": -4.194538593292236, "Y": -0.46306711435317993, "TOKEN": "brokered"}, {"X": -0.9504668712615967, "Y": -2.6521008014678955, "TOKEN": "coziness"}, {"X": -3.2169694900512695, "Y": -1.34792160987854, "TOKEN": "trustworthy"}, {"X": -3.232922315597534, "Y": -0.43963199853897095, "TOKEN": "pachacutec"}, {"X": 3.014150381088257, "Y": -0.3218182921409607, "TOKEN": "cowick"}, {"X": 2.9208383560180664, "Y": -0.6919279098510742, "TOKEN": "nhill"}, {"X": 2.310187339782715, "Y": -2.829028367996216, "TOKEN": "6:13"}, {"X": -0.4454020857810974, "Y": -5.969161033630371, "TOKEN": "777-200er"}, {"X": -2.2969775199890137, "Y": -3.2772014141082764, "TOKEN": "repossess"}, {"X": 2.8350846767425537, "Y": -4.34003210067749, "TOKEN": "5.89"}, {"X": 3.9323010444641113, "Y": -3.878338098526001, "TOKEN": "fiorano"}, {"X": 3.6150853633880615, "Y": 2.230565309524536, "TOKEN": "trabert"}, {"X": -0.5139806866645813, "Y": -0.38217949867248535, "TOKEN": "spiderwebs"}, {"X": -1.3199050426483154, "Y": -0.017841219902038574, "TOKEN": "mojtahed"}, {"X": -3.4129576683044434, "Y": -0.13393434882164001, "TOKEN": "molly"}, {"X": -2.6286473274230957, "Y": -0.872711181640625, "TOKEN": "collateralised"}, {"X": 2.3791215419769287, "Y": -1.7171316146850586, "TOKEN": "1901-1902"}, {"X": -0.9271507263183594, "Y": 1.2115187644958496, "TOKEN": "nicha"}, {"X": 0.2249538153409958, "Y": 2.77278995513916, "TOKEN": "benzer"}, {"X": 0.8018366098403931, "Y": -3.1527457237243652, "TOKEN": "eight-car"}, {"X": -2.6494317054748535, "Y": 3.2734475135803223, "TOKEN": "minty"}, {"X": -1.9742168188095093, "Y": -1.1302001476287842, "TOKEN": "disparages"}, {"X": 1.2337990999221802, "Y": 2.2427284717559814, "TOKEN": "molen"}, {"X": -2.853076934814453, "Y": 2.4825828075408936, "TOKEN": "fauchon"}, {"X": -0.7433911561965942, "Y": 5.074145317077637, "TOKEN": "wilmerding"}, {"X": -3.206798791885376, "Y": -0.627529501914978, "TOKEN": "x.xx.xx.xx.x"}, {"X": 3.936239242553711, "Y": 3.683332681655884, "TOKEN": "inteligente"}, {"X": 0.1203841120004654, "Y": 3.0407137870788574, "TOKEN": "o'harrow"}, {"X": -6.66242790222168, "Y": -2.3524067401885986, "TOKEN": "musician"}, {"X": -0.8136078119277954, "Y": -2.547804594039917, "TOKEN": "mariological"}, {"X": 0.7800894379615784, "Y": -1.7628796100616455, "TOKEN": "highest-paid"}, {"X": 1.6666696071624756, "Y": 0.24405747652053833, "TOKEN": "chesed"}, {"X": -4.690006732940674, "Y": 3.487828016281128, "TOKEN": "sund"}, {"X": -0.6346942186355591, "Y": -3.6366071701049805, "TOKEN": "radiated"}, {"X": -0.9127331376075745, "Y": -0.14229127764701843, "TOKEN": "britannic"}, {"X": -6.353951454162598, "Y": -2.0668270587921143, "TOKEN": "hevesi"}, {"X": -6.076382160186768, "Y": -3.065415859222412, "TOKEN": "profile"}, {"X": -0.14395026862621307, "Y": -1.6705470085144043, "TOKEN": "hypochondriacal"}, {"X": 1.938109278678894, "Y": -1.5493220090866089, "TOKEN": "xviii"}, {"X": 0.4925103187561035, "Y": -0.4295445382595062, "TOKEN": "b.i."}, {"X": -1.5478308200836182, "Y": -2.5629689693450928, "TOKEN": "marinating"}, {"X": -5.899946689605713, "Y": -0.46502020955085754, "TOKEN": "finland"}, {"X": -3.3135159015655518, "Y": 3.8938348293304443, "TOKEN": "malaki"}, {"X": 5.567732334136963, "Y": 2.8583343029022217, "TOKEN": "ort"}, {"X": 3.1059718132019043, "Y": -4.334353923797607, "TOKEN": "11.13"}, {"X": -0.046548325568437576, "Y": 2.616666555404663, "TOKEN": "soeryadjaya"}, {"X": 3.3414580821990967, "Y": -3.112281084060669, "TOKEN": "109.19"}, {"X": -3.1504898071289062, "Y": -7.149704933166504, "TOKEN": "h6"}, {"X": -1.0636829137802124, "Y": 4.34072208404541, "TOKEN": "dellavedova"}, {"X": 0.38719117641448975, "Y": 1.740709662437439, "TOKEN": "baso"}, {"X": 0.8946009278297424, "Y": -3.985924005508423, "TOKEN": "full-thickness"}, {"X": -6.256598472595215, "Y": -3.3946077823638916, "TOKEN": "breath"}, {"X": 0.716523289680481, "Y": 0.8947343230247498, "TOKEN": "skully"}, {"X": -4.479796886444092, "Y": -4.82516622543335, "TOKEN": "capricorni"}, {"X": 1.681504249572754, "Y": -6.788434028625488, "TOKEN": ".230"}, {"X": 3.2994844913482666, "Y": -0.041545603424310684, "TOKEN": "grangeville"}, {"X": -5.576884746551514, "Y": 1.113685965538025, "TOKEN": "sieberg"}, {"X": -1.9667301177978516, "Y": -0.7647712826728821, "TOKEN": "sould"}, {"X": -1.9205964803695679, "Y": -5.824026107788086, "TOKEN": "accompaniments"}, {"X": -1.2841564416885376, "Y": -2.9386911392211914, "TOKEN": "tattling"}]}}, {"mode": "vega-lite"});
</script></div></div>
</div>
</div>
<div class="section" id="other-relationships">
<h3><span class="section-number">4.3.3. </span>Other relationships<a class="headerlink" href="#other-relationships" title="Permalink to this headline">¶</a></h3>
<p>Beyond cosine similarity, there are other word relationships to explore via vector space math. For example, one way we might model something like a <em>concept</em> is through addition. What would happen if we added to vectors together to create a new vector? Which words would this new vector be closest to in the vector space? Using the <code class="docutils literal notranslate"><span class="pre">.similar_by_vector()</span></code> method, we can find out.</p>
<div class="margin sidebar">
<p class="sidebar-title">What this loop does</p>
<p>For each concept in our <code class="docutils literal notranslate"><span class="pre">concepts</span></code> dictionary:</p>
<ol class="simple">
<li><p>Get its associated pair of words</p></li>
<li><p>Query the model for those words’ vectors and add them together to create a new vector</p></li>
<li><p>Find the most similar words to this new vector</p></li>
<li><p>Use a dataframe to display the results</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">concepts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;beach&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;sand&#39;</span><span class="p">,</span> <span class="s1">&#39;ocean&#39;</span><span class="p">),</span> <span class="s1">&#39;hotel&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;vacation&#39;</span><span class="p">,</span> <span class="s1">&#39;room&#39;</span><span class="p">),</span> <span class="s1">&#39;airplane&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;air&#39;</span><span class="p">,</span> <span class="s1">&#39;car&#39;</span><span class="p">)}</span>
<span class="k">for</span> <span class="n">concept</span> <span class="ow">in</span> <span class="n">concepts</span><span class="p">:</span>
    <span class="n">pair</span> <span class="o">=</span> <span class="n">concepts</span><span class="p">[</span><span class="n">concept</span><span class="p">]</span>
    <span class="n">generated_concept</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="n">model</span><span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">generated_concept</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Most similar tokens to &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; + &#39;</span><span class="si">{</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; (for &#39;</span><span class="si">{</span><span class="n">concept</span><span class="si">}</span><span class="s2">&#39;)&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
    <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Most similar tokens to &#39;sand&#39; + &#39;ocean&#39; (for &#39;beach&#39;)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>sand</td>
      <td>0.845458</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ocean</td>
      <td>0.845268</td>
    </tr>
    <tr>
      <th>2</th>
      <td>sea</td>
      <td>0.687682</td>
    </tr>
    <tr>
      <th>3</th>
      <td>beaches</td>
      <td>0.667521</td>
    </tr>
    <tr>
      <th>4</th>
      <td>waters</td>
      <td>0.664894</td>
    </tr>
    <tr>
      <th>5</th>
      <td>coastal</td>
      <td>0.632485</td>
    </tr>
    <tr>
      <th>6</th>
      <td>water</td>
      <td>0.618701</td>
    </tr>
    <tr>
      <th>7</th>
      <td>coast</td>
      <td>0.604373</td>
    </tr>
    <tr>
      <th>8</th>
      <td>dunes</td>
      <td>0.599333</td>
    </tr>
    <tr>
      <th>9</th>
      <td>surface</td>
      <td>0.597545</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Most similar tokens to &#39;vacation&#39; + &#39;room&#39; (for &#39;hotel&#39;)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>vacation</td>
      <td>0.823460</td>
    </tr>
    <tr>
      <th>1</th>
      <td>room</td>
      <td>0.810719</td>
    </tr>
    <tr>
      <th>2</th>
      <td>rooms</td>
      <td>0.704233</td>
    </tr>
    <tr>
      <th>3</th>
      <td>bedroom</td>
      <td>0.658199</td>
    </tr>
    <tr>
      <th>4</th>
      <td>hotel</td>
      <td>0.647865</td>
    </tr>
    <tr>
      <th>5</th>
      <td>dining</td>
      <td>0.634925</td>
    </tr>
    <tr>
      <th>6</th>
      <td>stay</td>
      <td>0.617807</td>
    </tr>
    <tr>
      <th>7</th>
      <td>apartment</td>
      <td>0.616495</td>
    </tr>
    <tr>
      <th>8</th>
      <td>staying</td>
      <td>0.615182</td>
    </tr>
    <tr>
      <th>9</th>
      <td>home</td>
      <td>0.606009</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Most similar tokens to &#39;air&#39; + &#39;car&#39; (for &#39;airplane&#39;)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>air</td>
      <td>0.827957</td>
    </tr>
    <tr>
      <th>1</th>
      <td>car</td>
      <td>0.810086</td>
    </tr>
    <tr>
      <th>2</th>
      <td>vehicle</td>
      <td>0.719382</td>
    </tr>
    <tr>
      <th>3</th>
      <td>cars</td>
      <td>0.671697</td>
    </tr>
    <tr>
      <th>4</th>
      <td>truck</td>
      <td>0.645963</td>
    </tr>
    <tr>
      <th>5</th>
      <td>vehicles</td>
      <td>0.637166</td>
    </tr>
    <tr>
      <th>6</th>
      <td>passenger</td>
      <td>0.625993</td>
    </tr>
    <tr>
      <th>7</th>
      <td>aircraft</td>
      <td>0.624820</td>
    </tr>
    <tr>
      <th>8</th>
      <td>jet</td>
      <td>0.618584</td>
    </tr>
    <tr>
      <th>9</th>
      <td>airplane</td>
      <td>0.610345</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Not bad! Our target concept isn’t the most similar word for either of these examples, but it’s in the top 10.</p>
<p>Most famously, word embeddings enable quasi-logical reasoning. Though, as we mentioned earlier, relationships between antonyms and synonyms do not necessarily map to a vector space, certain analogies do. The logic here is that we add together two vectors, analogical together and then subtract a third vector, which has some kind of relation to one of the words in the analogue pair. Querying for the resultant vector should produce a similar relation between the resultant word and the other word in the analogue pair.</p>
<p>Here, we ask: “king is to man what X is to woman?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogies</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">analogies</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>queen</td>
      <td>0.697868</td>
    </tr>
    <tr>
      <th>1</th>
      <td>princess</td>
      <td>0.608175</td>
    </tr>
    <tr>
      <th>2</th>
      <td>monarch</td>
      <td>0.588975</td>
    </tr>
    <tr>
      <th>3</th>
      <td>throne</td>
      <td>0.577511</td>
    </tr>
    <tr>
      <th>4</th>
      <td>prince</td>
      <td>0.575100</td>
    </tr>
    <tr>
      <th>5</th>
      <td>elizabeth</td>
      <td>0.546360</td>
    </tr>
    <tr>
      <th>6</th>
      <td>daughter</td>
      <td>0.539913</td>
    </tr>
    <tr>
      <th>7</th>
      <td>kingdom</td>
      <td>0.531805</td>
    </tr>
    <tr>
      <th>8</th>
      <td>mother</td>
      <td>0.516854</td>
    </tr>
    <tr>
      <th>9</th>
      <td>crown</td>
      <td>0.516447</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And here, we ask: “France is to Paris what X is to Berlin”?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogies</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;france&#39;</span><span class="p">,</span> <span class="s1">&#39;berlin&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;paris&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">analogies</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>germany</td>
      <td>0.835242</td>
    </tr>
    <tr>
      <th>1</th>
      <td>german</td>
      <td>0.684480</td>
    </tr>
    <tr>
      <th>2</th>
      <td>austria</td>
      <td>0.612803</td>
    </tr>
    <tr>
      <th>3</th>
      <td>poland</td>
      <td>0.581331</td>
    </tr>
    <tr>
      <th>4</th>
      <td>germans</td>
      <td>0.574868</td>
    </tr>
    <tr>
      <th>5</th>
      <td>munich</td>
      <td>0.543591</td>
    </tr>
    <tr>
      <th>6</th>
      <td>belgium</td>
      <td>0.532413</td>
    </tr>
    <tr>
      <th>7</th>
      <td>britain</td>
      <td>0.529541</td>
    </tr>
    <tr>
      <th>8</th>
      <td>europe</td>
      <td>0.524402</td>
    </tr>
    <tr>
      <th>9</th>
      <td>czech</td>
      <td>0.515241</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Both of the above produce compelling results, though your mileage may vary. Consider the following: “arm is to hand what leg is to X?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogies</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;arm&#39;</span><span class="p">,</span> <span class="s1">&#39;leg&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;hand&#39;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">analogies</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;SCORE&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>legs</td>
      <td>0.519315</td>
    </tr>
    <tr>
      <th>1</th>
      <td>groin</td>
      <td>0.501345</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spinner</td>
      <td>0.493277</td>
    </tr>
    <tr>
      <th>3</th>
      <td>thigh</td>
      <td>0.476108</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ankle</td>
      <td>0.465417</td>
    </tr>
    <tr>
      <th>5</th>
      <td>knee</td>
      <td>0.461705</td>
    </tr>
    <tr>
      <th>6</th>
      <td>wrist</td>
      <td>0.455966</td>
    </tr>
    <tr>
      <th>7</th>
      <td>seamer</td>
      <td>0.454812</td>
    </tr>
    <tr>
      <th>8</th>
      <td>calf</td>
      <td>0.453236</td>
    </tr>
    <tr>
      <th>9</th>
      <td>paceman</td>
      <td>0.453196</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
</div>
<div class="section" id="document-similarity">
<h2><span class="section-number">4.4. </span>Document similarity<a class="headerlink" href="#document-similarity" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pruned</span> <span class="o">=</span> <span class="p">[[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">in_glove</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
<span class="n">doc_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="n">doc</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">pruned</span><span class="p">]</span>
<span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">)</span>
<span class="n">vis_data</span> <span class="o">=</span> <span class="n">prepare_vis_data</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;NAME&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">vis_data</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
    <span class="n">tooltip</span><span class="o">=</span><span class="s1">&#39;TOKEN&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">650</span>
<span class="p">)</span><span class="o">.</span><span class="n">interactive</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-fd7822dded854a62a07f8fa33980618d"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-fd7822dded854a62a07f8fa33980618d") {
      outputDiv = document.getElementById("altair-viz-fd7822dded854a62a07f8fa33980618d");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "4.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "data": {"name": "data-61f387aa92192012459f5273bcb96484"}, "mark": {"type": "circle", "size": 30}, "encoding": {"tooltip": {"field": "TOKEN", "type": "nominal"}, "x": {"field": "X", "type": "quantitative"}, "y": {"field": "Y", "type": "quantitative"}}, "height": 650, "selection": {"selector002": {"type": "interval", "bind": "scales", "encodings": ["x", "y"]}}, "width": 650, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-61f387aa92192012459f5273bcb96484": [{"X": 9.629589080810547, "Y": 0.9365087747573853, "TOKEN": "Ada Lovelace"}, {"X": 4.867339134216309, "Y": -13.727782249450684, "TOKEN": "Robert E Lee"}, {"X": 2.516657829284668, "Y": -18.354692459106445, "TOKEN": "Andrew Johnson"}, {"X": 5.089731216430664, "Y": -12.619865417480469, "TOKEN": "Bedford Forrest"}, {"X": -0.30361512303352356, "Y": -9.816756248474121, "TOKEN": "Lucretia Mott"}, {"X": 11.412369728088379, "Y": 0.43414148688316345, "TOKEN": "Charles Darwin"}, {"X": 3.7749245166778564, "Y": -13.713809967041016, "TOKEN": "Ulysses Grant"}, {"X": -10.804316520690918, "Y": 3.915217161178589, "TOKEN": "Mary Ewing Outerbridge"}, {"X": 5.313749313354492, "Y": 7.089348793029785, "TOKEN": "Emma Lazarus"}, {"X": 0.1769566833972931, "Y": 4.888604164123535, "TOKEN": "Louisa M Alcott"}, {"X": -2.5393667221069336, "Y": -1.0644351243972778, "TOKEN": "P T Barnum"}, {"X": 2.2706544399261475, "Y": 4.895766735076904, "TOKEN": "R L Stevenson"}, {"X": 1.4236607551574707, "Y": -11.55919361114502, "TOKEN": "Fred Douglass"}, {"X": 1.1170852184295654, "Y": 5.052165508270264, "TOKEN": "Harriet Beecher Stowe"}, {"X": 1.3462700843811035, "Y": 3.350062131881714, "TOKEN": "Stephen Crane"}, {"X": 11.823625564575195, "Y": 5.069955348968506, "TOKEN": "Nietzsche"}, {"X": 3.474562406539917, "Y": -14.088415145874023, "TOKEN": "William McKinley"}, {"X": -5.221592903137207, "Y": -9.45670223236084, "TOKEN": "Queen Victoria"}, {"X": -0.3932449519634247, "Y": -16.153573989868164, "TOKEN": "Benjamin Harrison"}, {"X": 0.8819878101348877, "Y": -10.319847106933594, "TOKEN": "Elizabeth Cady Stanton"}, {"X": 5.680935859680176, "Y": 11.466192245483398, "TOKEN": "James M N Whistler"}, {"X": -6.502901077270508, "Y": -3.6163651943206787, "TOKEN": "Emily Warren Roebling"}, {"X": 0.9912346601486206, "Y": -10.336971282958984, "TOKEN": "Susan B Anthony"}, {"X": 13.706411361694336, "Y": -15.936660766601562, "TOKEN": "Qiu Jin"}, {"X": 1.1903727054595947, "Y": -15.833939552307129, "TOKEN": "Cleveland"}, {"X": 0.06014833599328995, "Y": 3.009098529815674, "TOKEN": "Sarah Orne Jewett"}, {"X": 5.059043884277344, "Y": -12.145303726196289, "TOKEN": "Geronimo"}, {"X": 8.49888801574707, "Y": -6.145576000213623, "TOKEN": "William James"}, {"X": -5.059149742126465, "Y": -8.053226470947266, "TOKEN": "Florence Nightingale"}, {"X": 9.45051097869873, "Y": 4.464698314666748, "TOKEN": "Tolstoy"}, {"X": 2.86517333984375, "Y": -4.517377853393555, "TOKEN": "Joseph Pulitzer"}, {"X": 7.037650108337402, "Y": -10.98705768585205, "TOKEN": "John P Holland"}, {"X": 7.9997172355651855, "Y": -11.967913627624512, "TOKEN": "Alfred Thayer Mahan"}, {"X": 7.371427536010742, "Y": -2.075955390930176, "TOKEN": "John Muir"}, {"X": -0.6195724010467529, "Y": -4.894759654998779, "TOKEN": "F W Taylor"}, {"X": -1.4776277542114258, "Y": -9.218297958374023, "TOKEN": "B T Washington"}, {"X": -3.7878124713897705, "Y": -4.599079608917236, "TOKEN": "J J Hill"}, {"X": -1.648998737335205, "Y": 2.9855120182037354, "TOKEN": "Jack London"}, {"X": 9.92032241821289, "Y": -1.5003327131271362, "TOKEN": "Martian Theory"}, {"X": 7.427640914916992, "Y": 11.48624324798584, "TOKEN": "Hilaire G E Degas"}, {"X": -4.147123336791992, "Y": -3.4893581867218018, "TOKEN": "C J Walker"}, {"X": -2.7459497451782227, "Y": -4.899856090545654, "TOKEN": "Carnegie Started"}, {"X": 0.44971612095832825, "Y": -10.595927238464355, "TOKEN": "Anna H Shaw"}, {"X": -1.8113512992858887, "Y": 13.811561584472656, "TOKEN": "Marlene Dietrich"}, {"X": -1.9595662355422974, "Y": 1.944274663925171, "TOKEN": "Nellie Bly"}, {"X": -3.6001923084259033, "Y": 0.19057707488536835, "TOKEN": "Alexander Graham Bell"}, {"X": 1.1338599920272827, "Y": -17.525060653686523, "TOKEN": "Warren Harding"}, {"X": -3.8007283210754395, "Y": 5.842870712280273, "TOKEN": "Harry Houdini"}, {"X": 0.49707192182540894, "Y": -9.317200660705566, "TOKEN": "Victoria Martin"}, {"X": -2.137650966644287, "Y": -11.47287368774414, "TOKEN": "Mabel Craty"}, {"X": 13.6246976852417, "Y": -0.4075273275375366, "TOKEN": "Marie Curie"}, {"X": 9.034079551696777, "Y": -13.869738578796387, "TOKEN": "Balfour"}, {"X": 6.1557183265686035, "Y": -4.759181499481201, "TOKEN": "Elmer Sperry"}, {"X": 2.7730519771575928, "Y": -17.90154457092285, "TOKEN": "William Howard Taft"}, {"X": 4.122943878173828, "Y": 5.514073848724365, "TOKEN": "Conan Doyle"}, {"X": 3.894010305404663, "Y": -7.373220443725586, "TOKEN": "Ida B Wells"}, {"X": -0.6402124762535095, "Y": -2.9942216873168945, "TOKEN": "Melvil Dewey"}, {"X": 0.8306495547294617, "Y": 10.522436141967773, "TOKEN": "Thomas Edison"}, {"X": -13.630939483642578, "Y": 3.5920586585998535, "TOKEN": "Knute Rocke"}, {"X": -11.455645561218262, "Y": 17.965166091918945, "TOKEN": "John Philip Sousa"}, {"X": -6.687372207641602, "Y": 12.15854263305664, "TOKEN": "Florenz Ziegfeld"}, {"X": 2.818314552307129, "Y": -17.133880615234375, "TOKEN": "Calvin Coolidge"}, {"X": -1.5672276020050049, "Y": 4.57751989364624, "TOKEN": "Ring Lardner"}, {"X": -7.8486647605896, "Y": -0.3336794078350067, "TOKEN": "Louis C Tiffany"}, {"X": -3.5341227054595947, "Y": 0.47870972752571106, "TOKEN": "T A Watson"}, {"X": 4.497917652130127, "Y": -9.539314270019531, "TOKEN": "Justice Holmes"}, {"X": 2.8870668411254883, "Y": -10.985051155090332, "TOKEN": "Jane Addams"}, {"X": -2.852731466293335, "Y": 5.445327281951904, "TOKEN": "Will Rogers"}, {"X": 2.634998083114624, "Y": -4.535669803619385, "TOKEN": "Adolph S Ochs"}, {"X": -0.8043901920318604, "Y": 6.835999965667725, "TOKEN": "Anne Macy"}, {"X": -14.196510314941406, "Y": 2.408979892730713, "TOKEN": "John W Heisman"}, {"X": 9.145484924316406, "Y": 4.776538848876953, "TOKEN": "Maxim Gorky"}, {"X": -14.743207931518555, "Y": 16.750234603881836, "TOKEN": "Maurice Ravel"}, {"X": 2.715054750442505, "Y": 4.199475288391113, "TOKEN": "Edith Wharton"}, {"X": -2.939866542816162, "Y": -3.85809588432312, "TOKEN": "John Rockefeller"}, {"X": 4.025189399719238, "Y": -9.70034122467041, "TOKEN": "Clarence Darrow"}, {"X": 13.446882247924805, "Y": -3.987703800201416, "TOKEN": "George E Hale"}, {"X": 2.4567582607269287, "Y": 15.349662780761719, "TOKEN": "Constantin Stanislavsky"}, {"X": 3.4906468391418457, "Y": 7.675736427307129, "TOKEN": "W B Yeats"}, {"X": -8.473063468933105, "Y": -9.35084056854248, "TOKEN": "Pope Pius XI"}, {"X": -5.475143909454346, "Y": 3.418489933013916, "TOKEN": "Howard Carter"}, {"X": 1.8188273906707764, "Y": 4.609699726104736, "TOKEN": "Scott Fitzgerald"}, {"X": -1.1627827882766724, "Y": -7.4035563468933105, "TOKEN": "Marcus Garvey"}, {"X": -4.838081359863281, "Y": 0.37718555331230164, "TOKEN": "Frank Conrad"}, {"X": -16.145322799682617, "Y": 6.350594997406006, "TOKEN": "Lou Gehrig"}, {"X": 3.6721866130828857, "Y": 4.287343502044678, "TOKEN": "James Joyce"}, {"X": 2.9453887939453125, "Y": 2.5019946098327637, "TOKEN": "Virginia Woolf"}, {"X": -7.108323097229004, "Y": 14.725702285766602, "TOKEN": "George M Cohan"}, {"X": 8.089288711547852, "Y": -3.9795265197753906, "TOKEN": "J H Kellogg"}, {"X": 7.745167255401611, "Y": -3.375455617904663, "TOKEN": "George Washington Carver"}, {"X": 0.2762145698070526, "Y": -17.672365188598633, "TOKEN": "Alfred E Smith"}, {"X": 4.564617156982422, "Y": -2.2918553352355957, "TOKEN": "Ida M Tarbell"}, {"X": -3.5655741691589355, "Y": 4.17132568359375, "TOKEN": "Ernie Pyle"}, {"X": 4.576817512512207, "Y": -17.442001342773438, "TOKEN": "Harry S Truman"}, {"X": 5.769756317138672, "Y": -13.118968963623047, "TOKEN": "George Patton"}, {"X": -0.487134724855423, "Y": -17.166996002197266, "TOKEN": "FDR"}, {"X": -10.25243854522705, "Y": 16.744211196899414, "TOKEN": "Jerome Kern"}, {"X": 10.450125694274902, "Y": -17.857093811035156, "TOKEN": "Adolf Hitler"}, {"X": -15.567388534545898, "Y": 17.081615447998047, "TOKEN": "Bela Bartok"}, {"X": 3.3424673080444336, "Y": 4.723523139953613, "TOKEN": "Gertrude Stein"}, {"X": 9.325911521911621, "Y": -12.526713371276855, "TOKEN": "Lord Keynes"}, {"X": 14.586517333984375, "Y": -4.484496593475342, "TOKEN": "C E M Clung"}, {"X": 1.1403337717056274, "Y": 2.509639024734497, "TOKEN": "Willa Cather"}, {"X": -4.850642204284668, "Y": 6.138102054595947, "TOKEN": "Al Capone"}, {"X": 2.3354763984680176, "Y": -15.820819854736328, "TOKEN": "Fiorello La Guardia"}, {"X": 14.609024047851562, "Y": -0.10717794299125671, "TOKEN": "Max Planck"}, {"X": -1.1804206371307373, "Y": -5.477313041687012, "TOKEN": "Henry Ford"}, {"X": 6.029560565948486, "Y": -13.791752815246582, "TOKEN": "John Pershing"}, {"X": 2.0751850605010986, "Y": 15.046469688415527, "TOKEN": "Sergei Eisenstein"}, {"X": 8.11977481842041, "Y": -20.070783615112305, "TOKEN": "Mohandas K Gandhi"}, {"X": -14.677245140075684, "Y": 4.58611536026001, "TOKEN": "Babe Ruth"}, {"X": -0.5605786442756653, "Y": 1.7567886114120483, "TOKEN": "Mitchell"}, {"X": 14.690046310424805, "Y": -2.3300743103027344, "TOKEN": "A J Dempster"}, {"X": 0.5355619788169861, "Y": 4.450295925140381, "TOKEN": "Edna St V Millay"}, {"X": 5.150554656982422, "Y": -16.838817596435547, "TOKEN": "Henry L Stimson"}, {"X": -7.985596656799316, "Y": 14.369399070739746, "TOKEN": "Fanny Brice"}, {"X": 10.201523780822754, "Y": -3.951920509338379, "TOKEN": "Henrietta Lacks"}, {"X": 7.256105422973633, "Y": -15.879838943481445, "TOKEN": "Eva Peron"}, {"X": 7.958618640899658, "Y": -7.40358304977417, "TOKEN": "John Dewey"}, {"X": 8.137778282165527, "Y": -22.409303665161133, "TOKEN": "Chaim Weizmann"}, {"X": -3.047520399093628, "Y": -6.486069679260254, "TOKEN": "Charles Spaulding"}, {"X": 3.39670467376709, "Y": -18.607646942138672, "TOKEN": "Fred Vinson"}, {"X": 0.5231119394302368, "Y": 2.0844290256500244, "TOKEN": "Marjorie Rawlings"}, {"X": 11.478231430053711, "Y": -18.480487823486328, "TOKEN": "Joseph Stalin"}, {"X": -13.02806568145752, "Y": 4.952950477600098, "TOKEN": "Jim Thorpe"}, {"X": -1.8037348985671997, "Y": 3.449126958847046, "TOKEN": "Eugene O Neill"}, {"X": 3.311312437057495, "Y": -3.8299105167388916, "TOKEN": "Anne O Hare McCormick"}, {"X": 5.014588832855225, "Y": 10.005289077758789, "TOKEN": "Frida Kahlo"}, {"X": 8.413653373718262, "Y": -17.18549156188965, "TOKEN": "Getulio Vargas"}, {"X": 14.445099830627441, "Y": -1.8850390911102295, "TOKEN": "Enrico Fermi"}, {"X": 6.477755069732666, "Y": 11.336223602294922, "TOKEN": "Henri Matisse"}, {"X": 7.114038944244385, "Y": -2.987826347351074, "TOKEN": "Liberty H Bailey"}, {"X": -5.830735206604004, "Y": 15.424117088317871, "TOKEN": "Lionel Barrymore"}, {"X": 9.499425888061523, "Y": 6.423043251037598, "TOKEN": "Thomas Mann"}, {"X": 12.435925483703613, "Y": 0.6852051019668579, "TOKEN": "Albert Einstein"}, {"X": -12.06922435760498, "Y": 6.419966220855713, "TOKEN": "Margaret Abbott"}, {"X": -0.3511526584625244, "Y": -12.968914031982422, "TOKEN": "Walter White"}, {"X": -15.838933944702148, "Y": 5.7552266120910645, "TOKEN": "Cy Young"}, {"X": -0.6612284779548645, "Y": -0.9929090142250061, "TOKEN": "Dale Carnegie"}, {"X": -11.752488136291504, "Y": 4.983523368835449, "TOKEN": "Babe Zaharias"}, {"X": -2.0537309646606445, "Y": -3.7528295516967773, "TOKEN": "Charles Merrill"}, {"X": -2.085387706756592, "Y": -4.954380035400391, "TOKEN": "Thomas J Watson Sr"}, {"X": -2.122974395751953, "Y": -5.693045616149902, "TOKEN": "Gerard Swope"}, {"X": 6.393711090087891, "Y": 13.563359260559082, "TOKEN": "Christian Dior"}, {"X": -10.264820098876953, "Y": 19.21905517578125, "TOKEN": "W C Handy"}, {"X": -9.749808311462402, "Y": 19.22344207763672, "TOKEN": "Billie Holiday"}, {"X": -7.139647483825684, "Y": -2.679274320602417, "TOKEN": "Frank Lloyd Wright"}, {"X": -6.252832412719727, "Y": 15.452739715576172, "TOKEN": "Ethel Barrymore"}, {"X": -1.6900622844696045, "Y": 16.44804573059082, "TOKEN": "Cecil De Mille"}, {"X": 14.377174377441406, "Y": -4.461243152618408, "TOKEN": "Ross G Harrison"}, {"X": 5.730043411254883, "Y": -17.34250259399414, "TOKEN": "John Dulles"}, {"X": 8.909778594970703, "Y": 5.260741233825684, "TOKEN": "Boris Pasternak"}, {"X": 15.18835163116455, "Y": -3.321732997894287, "TOKEN": "Beno Gutenberg"}, {"X": 6.0727105140686035, "Y": 0.7039583921432495, "TOKEN": "Emily Post"}, {"X": 1.2883164882659912, "Y": 0.7812788486480713, "TOKEN": "Richard Wright"}, {"X": 6.606455326080322, "Y": -17.90180778503418, "TOKEN": "Hammarskjold"}, {"X": 2.51444411277771, "Y": 5.4511637687683105, "TOKEN": "Ernest Hemingway"}, {"X": 3.3103435039520264, "Y": 9.779006958007812, "TOKEN": "Primitive Artist"}, {"X": -1.2825796604156494, "Y": -11.307229995727539, "TOKEN": "Emily Balch"}, {"X": 1.1649328470230103, "Y": -18.612628936767578, "TOKEN": "Sam Rayburn"}, {"X": 12.17062759399414, "Y": 2.2361319065093994, "TOKEN": "Carl G Jung"}, {"X": -4.2258782386779785, "Y": 10.714470863342285, "TOKEN": "Marilyn Monroe"}, {"X": 1.90577232837677, "Y": -14.886177062988281, "TOKEN": "Eleanor Roosevelt"}, {"X": 3.077925443649292, "Y": 3.4568052291870117, "TOKEN": "William Faulkner"}, {"X": 3.701728343963623, "Y": 2.523127794265747, "TOKEN": "Sylvia Plath"}, {"X": 2.2289395332336426, "Y": -19.08131217956543, "TOKEN": "John F Kennedy"}, {"X": 4.192113876342773, "Y": 3.648496150970459, "TOKEN": "Robert Frost"}, {"X": -0.7063899636268616, "Y": -11.29255485534668, "TOKEN": "W E B DuBois"}, {"X": -1.7259103059768677, "Y": -14.54096508026123, "TOKEN": "Herbert Hoover"}, {"X": 6.282223224639893, "Y": -13.74743938446045, "TOKEN": "Douglas MacArthur"}, {"X": 1.7478026151657104, "Y": 7.9571709632873535, "TOKEN": "Sean O Casey"}, {"X": 8.371752738952637, "Y": -1.8794926404953003, "TOKEN": "Rachel Carson"}, {"X": -9.30225944519043, "Y": 16.191822052001953, "TOKEN": "Cole Porter"}, {"X": 3.076063871383667, "Y": 1.314249038696289, "TOKEN": "Nella Larsen"}, {"X": 1.6825166940689087, "Y": -17.522794723510742, "TOKEN": "Adlai Ewing Stevenson"}, {"X": -3.164984941482544, "Y": 17.577924728393555, "TOKEN": "David O Selznick"}, {"X": 8.69693660736084, "Y": -14.189995765686035, "TOKEN": "Churchill"}, {"X": -15.152002334594727, "Y": 3.9001054763793945, "TOKEN": "Branch Rickey"}, {"X": 11.748671531677246, "Y": 4.267360687255859, "TOKEN": "Martin Buber"}, {"X": 1.4773584604263306, "Y": -3.502977132797241, "TOKEN": "Edward R Murrow"}, {"X": 10.981849670410156, "Y": 3.3068172931671143, "TOKEN": "Albert Schweitzer"}, {"X": 1.810014009475708, "Y": 6.656957626342773, "TOKEN": "Shirley Jackson"}, {"X": 1.8925938606262207, "Y": -9.45506477355957, "TOKEN": "Margaret Sanger"}, {"X": 6.807074069976807, "Y": -11.948288917541504, "TOKEN": "Chester Nimitz"}, {"X": -2.056762218475342, "Y": 12.418746948242188, "TOKEN": "Buster Keaton"}, {"X": -2.6335644721984863, "Y": 6.498964309692383, "TOKEN": "Lenny Bruce"}, {"X": -0.9291820526123047, "Y": 17.55532455444336, "TOKEN": "Walt Disney"}, {"X": -1.7889564037322998, "Y": -5.057743072509766, "TOKEN": "Alfred P Sloan Jr"}, {"X": 10.934745788574219, "Y": -4.189286231994629, "TOKEN": "Gregory Pincus"}, {"X": 3.474595785140991, "Y": -1.6320171356201172, "TOKEN": "Henry R Luce"}, {"X": 5.184930801391602, "Y": 2.462585687637329, "TOKEN": "Langston Hughes"}, {"X": 12.949685096740723, "Y": -1.5517901182174683, "TOKEN": "J Robert Oppenheimer"}, {"X": 0.8506120443344116, "Y": -16.64697265625, "TOKEN": "Robert Francis Kennedy"}, {"X": 0.06093268468976021, "Y": 6.959182262420654, "TOKEN": "Helen Keller"}, {"X": 4.050485610961914, "Y": -1.4843933582305908, "TOKEN": "Upton Sinclair"}, {"X": 4.620257377624512, "Y": -8.236494064331055, "TOKEN": "Martin Luther King Jr"}, {"X": -6.331336498260498, "Y": -12.70248031616211, "TOKEN": "Yuri Gagarin"}, {"X": -7.336552619934082, "Y": -2.553863286972046, "TOKEN": "Mies van der Rohe"}, {"X": 4.473064422607422, "Y": -17.60696029663086, "TOKEN": "David Eisenhower"}, {"X": -12.037976264953613, "Y": 20.4371280670166, "TOKEN": "Coleman Hawkins"}, {"X": -1.0962679386138916, "Y": 14.572998046875, "TOKEN": "Madhubala"}, {"X": -6.652101516723633, "Y": 13.022965431213379, "TOKEN": "Judy Garland"}, {"X": -11.519660949707031, "Y": 5.149566173553467, "TOKEN": "Maureen Connolly"}, {"X": 11.490662574768066, "Y": -16.636425018310547, "TOKEN": "Ho Chi Minh"}, {"X": -12.171581268310547, "Y": 7.268080234527588, "TOKEN": "Sonja Henie"}, {"X": 2.202540159225464, "Y": -16.904741287231445, "TOKEN": "Everett Dirksen"}, {"X": -2.9562528133392334, "Y": -14.008023262023926, "TOKEN": "Walter Reuther"}, {"X": 10.0339937210083, "Y": -17.23975372314453, "TOKEN": "Edouard Daladier"}, {"X": 8.901248931884766, "Y": 6.926815032958984, "TOKEN": "Erich Maria Remarque"}, {"X": 9.946044921875, "Y": -16.896339416503906, "TOKEN": "De Gaulle Rallied"}, {"X": 6.375982284545898, "Y": 14.089868545532227, "TOKEN": "Coco Chanel"}, {"X": 5.421616554260254, "Y": -14.56261157989502, "TOKEN": "Florence Blanchfield"}, {"X": 11.947996139526367, "Y": -18.720796585083008, "TOKEN": "Khrushchev"}, {"X": 3.6141252517700195, "Y": 11.977758407592773, "TOKEN": "Diane Arbus"}, {"X": 3.14437198638916, "Y": -11.699508666992188, "TOKEN": "Ralph Bunche"}, {"X": -11.873806953430176, "Y": 5.324117183685303, "TOKEN": "Bobby Jones"}, {"X": -10.47250747680664, "Y": 19.456758499145508, "TOKEN": "Louis Armstrong"}, {"X": 5.490179538726807, "Y": -17.52106285095215, "TOKEN": "Dean Acheson"}, {"X": -14.25207233428955, "Y": 16.682188034057617, "TOKEN": "Igor Stravinsky"}, {"X": 3.6063179969787598, "Y": -20.43814468383789, "TOKEN": "Hugo Black"}, {"X": -8.741172790527344, "Y": 20.10617446899414, "TOKEN": "Mahalia Jackson"}, {"X": -14.888214111328125, "Y": 4.1791863441467285, "TOKEN": "Jackie Robinson"}, {"X": -5.368350982666016, "Y": -9.634806632995605, "TOKEN": "The Duke of Windsor"}, {"X": -0.6426783204078674, "Y": -14.689474105834961, "TOKEN": "J Edgar Hoover"}, {"X": -2.308452844619751, "Y": -14.421500205993652, "TOKEN": "Lyndon Johnson"}, {"X": -13.81699275970459, "Y": 16.135818481445312, "TOKEN": "Otto Klemperer"}, {"X": 0.9663636684417725, "Y": -6.817568778991699, "TOKEN": "Eddie Rickenbacker"}, {"X": 1.193108320236206, "Y": -20.33041000366211, "TOKEN": "Jeanette Rankin"}, {"X": 6.1103901863098145, "Y": 10.780670166015625, "TOKEN": "Pablo Picasso"}, {"X": 0.5244559049606323, "Y": -6.968406677246094, "TOKEN": "Roberto Clemente"}, {"X": 3.218234062194824, "Y": 6.269067764282227, "TOKEN": "Nancy Mitford"}, {"X": 3.557950735092163, "Y": -20.199440002441406, "TOKEN": "Earl Warren"}, {"X": 1.1160459518432617, "Y": 8.574992179870605, "TOKEN": "Sylvia Plath"}, {"X": -6.244418621063232, "Y": 10.41963005065918, "TOKEN": "Ed Sullivan"}, {"X": -6.011885166168213, "Y": 14.240015029907227, "TOKEN": "Katharine Cornell"}, {"X": 2.142029285430908, "Y": -6.557278156280518, "TOKEN": "Charles Lindbergh"}, {"X": 11.842513084411621, "Y": -14.831551551818848, "TOKEN": "Haile Selassie"}, {"X": 5.107857704162598, "Y": -7.8079938888549805, "TOKEN": "Elijah Muhammad"}, {"X": 9.465653419494629, "Y": -17.16649627685547, "TOKEN": "Franco"}, {"X": 3.864711284637451, "Y": 11.90059757232666, "TOKEN": "Walker Evans"}, {"X": 12.281484603881836, "Y": -16.960769653320312, "TOKEN": "Chiang Kai shek"}, {"X": -3.19472599029541, "Y": -3.142597198486328, "TOKEN": "J Paul Getty"}, {"X": 12.985475540161133, "Y": -16.64879608154297, "TOKEN": "Mao Tse Tung"}, {"X": 6.411359786987305, "Y": 10.695928573608398, "TOKEN": "Max Ernst"}, {"X": -1.0085030794143677, "Y": -18.12965202331543, "TOKEN": "Richard Daley"}, {"X": 13.216479301452637, "Y": -0.3249548673629761, "TOKEN": "Jacques Monod"}, {"X": -1.367120623588562, "Y": 17.1641902923584, "TOKEN": "Adolph Zukor"}, {"X": -2.0315723419189453, "Y": 12.687402725219727, "TOKEN": "Charles Chaplin"}, {"X": -5.378741264343262, "Y": 13.529718399047852, "TOKEN": "Joan Crawford"}, {"X": 9.092671394348145, "Y": -14.452479362487793, "TOKEN": "Dash Ended"}, {"X": -11.737982749938965, "Y": 15.359981536865234, "TOKEN": "Maria Callas"}, {"X": 8.854317665100098, "Y": -22.21473503112793, "TOKEN": "Golda Meir"}, {"X": 10.609031677246094, "Y": 1.3082987070083618, "TOKEN": "Margaret Mead"}, {"X": 6.118866920471191, "Y": -9.024884223937988, "TOKEN": "Pope Paul VI"}, {"X": -7.283303260803223, "Y": -6.654460906982422, "TOKEN": "Bruce Catton"}, {"X": -12.953668594360352, "Y": 16.32244873046875, "TOKEN": "Arthur Fiedler"}, {"X": -3.606452465057373, "Y": 12.465727806091309, "TOKEN": "John Wayne"}, {"X": 1.2237907648086548, "Y": -13.392544746398926, "TOKEN": "A Philip Randolph"}, {"X": -12.13571548461914, "Y": 19.790708541870117, "TOKEN": "Stan Kenton"}, {"X": -9.776265144348145, "Y": 16.63808250427246, "TOKEN": "Richard Rodgers"}, {"X": -12.791735649108887, "Y": 4.72695779800415, "TOKEN": "Jesse Owens"}, {"X": 0.1744154691696167, "Y": 13.81369400024414, "TOKEN": "Alfred Hitchcock"}, {"X": 11.404035568237305, "Y": 1.2002414464950562, "TOKEN": "Jean Piaget"}, {"X": 10.629410743713379, "Y": 5.37824010848999, "TOKEN": "Jean Paul Sartre"}, {"X": -9.51002311706543, "Y": 6.543694019317627, "TOKEN": "Joe Louis"}, {"X": -4.598476886749268, "Y": -5.0298991203308105, "TOKEN": "Robert Moses"}, {"X": 9.557087898254395, "Y": -22.23974609375, "TOKEN": "Anwar el Sadat"}, {"X": -4.135482311248779, "Y": 14.641820907592773, "TOKEN": "Ingrid Bergman"}, {"X": 11.428175926208496, "Y": 2.1299140453338623, "TOKEN": "Anna Freud"}, {"X": 11.94921875, "Y": -19.105857849121094, "TOKEN": "Leonid Brezhnev"}, {"X": -13.345328330993652, "Y": 17.284900665283203, "TOKEN": "Arthur Rubinstein"}, {"X": -11.595566749572754, "Y": 19.937498092651367, "TOKEN": "Thelonious Monk"}, {"X": -4.014244556427002, "Y": 16.423534393310547, "TOKEN": "Lee Strasberg"}, {"X": -15.019454956054688, "Y": 4.6996564865112305, "TOKEN": "Satchel Paige"}, {"X": -9.574095726013184, "Y": 6.669792175292969, "TOKEN": "Jack Dempsey"}, {"X": -11.981386184692383, "Y": 21.897388458251953, "TOKEN": "Earl Hines"}, {"X": -10.348932266235352, "Y": 21.249935150146484, "TOKEN": "Muddy Waters"}, {"X": 2.417151689529419, "Y": 5.5440239906311035, "TOKEN": "Truman Capote"}, {"X": 0.5363287925720215, "Y": 6.157252311706543, "TOKEN": "Lillian Hellman"}, {"X": -10.660518646240234, "Y": 7.940510272979736, "TOKEN": "Johnny Weissmuller"}, {"X": -7.373063564300537, "Y": 0.5054081082344055, "TOKEN": "Ansel Adams"}, {"X": -8.515951156616211, "Y": 15.578156471252441, "TOKEN": "Ethel Merman"}, {"X": 8.100703239440918, "Y": -20.053998947143555, "TOKEN": "Indira Gandhi"}, {"X": -3.5809130668640137, "Y": -2.1500587463378906, "TOKEN": "Ray A Kroc"}, {"X": -5.22397518157959, "Y": 15.093145370483398, "TOKEN": "Richard Burton"}, {"X": -11.587529182434082, "Y": 21.17261505126953, "TOKEN": "Count Basie"}, {"X": 1.3015097379684448, "Y": 7.229896545410156, "TOKEN": "E B White"}, {"X": -2.68548321723938, "Y": 15.73446273803711, "TOKEN": "Orson Welles"}, {"X": -15.570432662963867, "Y": 5.330615520477295, "TOKEN": "Roger Maris"}, {"X": -3.728980779647827, "Y": 13.766958236694336, "TOKEN": "James Cagney"}, {"X": 4.857714653015137, "Y": 11.632116317749023, "TOKEN": "Georgia O Keeffe"}, {"X": -11.422660827636719, "Y": 20.63717269897461, "TOKEN": "Benny Goodman"}, {"X": 6.449415683746338, "Y": 5.915811061859131, "TOKEN": "Jorge Luis Borges"}, {"X": 5.973481178283691, "Y": 5.009847164154053, "TOKEN": "Bernard Malamud"}, {"X": -7.632770538330078, "Y": 10.633408546447754, "TOKEN": "Kate Smith"}, {"X": -2.7195780277252197, "Y": -8.981995582580566, "TOKEN": "The Challenger"}, {"X": 8.389544486999512, "Y": 7.210494518280029, "TOKEN": "Primo Levi"}, {"X": 2.7997899055480957, "Y": -2.075590133666992, "TOKEN": "Clare Boothe Luce"}, {"X": -2.9558186531066895, "Y": 14.558831214904785, "TOKEN": "John Huston"}, {"X": -5.270463466644287, "Y": 12.391576766967773, "TOKEN": "Rita Hayworth"}, {"X": 4.1467084884643555, "Y": -6.818028926849365, "TOKEN": "James Baldwin"}, {"X": -0.20809608697891235, "Y": -17.895172119140625, "TOKEN": "Alf Landon"}, {"X": -13.740716934204102, "Y": 18.37775230407715, "TOKEN": "Andres Segovie"}, {"X": -3.0713820457458496, "Y": 16.13964080810547, "TOKEN": "John Houseman"}, {"X": 2.1412930488586426, "Y": 3.7850356101989746, "TOKEN": "Louis L Amour"}, {"X": 12.002198219299316, "Y": -2.0154478549957275, "TOKEN": "William B Shockley"}, {"X": -7.188117504119873, "Y": 13.554760932922363, "TOKEN": "Lucille Ball"}, {"X": 3.9904696941375732, "Y": 3.6188926696777344, "TOKEN": "Robert Penn Warren"}, {"X": -2.993260622024536, "Y": -15.6643705368042, "TOKEN": "Ferdinand Marcos"}, {"X": 12.461625099182129, "Y": -19.119094848632812, "TOKEN": "Andrei Sakharov"}, {"X": 11.80474853515625, "Y": -19.722991943359375, "TOKEN": "Andrei A Gromyko"}, {"X": 3.803722858428955, "Y": -5.2729573249816895, "TOKEN": "I F Stone"}, {"X": -13.136995315551758, "Y": 17.290422439575195, "TOKEN": "Vladimir Horowitz"}, {"X": 12.181300163269043, "Y": -14.329141616821289, "TOKEN": "Hirohito"}, {"X": -4.576432704925537, "Y": -1.7550851106643677, "TOKEN": "August A Busch Jr"}, {"X": 0.6167632937431335, "Y": -19.481542587280273, "TOKEN": "Claude Pepper"}, {"X": 3.112546443939209, "Y": 7.466588973999023, "TOKEN": "Samuel Beckett"}, {"X": -2.007981061935425, "Y": 13.82043743133545, "TOKEN": "Greta Garbo"}, {"X": -5.084105014801025, "Y": 13.166306495666504, "TOKEN": "Sammy Davis Jr"}, {"X": -12.992807388305664, "Y": 16.50141143798828, "TOKEN": "Leonard Bernstein"}, {"X": 5.654520034790039, "Y": 13.032342910766602, "TOKEN": "Erte"}, {"X": 0.8844993710517883, "Y": -12.610634803771973, "TOKEN": "Ralph David Abernathy"}, {"X": -6.092902660369873, "Y": 16.384614944458008, "TOKEN": "Rex Harrison"}, {"X": -2.862534999847412, "Y": 14.518528938293457, "TOKEN": "Frank Capra"}, {"X": -0.7489873766899109, "Y": 8.879402160644531, "TOKEN": "Dr Seuss"}, {"X": -12.450113296508789, "Y": 20.800580978393555, "TOKEN": "Miles Davis"}, {"X": -10.867227554321289, "Y": 13.873992919921875, "TOKEN": "Martha Graham"}, {"X": -15.863840103149414, "Y": 4.244723320007324, "TOKEN": "Leo Durocher"}, {"X": -5.181403636932373, "Y": 16.91637420654297, "TOKEN": "Peggy Ashcroft"}, {"X": 1.953492522239685, "Y": 1.8476524353027344, "TOKEN": "Alex Haley"}, {"X": -14.235946655273438, "Y": 17.534887313842773, "TOKEN": "John Cage"}, {"X": 9.157862663269043, "Y": -22.620685577392578, "TOKEN": "Menachem Begin"}, {"X": -6.655925750732422, "Y": 15.450852394104004, "TOKEN": "Shirley Booth"}, {"X": 6.403168678283691, "Y": 4.0633955001831055, "TOKEN": "Isaac Asimov"}, {"X": 2.9871246814727783, "Y": -0.25527191162109375, "TOKEN": "William Shawn"}, {"X": 3.1391007900238037, "Y": -7.799642562866211, "TOKEN": "Marsha P Johnson"}, {"X": 3.6216492652893066, "Y": -20.73087501525879, "TOKEN": "Thurgood Marshall"}, {"X": -0.025618620216846466, "Y": 14.099115371704102, "TOKEN": "Federico Fellini"}, {"X": 1.7541379928588867, "Y": -13.795405387878418, "TOKEN": "Cesar Chavez"}, {"X": -14.002293586730957, "Y": 19.273035049438477, "TOKEN": "Carlos Montoya"}, {"X": -12.635844230651855, "Y": 21.199996948242188, "TOKEN": "Dizzy Gillespie"}, {"X": -11.325723648071289, "Y": 4.3122029304504395, "TOKEN": "Arthur Ashe"}, {"X": 4.702512741088867, "Y": 5.378389358520508, "TOKEN": "William Golding"}, {"X": 11.471202850341797, "Y": -5.67047643661499, "TOKEN": "Albert Sabin"}, {"X": 0.7590458989143372, "Y": -18.519386291503906, "TOKEN": "Thomas P O Neill Jr"}, {"X": 1.6892129182815552, "Y": -17.274255752563477, "TOKEN": "Richard Nixon"}, {"X": 11.3215913772583, "Y": 1.8216707706451416, "TOKEN": "Erik Erikson"}, {"X": 12.909110069274902, "Y": -1.4237960577011108, "TOKEN": "Linus C Pauling"}, {"X": -5.725397109985352, "Y": 14.565417289733887, "TOKEN": "Jessica Tandy"}, {"X": 9.70884895324707, "Y": -8.380881309509277, "TOKEN": "Jan Tinbergen"}, {"X": 1.5300692319869995, "Y": -0.6255696415901184, "TOKEN": "Jacqueline Kennedy"}, {"X": 11.458001136779785, "Y": -5.619353771209717, "TOKEN": "Jonas Salk"}, {"X": 3.310856819152832, "Y": 12.234801292419434, "TOKEN": "Alfred Eisenstaedt"}, {"X": -7.527838230133057, "Y": 15.655458450317383, "TOKEN": "Ginger Rogers"}, {"X": -6.486595630645752, "Y": 14.33803939819336, "TOKEN": "George Abbott"}, {"X": 9.317960739135742, "Y": -22.759197235107422, "TOKEN": "Yitzhak Rabin"}, {"X": 10.1116361618042, "Y": -1.4016354084014893, "TOKEN": "Carl Sagan"}, {"X": 4.682015895843506, "Y": -0.24707086384296417, "TOKEN": "Timothy Leary"}, {"X": -7.98546838760376, "Y": 16.594234466552734, "TOKEN": "Gene Kelly"}, {"X": 4.855006217956543, "Y": 3.3466439247131348, "TOKEN": "Allen Ginsberg"}, {"X": 13.162900924682617, "Y": -17.003646850585938, "TOKEN": "Deng Xiaoping"}, {"X": -3.966529369354248, "Y": 13.405994415283203, "TOKEN": "James Stewart"}, {"X": -2.5082755088806152, "Y": 11.209596633911133, "TOKEN": "Bob Kane"}, {"X": 6.970714569091797, "Y": 0.5519394278526306, "TOKEN": "Benjamin Spock"}, {"X": -11.216814994812012, "Y": 5.593843460083008, "TOKEN": "Helen Moody"}, {"X": -5.174684524536133, "Y": 15.951761245727539, "TOKEN": "Maureen O Sullivan"}, {"X": 9.226228713989258, "Y": -7.758716106414795, "TOKEN": "Theodore Schultz"}, {"X": -6.292104244232178, "Y": -12.628838539123535, "TOKEN": "Alan B Shepard Jr"}, {"X": -11.844756126403809, "Y": 13.566322326660156, "TOKEN": "Galina Ulanova"}, {"X": -0.42799046635627747, "Y": -19.266468048095703, "TOKEN": "Bella Abzug"}, {"X": 1.4201784133911133, "Y": -3.4274697303771973, "TOKEN": "Fred W Friendly"}, {"X": -9.123132705688477, "Y": 18.30845832824707, "TOKEN": "Frank Sinatra"}, {"X": 10.336825370788574, "Y": -21.98391342163086, "TOKEN": "Hassan II"}, {"X": 5.405998706817627, "Y": 6.564864635467529, "TOKEN": "Iris Murdoch"}, {"X": 10.242327690124512, "Y": -22.4055118560791, "TOKEN": "King Hussein"}, {"X": 9.397595405578613, "Y": -15.240979194641113, "TOKEN": "Pierre Trudeau"}, {"X": 0.28496065735816956, "Y": -16.12995719909668, "TOKEN": "Elliot Richardson"}, {"X": -1.7634185552597046, "Y": 9.884428977966309, "TOKEN": "Charles M Schulz"}, {"X": 9.921907424926758, "Y": 0.5387915968894958, "TOKEN": "Karen Sparck Jones"}]}}, {"mode": "vega-lite"});
</script></div></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "nlp"
        },
        kernelOptions: {
            kernelName: "nlp",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'nlp'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="03_classification-and-feature-engineering.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">3. </span>Classification and Feature Engineering</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Tyler Shoemaker and Carl Stahmer<br/>
        
          <div class="extra_footer">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">
  <img alt="CC BY-SA 4.0" src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg"> 
</a>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>