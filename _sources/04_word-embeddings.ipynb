{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ae298c",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "===============\n",
    "\n",
    "```{admonition} Learning objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "+ Explain what word embeddings are\n",
    "+ Use `gensim` to train and load word embeddings models\n",
    "+ Identify and analyze word relationships in these models\n",
    "+ Recognize how bias can inhere in embeddings\n",
    "+ Encode documents with a word embeddings model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ff353",
   "metadata": {},
   "source": [
    "Loading the Data\n",
    "--------------------\n",
    "\n",
    "Before we begin working with word embeddings in full, let's load a corpus manifest file, which will help us keep track of all the obituaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff68e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manifest = pd.read_csv('data/session_three/manifest.csv', index_col=0)\n",
    "manifest = manifest.assign(YEAR = pd.to_datetime(manifest['YEAR'], format='%Y').dt.year)\n",
    "\n",
    "manifest.groupby('YEAR').count().plot(\n",
    "    figsize=(15, 5),\n",
    "    y='NAME',\n",
    "    title='Obituaries per Year',\n",
    "    ylabel='Num. obituaries',\n",
    "    xlabel='Year',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7777",
   "metadata": {},
   "source": [
    "Here's a sampling of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18acb156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rex Harrison (1990)\n",
      "Andrew Johnson (1875)\n",
      "Conan Doyle (1930)\n",
      "Elizabeth Cady Stanton (1902)\n",
      "Bob Kane (1998)\n"
     ]
    }
   ],
   "source": [
    "for idx in manifest.sample(5).index:\n",
    "    name, date = manifest.loc[idx, 'NAME'], manifest.loc[idx, 'YEAR']\n",
    "    print(f\"{name} ({date})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bd292",
   "metadata": {},
   "source": [
    "Now we can load the obituaries themselves. While the past two sessions have required full-text representations of documents, word embeddings work best with bags of words, especially when it comes to doing analysis with them. Accordingly, each of the files in the corpus have already processed by a text cleaning pipeline: they represent the lowercase, stopped, and lemmatized versions of the originals.\n",
    "\n",
    "No extra loading considerations are needed here either. We'll just use `glob` to get our file paths and iterate through the list, loading each document into a `corpus` list. Note that we still must split the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2575e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob('data/session_three/obits/*.txt')\n",
    "paths.sort()\n",
    "\n",
    "corpus = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as fin:\n",
    "        doc = fin.read()\n",
    "        doc = doc.split()\n",
    "        corpus.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d5031",
   "metadata": {},
   "source": [
    "With this done, we can move on to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01442f",
   "metadata": {},
   "source": [
    "Using an Embeddings Model\n",
    "----------------------------------\n",
    "\n",
    "At this point, we are at a crossroads. On the one hand, we could train a word embeddings model using our corpus documents as is. The `gensim` library offers functionality for this, and it's a relatively easy operation. On the other, we could use premade embeddings, which are usually trained on a more general – and much larger – set of documents. There is a tradeoff here:\n",
    "\n",
    "+ Training a corpus-specific model will more faithfully represent the token behavior of the texts we'd like to analyze, but these representations could be _too_ specific, especially if the model doesn't have enough data to train on; the resultant embeddings may be closer to topic models than to word-level semantics\n",
    "+ Using premade embeddings gives us the benefit of generalization: the vectors will cleave more closely to how we understand language; but such embeddings might a) miss out on certain nuances we'd like to capture, or b) introduce biases into our corpus (more on this below)\n",
    "\n",
    "In our case, the decision is difficult. When preparing this reader, we (Tyler and Carl) found that a model trained on the obituaries alone did not produce vectors that could fully demonstrate the capabilities of the word embedding technique. The corpus is just a little too specific, and perhaps a little too small. We could've used a larger corpus, but doing so would introduce slow-downs in the workshop session. Because of this, we decided to use a premade model, in this case, the Stanford [GloVe] embeddings (the 200-dimension version). GloVe was trained on billions of tokens, spanning Wikipedia data, newswire articles, even Twitter. More, the model's developers offer several different dimension sizes, which are helpful for selecting embeddings with the right amount of detail.\n",
    "\n",
    "That said, going with GloVe introduces its own problems. For one thing, we can't show you how to train a word embeddings model itself – at least not live. The code to do so, however, is reproduced below:\n",
    "\n",
    "```{margin} Model parameters\n",
    "There are many different parameters to select from in `gensim`. You can find them in the [Word2Vec documentation].\n",
    "\n",
    "[Word2Vec documentation]: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "```\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "n_dimensions = 100\n",
    "model = Word2Vec(n_dimensions)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_words=model.corpus_total_words, epochs=5)\n",
    "```\n",
    "\n",
    "Another problem has to do with the data GloVe was trained on. It's so large that we can't account for all the content, and this becomes particularly detrimental when it comes to bias. [Researchers have found] that general embeddings models reproduce gender-discriminatory language, even hate speech, by virtue of the fact that they are trained on huge amounts of text data, often without consideration of whether the content of such data is something one would endorse. GloVe is [known to be biased] in this way. We'll show an example later on in this chapter and will discuss this in much more detail during our live session, but for now just note that the effects of bias _do_ shape how we represent our corpus, and it's important to keep an eye out for this when working with the data.\n",
    "\n",
    "[GloVe]: https://nlp.stanford.edu/projects/glove/\n",
    "[Researchers have found]: https://www.technologyreview.com/2016/07/27/158634/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/\n",
    "[known to be biased]: http://arxiv.org/abs/1607.06520\n",
    "\n",
    "### Loading a model\n",
    "\n",
    "With all that said, we can move on. Below, we load GloVe embeddings into our workspace using a `gensim` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa49bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load('data/session_three/glove/glove-wiki-gigaword_200d.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e8a26",
   "metadata": {},
   "source": [
    "The `KeyedVectors` object acts almost like a dictionary. You can do certain Python operations directly on it, like using `len()` to find the number of tokens in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a4669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the model: 400,000\n"
     ]
    }
   ],
   "source": [
    "n_tokens = len(model)\n",
    "\n",
    "print(f\"Number of unique tokens in the model: {n_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042060a1",
   "metadata": {},
   "source": [
    "### Token mappings\n",
    "\n",
    "Each token in the model (what `gensim` calls a \"key\") has an associated index. This mapping is accessible via the `.key_to_index` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82fb46d",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843d4e5",
   "metadata": {},
   "source": [
    "If you want to get the vector representation for a token, you can use either the key or the index. The syntax is just like a Python dictionary. Below, we randomly select a single token from the model vocabulary's `.index_to_key` attribute and find the index associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b78a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index position for 'nesquik' is 207398\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_token = random.choice(model.index_to_key)\n",
    "rand_idx = model.key_to_index[rand_token]\n",
    "\n",
    "print(f\"The index position for '{rand_token}' is {rand_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3faa4d",
   "metadata": {},
   "source": [
    "Here's its vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29dd6a22",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.0610e-01, -1.6150e-01,  6.7672e-02,  4.5806e-01,  3.7816e-01,\n",
       "       -3.4497e-01,  6.4789e-01, -3.5420e-01,  1.3809e-02, -2.1930e-01,\n",
       "       -1.1200e-01,  1.1698e-01, -1.0956e-01,  2.4167e-01, -6.9266e-01,\n",
       "       -1.2957e-01,  3.1281e-01, -4.0355e-01,  1.7780e-01,  4.3801e-02,\n",
       "       -5.0048e-01, -6.2828e-01, -1.8032e-02, -8.4619e-01,  4.5639e-02,\n",
       "       -7.9362e-01,  3.9530e-01,  2.1440e-01, -7.6506e-02,  2.5765e-02,\n",
       "        3.4960e-02,  2.8049e-01,  3.9899e-02, -4.0625e-01,  1.2862e-01,\n",
       "       -4.9737e-01,  5.7278e-01,  2.7273e-01, -5.1922e-01,  8.8304e-03,\n",
       "       -1.0688e-02,  4.1766e-01, -3.3940e-01, -2.3985e-01, -1.4701e-01,\n",
       "       -5.6929e-01, -8.4959e-02, -5.9651e-01, -4.6448e-01,  1.9664e-01,\n",
       "        4.0700e-01, -2.0722e-01,  3.8220e-01,  6.0006e-02,  3.2848e-01,\n",
       "        2.6361e-01, -3.2718e-01, -2.8027e-01, -4.5250e-01, -2.8542e-01,\n",
       "        1.5290e-01,  1.0701e-01,  1.6510e-02, -3.2764e-01, -2.4728e-01,\n",
       "        2.5474e-01,  2.2040e-02,  1.3824e-01, -7.3297e-03,  3.7716e-02,\n",
       "        1.9997e-01,  5.3001e-02, -7.7438e-02, -2.4531e-01, -2.4104e-01,\n",
       "       -1.5386e-01, -6.3226e-01, -5.1501e-01, -4.7569e-01, -3.2359e-01,\n",
       "        4.9285e-02, -4.1933e-02,  2.7442e-01, -1.8621e-04, -2.5816e-02,\n",
       "        1.5963e-01,  2.5998e-01,  1.0331e-01, -6.3914e-01,  2.0063e-01,\n",
       "        2.5576e-01, -2.6422e-02, -3.6283e-01, -1.9804e-01,  8.2567e-02,\n",
       "       -9.1819e-01,  4.1386e-01,  2.1053e-01,  2.4172e-01, -4.9141e-01,\n",
       "        2.9092e-01,  3.8787e-01,  5.7943e-01,  2.6107e-01, -8.6667e-01,\n",
       "        3.8025e-01, -1.7014e-01, -7.5003e-01,  1.2931e-01, -6.9060e-01,\n",
       "       -2.1897e-01, -2.2024e-01,  4.7638e-01, -4.9525e-01,  3.3701e-01,\n",
       "       -1.2362e-01, -1.6957e-01, -4.6845e-01,  1.0159e-01, -2.3394e-01,\n",
       "       -3.1774e-01,  7.3502e-02,  3.3695e-01, -1.0748e-01,  7.2277e-01,\n",
       "        1.4155e-01, -3.2766e-02, -4.8237e-01, -5.9079e-01,  1.5535e-01,\n",
       "       -2.5256e-01, -5.9004e-01, -4.9948e-01,  6.4945e-01, -1.2238e-01,\n",
       "       -3.1796e-01, -1.4405e-01, -1.0018e-01,  2.0776e-01,  1.2049e-01,\n",
       "       -5.4959e-02, -5.1706e-01,  3.9959e-01,  1.2565e+00, -6.0234e-01,\n",
       "       -2.6708e-01,  6.3882e-01, -2.9647e-01,  3.2970e-01, -5.2240e-02,\n",
       "        2.7940e-01, -5.2595e-01, -3.0080e-01, -2.9363e-02,  5.7757e-02,\n",
       "        4.6999e-01,  5.3825e-01, -8.7407e-02,  2.5364e-01,  7.8902e-01,\n",
       "        2.1472e-01, -5.9671e-03, -1.1939e-02,  9.3282e-02, -1.5330e-01,\n",
       "        3.4210e-01, -4.9910e-02, -7.0686e-01,  2.2446e-01, -2.2813e-01,\n",
       "        6.2007e-02, -2.4104e-01, -6.3527e-01, -3.9770e-01, -6.3840e-02,\n",
       "        1.2667e-01,  7.6520e-01,  4.1395e-01, -1.7434e-01,  2.1440e-01,\n",
       "       -2.5204e-01,  4.6477e-01, -2.0104e-01, -3.8386e-02, -3.7666e-01,\n",
       "       -1.2487e-01, -4.4665e-01,  1.0785e-01,  7.9434e-02, -2.3342e-01,\n",
       "        2.3012e-01, -5.9022e-01, -3.2039e-01,  4.4667e-01,  7.2143e-01,\n",
       "       -1.9584e-02,  2.9935e-01, -4.6782e-01,  5.2440e-01,  4.2800e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3732b60",
   "metadata": {},
   "source": [
    "And here we show that accessing this vector with either the index or key produces the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b87abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array_equal(model[rand_idx], model[rand_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9c9b7",
   "metadata": {},
   "source": [
    "Finally, we can store the entire model vocabulary in a `set` and show a few examples of the tokens therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50e56cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuse\n",
      "srinath\n",
      "unconcern\n",
      "lavilla\n",
      "navin\n",
      "composers\n",
      "enormity\n",
      "govert\n",
      "liquidmetal\n",
      "59.13\n"
     ]
    }
   ],
   "source": [
    "model_vocab = set(model.index_to_key)\n",
    "\n",
    "for token in random.sample(model_vocab, 10):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4de567",
   "metadata": {},
   "source": [
    "You may find some unexpected tokens in this output. Though it has been ostensibly trained on an English corpus, GloVe contains multilingual text. It also contains lots of noisy tokens, which range from erroneous segmentations (\"drummer/percussionist\" is one token, for example) to password-like strings and even HTML markup. Depending on your task, you may not notice these tokens, but they do in fact influence the overall shape of the model, and sometimes you'll find them cropping up when you're hunting around for similar terms and the like (more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae401f",
   "metadata": {},
   "source": [
    "### Out-of-vocabulary tokens\n",
    "\n",
    "While GloVe's vocabulary sometimes seems _too_ expansive, there are other instances where it's too restricted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4abca184",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not in vocabulary!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_1266/744621433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'unshaped'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not in vocabulary!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Not in vocabulary!"
     ]
    }
   ],
   "source": [
    "assert 'unshaped' in model, \"Not in vocabulary!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c8da8",
   "metadata": {},
   "source": [
    "If the model wasn't trained on a particular word, it won't have a corresponding vector for that word either. This is crucial. Because models like GloVe only know what they've been trained on, you need to be aware of any potential discrepancies between their vocabularies and your corpus data. If you don't keep this in mind, sending unseen, or **out-of-vocabulary**, tokens to GloVe will throw errors in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9210b8a9",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'unshaped' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_1266/3553718512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unshaped'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'unshaped' not present\""
     ]
    }
   ],
   "source": [
    "model['unshaped']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd017ccd",
   "metadata": {},
   "source": [
    "There are a few ways to handle this problem. The most common is to simply _not encode_ tokens in your corpus that don't have a corresponding vector in GloVe. Below, we construct three dictionaries for our corpus data. The first contains all tokens, while the second and third are comprised of tokens that are and are not in Glove, respectively. We identify whether the model has a token using its `.has_index_for()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92e2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the corpus vocabulary: 29,330 \n",
      "Number of corpus words in GloVe: 27,488 \n",
      "Number of corpus words not in GloVe: 1,842\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab = set(token for doc in corpus for token in doc)\n",
    "in_glove = set(token for token in corpus_vocab if model.has_index_for(token))\n",
    "no_glove = set(token for token in corpus_vocab if model.has_index_for(token) == False)\n",
    "\n",
    "print(\n",
    "    f\"Total words in the corpus vocabulary: {len(corpus_vocab):,}\",\n",
    "    f\"\\nNumber of corpus words in GloVe: {len(in_glove):,}\",\n",
    "    f\"\\nNumber of corpus words not in GloVe: {len(no_glove):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac92b5f",
   "metadata": {},
   "source": [
    "Any subsequent code we write will need to reference these dictionaries to determine whether it should encode a token.\n",
    "\n",
    "While this is what we'll indeed do below, obviously it isn't an ideal situation. But it's one of the consequences of using premade models. There are, however, a few other ways to handle out-of-vocabulary terms. Some models offer special \"UNK\" tokens, which you could associate with all of your problem tokens. This, at the very least, enables you to have _some_ representation of your data. A more complex approach involves taking the mean embedding of the word vectors surrounding an unknown token; and depending on the model, you can also train it further, adding extra tokens from your domain-specific text. Instructions for this last option are available [here] in the `gensim` documentation.\n",
    "\n",
    "[here]: https://radimrehurek.com/gensim/models/word2vec.html#usage-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268817e9",
   "metadata": {},
   "source": [
    "Word relationships\n",
    "----------------------\n",
    "\n",
    "Later on we'll use GloVe to encode our corpus texts. But before we do, it's worth demonstrating more generally some of the properties of word vectors. Vector representations of text allow us to perform various mathematical operations on our corpus that approximate (though maybe _only_ approximate) semantics. The most common among these operations is finding the **cosine similarity** between two vectors. Our Getting Started with Textual Data series has a whole [chapter] on this measure, so if you haven't encountered it before, we recommend you read that. But in short: cosine similarity measures the difference between vectors' orientation in a feature space (here, the feature space is comprised of each of the vectors' 200 dimentions). The closer two vectors are, the more likely they are to share semantic similarities.\n",
    "\n",
    "[chapter]: https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/05_clustering-and-classification.html#\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "`gensim` provides easy access to this measure and other such vector space operations, and we can use this functionality to explore relationships between words in a model. To find the cosine similarity between the vectors for two words in GloVe, simply use the model's `.similarity()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c30643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for 'calculate' and 'compute': 0.6991\n"
     ]
    }
   ],
   "source": [
    "a, b = 'calculate', 'compute'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for '{a}' and '{b}': {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bc5ae",
   "metadata": {},
   "source": [
    "The only difference between the score above and the one that you might produce, say, with `scikit-learn`'s cosine similarity implementation is that `gensim` bounds its values from `[-1,1]`, whereas the latter uses a `[0,1]` scale. While in `gensim` it's still the case that similar words score closer to `1`, highly dissimilar words will be closer to `-1`.\n",
    "\n",
    "At any rate, we can get the top _n_ most similar words for a word using `.most_similar()`. The function defaults to 10 entries, but you can change that with the `topn` paramter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412f067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'proofs':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theorems</td>\n",
       "      <td>0.638435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zero-knowledge</td>\n",
       "      <td>0.616407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>refutations</td>\n",
       "      <td>0.593198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mathematical</td>\n",
       "      <td>0.553695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkable</td>\n",
       "      <td>0.540866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>completeness</td>\n",
       "      <td>0.532250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>probabilistically</td>\n",
       "      <td>0.530754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conjectures</td>\n",
       "      <td>0.525487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>refutation</td>\n",
       "      <td>0.516713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>non-constructive</td>\n",
       "      <td>0.516433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                WORD     SCORE\n",
       "0           theorems  0.638435\n",
       "1     zero-knowledge  0.616407\n",
       "2        refutations  0.593198\n",
       "3       mathematical  0.553695\n",
       "4          checkable  0.540866\n",
       "5       completeness  0.532250\n",
       "6  probabilistically  0.530754\n",
       "7        conjectures  0.525487\n",
       "8         refutation  0.516713\n",
       "9   non-constructive  0.516433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'bonfire':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aggie</td>\n",
       "      <td>0.578455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bonfires</td>\n",
       "      <td>0.529283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanities</td>\n",
       "      <td>0.487215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candle</td>\n",
       "      <td>0.387145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lit</td>\n",
       "      <td>0.379182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>barbecues</td>\n",
       "      <td>0.377428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>homecoming</td>\n",
       "      <td>0.377209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fireworks</td>\n",
       "      <td>0.371795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pyre</td>\n",
       "      <td>0.369518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stonehenge</td>\n",
       "      <td>0.363889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         WORD     SCORE\n",
       "0       aggie  0.578455\n",
       "1    bonfires  0.529283\n",
       "2    vanities  0.487215\n",
       "3      candle  0.387145\n",
       "4         lit  0.379182\n",
       "5   barbecues  0.377428\n",
       "6  homecoming  0.377209\n",
       "7   fireworks  0.371795\n",
       "8        pyre  0.369518\n",
       "9  stonehenge  0.363889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'merion':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ardmore</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>montoursville</td>\n",
       "      <td>0.502387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wellsboro</td>\n",
       "      <td>0.496028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>northview</td>\n",
       "      <td>0.489293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haverford</td>\n",
       "      <td>0.485248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gladwyne</td>\n",
       "      <td>0.484112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hollidaysburg</td>\n",
       "      <td>0.472328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doylestown</td>\n",
       "      <td>0.468527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saucon</td>\n",
       "      <td>0.464676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>allentown</td>\n",
       "      <td>0.461663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WORD     SCORE\n",
       "0        ardmore  0.523000\n",
       "1  montoursville  0.502387\n",
       "2      wellsboro  0.496028\n",
       "3      northview  0.489293\n",
       "4      haverford  0.485248\n",
       "5       gladwyne  0.484112\n",
       "6  hollidaysburg  0.472328\n",
       "7     doylestown  0.468527\n",
       "8         saucon  0.464676\n",
       "9      allentown  0.461663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'federals':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confederates</td>\n",
       "      <td>0.669776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hessians</td>\n",
       "      <td>0.565406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cavalrymen</td>\n",
       "      <td>0.536918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entrenchments</td>\n",
       "      <td>0.518521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hernici</td>\n",
       "      <td>0.473023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>counterattacked</td>\n",
       "      <td>0.473019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>retreating</td>\n",
       "      <td>0.472633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>villistas</td>\n",
       "      <td>0.466564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>argives</td>\n",
       "      <td>0.463875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thebans</td>\n",
       "      <td>0.461612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              WORD     SCORE\n",
       "0     confederates  0.669776\n",
       "1         hessians  0.565406\n",
       "2       cavalrymen  0.536918\n",
       "3    entrenchments  0.518521\n",
       "4          hernici  0.473023\n",
       "5  counterattacked  0.473019\n",
       "6       retreating  0.472633\n",
       "7        villistas  0.466564\n",
       "8          argives  0.463875\n",
       "9          thebans  0.461612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'aranha':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>graça</td>\n",
       "      <td>0.499553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kirsh</td>\n",
       "      <td>0.480975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d'antin</td>\n",
       "      <td>0.475294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>felipa</td>\n",
       "      <td>0.471845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chrzanowski</td>\n",
       "      <td>0.467236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>maurico</td>\n",
       "      <td>0.462281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toppin</td>\n",
       "      <td>0.461796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>klussmann</td>\n",
       "      <td>0.458761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jansma</td>\n",
       "      <td>0.457442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pasca</td>\n",
       "      <td>0.455031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0        graça  0.499553\n",
       "1        kirsh  0.480975\n",
       "2      d'antin  0.475294\n",
       "3       felipa  0.471845\n",
       "4  chrzanowski  0.467236\n",
       "5      maurico  0.462281\n",
       "6       toppin  0.461796\n",
       "7    klussmann  0.458761\n",
       "8       jansma  0.457442\n",
       "9        pasca  0.455031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = random.sample(in_glove, 5)\n",
    "\n",
    "for token in targets:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3983f5",
   "metadata": {},
   "source": [
    "We can also find the _least_ similar word. This is useful to show, because it pressures our idea of what counts as similarity. Mathematical similarity does not always align with concepts like synonyms and antonyms. For example, it's probably safe to say that the semantic opposite of \"good\" – that is, its antonym – is \"evil.\" But in the world of vector spaces, the least similar word to \"good\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ddb5788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cw96', -0.6553234457969666)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good', topn=len(model))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afacde",
   "metadata": {},
   "source": [
    "Just noise! Relatively speaking, the vectors for \"good\" and \"evil\" are actually quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfd03710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consine similarity score for good and evil: 0.3378\n"
     ]
    }
   ],
   "source": [
    "a, b = 'good', 'evil'\n",
    "sim = model.similarity(a, b)\n",
    "\n",
    "print(f\"Consine similarity score for {a} and {b}: {sim:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ef20e",
   "metadata": {},
   "source": [
    "How do we make sense of this? Well, it has to do with the way the word embeddings are created. Since embeddings models are ultimately trained on co-occurrence data, words that tend to appear in similar kinds of contexts will be more similar in a mathematical sense than those that don't.\n",
    "\n",
    "Keeping this in mind is also important for considerations of bias. Since, in one sense, _embeddings reflect the interchangeability between tokens_, they will reinforce negative, even harmful patterns in the data (which is to say in culture at large). For example, consider the most similar words for \"doctor\" and \"nurse.\" The latter is locked up within gendered language: according to GloVe, a nurse is like a midwife is like a mother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831246c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'doctor':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.736021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.672406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surgeon</td>\n",
       "      <td>0.655147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dr.</td>\n",
       "      <td>0.652498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.648189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.636380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.619159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.584747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>psychiatrist</td>\n",
       "      <td>0.568571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD     SCORE\n",
       "0     physician  0.736021\n",
       "1       doctors  0.672406\n",
       "2       surgeon  0.655147\n",
       "3           dr.  0.652498\n",
       "4         nurse  0.651449\n",
       "5       medical  0.648189\n",
       "6      hospital  0.636380\n",
       "7       patient  0.619159\n",
       "8       dentist  0.584747\n",
       "9  psychiatrist  0.568571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens most similar to 'nurse':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurses</td>\n",
       "      <td>0.714051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.651449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.626937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>midwife</td>\n",
       "      <td>0.614592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anesthetist</td>\n",
       "      <td>0.610603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.610359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.609222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.586503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>therapist</td>\n",
       "      <td>0.580488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.573556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          WORD     SCORE\n",
       "0       nurses  0.714051\n",
       "1       doctor  0.651449\n",
       "2      nursing  0.626937\n",
       "3      midwife  0.614592\n",
       "4  anesthetist  0.610603\n",
       "5    physician  0.610359\n",
       "6     hospital  0.609222\n",
       "7       mother  0.586503\n",
       "8    therapist  0.580488\n",
       "9      dentist  0.573556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bias_example = ['doctor', 'nurse']\n",
    "\n",
    "for token in bias_example:\n",
    "    similarities = model.most_similar(token)\n",
    "    print(f\"Tokens most similar to '{token}':\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f765b0",
   "metadata": {},
   "source": [
    "### Visualizing the vector space\n",
    "\n",
    "One way to start getting a feel for all this is to visualize the word vectors. We do so below by sampling a portion of the GloVe vectors and then reducing them into two-dimensional data, which we can plot. First, let's build two functions.\n",
    "\n",
    "```{margin} How we create the visualization data\n",
    "`sample_embeddings()` takes a sample from GloVe:\n",
    "\n",
    "1. First it randomly selects indices in the model\n",
    "2. Then it uses these to subset the vectors\n",
    "3. Finally it associates the tokens with their respective indices to produce a set of labels\n",
    "\n",
    "`prepare_vis_data()` takes the sampled vectors and their labels and reduces them with a t-SNE embedder\n",
    "\n",
    "1. The `TSNE()` portion of the code does the work of reducing our 200-dimension vectors into only two dimensions\n",
    "2. Then the function converts the two-dimensional data into a dataframe and associates the labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e575b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def sample_embeddings(vectors, samp=1000):\n",
    "    n_vectors = vectors.shape[0]\n",
    "    mask = random.sample(range(n_vectors), samp)\n",
    "    vectors = vectors[mask]\n",
    "    vocab = [model.index_to_key[idx] for idx in mask]\n",
    "    \n",
    "    return vectors, vocab\n",
    "\n",
    "def prepare_vis_data(vectors, labels):\n",
    "    reduced = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate='auto',\n",
    "        init='random',\n",
    "        angle=0.65,\n",
    "        random_state=357\n",
    "    ).fit_transform(vectors)\n",
    "    \n",
    "    vis_data = pd.DataFrame(reduced, columns=['X', 'Y'])\n",
    "    vis_data['LABEL'] = labels\n",
    "    \n",
    "    return vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a867847",
   "metadata": {},
   "source": [
    "Now we can retrieve all the vectors from GloVe using the `.key_to_index` attribute. With those stored in a `numpy` array, it's time to sample them and create the visualization data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590ae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.array([model[idx] for idx in model.key_to_index])\n",
    "\n",
    "sampled, sampled_vocab = sample_embeddings(all_vectors)\n",
    "vis_data = prepare_vis_data(sampled, sampled_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f25012",
   "metadata": {},
   "source": [
    "```{margin} This visualization is partial\n",
    "Since we've run a t-SNE reduction on the sampled embeddings, the graph layout only takes into account the relative differences between that sampling, not all of GloVe. Going the latter route would take a while to compute – there would be 400,000 embeddings to reduce!\n",
    "```\n",
    "\n",
    "With the reduced embeddings made, it's time to plot them. Have a look around at the results. What seems right to you? What surprises you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18195af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-93501f8a7811441f9e213da168b740fa\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-93501f8a7811441f9e213da168b740fa\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-93501f8a7811441f9e213da168b740fa\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3d7c5918e33779de31120c471ad148ad\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"LABEL\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-3d7c5918e33779de31120c471ad148ad\": [{\"X\": -4.057456970214844, \"Y\": -7.146651268005371, \"LABEL\": \"footbridge\"}, {\"X\": 5.267815589904785, \"Y\": 4.705569267272949, \"LABEL\": \"shaktar\"}, {\"X\": -0.36230579018592834, \"Y\": -1.8950012922286987, \"LABEL\": \"cgs\"}, {\"X\": 1.4207656383514404, \"Y\": 2.7683281898498535, \"LABEL\": \"d'amours\"}, {\"X\": -1.442946434020996, \"Y\": -1.2026314735412598, \"LABEL\": \"mediating\"}, {\"X\": 0.7062615752220154, \"Y\": 1.1183300018310547, \"LABEL\": \"thoris\"}, {\"X\": 1.5705585479736328, \"Y\": 2.8315114974975586, \"LABEL\": \"senechal\"}, {\"X\": -0.901840090751648, \"Y\": 0.46414390206336975, \"LABEL\": \"richardsons\"}, {\"X\": 1.0695676803588867, \"Y\": -3.40129017829895, \"LABEL\": \"102nd\"}, {\"X\": -3.0399274826049805, \"Y\": 2.1826510429382324, \"LABEL\": \"ehud\"}, {\"X\": 3.43440318107605, \"Y\": -1.500963568687439, \"LABEL\": \"wattenscheid\"}, {\"X\": -7.593599796295166, \"Y\": -1.1923550367355347, \"LABEL\": \".293\"}, {\"X\": 2.1604771614074707, \"Y\": 2.46927547454834, \"LABEL\": \"corea\"}, {\"X\": 0.4016643464565277, \"Y\": 5.377397060394287, \"LABEL\": \"subuh\"}, {\"X\": -0.7594908475875854, \"Y\": 0.19454576075077057, \"LABEL\": \"emusic.com\"}, {\"X\": 1.641020655632019, \"Y\": 3.343855619430542, \"LABEL\": \"sittig\"}, {\"X\": 1.2844395637512207, \"Y\": 2.871044635772705, \"LABEL\": \"harpham\"}, {\"X\": -0.5998409390449524, \"Y\": -7.343282222747803, \"LABEL\": \"pursuits\"}, {\"X\": 3.590635061264038, \"Y\": -1.2777026891708374, \"LABEL\": \"shutesbury\"}, {\"X\": 1.200561761856079, \"Y\": 2.2685482501983643, \"LABEL\": \"manager/producer\"}, {\"X\": 2.2776565551757812, \"Y\": 1.7591161727905273, \"LABEL\": \"sauza\"}, {\"X\": 3.264490842819214, \"Y\": 0.08231320977210999, \"LABEL\": \"letran\"}, {\"X\": 2.387152910232544, \"Y\": -0.2697209417819977, \"LABEL\": \"v\\u00e5r\"}, {\"X\": -3.2820327281951904, \"Y\": -0.5071150660514832, \"LABEL\": \"39-day\"}, {\"X\": -2.9432013034820557, \"Y\": 1.124199390411377, \"LABEL\": \"lucid\"}, {\"X\": -0.5443755388259888, \"Y\": 0.9618790149688721, \"LABEL\": \"illyrians\"}, {\"X\": -2.253906726837158, \"Y\": 0.06814831495285034, \"LABEL\": \"prepped\"}, {\"X\": -2.274419069290161, \"Y\": -2.5929043292999268, \"LABEL\": \"unattached\"}, {\"X\": -1.9963001012802124, \"Y\": -5.997495651245117, \"LABEL\": \"finalists\"}, {\"X\": -0.18802785873413086, \"Y\": -2.5146915912628174, \"LABEL\": \"emasculate\"}, {\"X\": -0.028165044263005257, \"Y\": 3.613913059234619, \"LABEL\": \"norgaard\"}, {\"X\": 0.27828875184059143, \"Y\": 3.1388654708862305, \"LABEL\": \"nauert\"}, {\"X\": 6.453524589538574, \"Y\": 2.230036735534668, \"LABEL\": \"http://www.unos.org\"}, {\"X\": -2.1224722862243652, \"Y\": -1.2470121383666992, \"LABEL\": \"26km\"}, {\"X\": -3.011197566986084, \"Y\": -1.0318388938903809, \"LABEL\": \"envisages\"}, {\"X\": 5.645402431488037, \"Y\": 0.03964269161224365, \"LABEL\": \"alvand\"}, {\"X\": 2.0251197814941406, \"Y\": -4.973434925079346, \"LABEL\": \"antbirds\"}, {\"X\": 3.0311479568481445, \"Y\": 6.4259514808654785, \"LABEL\": \"mccallum\"}, {\"X\": 0.6950645446777344, \"Y\": 1.937709093093872, \"LABEL\": \"mishchenko\"}, {\"X\": -1.4809370040893555, \"Y\": -6.801810264587402, \"LABEL\": \"tiger\"}, {\"X\": 4.134557247161865, \"Y\": 0.34474581480026245, \"LABEL\": \"asuncion\"}, {\"X\": 2.9184257984161377, \"Y\": 2.967761993408203, \"LABEL\": \"vianello\"}, {\"X\": 4.7268290519714355, \"Y\": 2.431962013244629, \"LABEL\": \"obc\"}, {\"X\": 4.234256267547607, \"Y\": 1.8092617988586426, \"LABEL\": \"baleka\"}, {\"X\": 0.2184782773256302, \"Y\": 3.265141248703003, \"LABEL\": \"deblanc\"}, {\"X\": -6.221444129943848, \"Y\": -0.20861297845840454, \"LABEL\": \"11-10\"}, {\"X\": -0.8812609314918518, \"Y\": -1.266876459121704, \"LABEL\": \"teen-age\"}, {\"X\": 5.324257850646973, \"Y\": 1.4734406471252441, \"LABEL\": \"pancha\"}, {\"X\": -0.5294208526611328, \"Y\": 4.247335910797119, \"LABEL\": \"dunkle\"}, {\"X\": 0.48651379346847534, \"Y\": -4.841713905334473, \"LABEL\": \"lean-to\"}, {\"X\": 4.330123424530029, \"Y\": -1.1652569770812988, \"LABEL\": \"sinsen\"}, {\"X\": 0.041971806436777115, \"Y\": 1.136008620262146, \"LABEL\": \"siwarak\"}, {\"X\": 0.3479451537132263, \"Y\": 1.8251718282699585, \"LABEL\": \"zelophehad\"}, {\"X\": 2.687248945236206, \"Y\": 0.061179738491773605, \"LABEL\": \"simca\"}, {\"X\": -1.6767910718917847, \"Y\": -6.583217144012451, \"LABEL\": \"dominate\"}, {\"X\": 0.5983561277389526, \"Y\": -2.387281656265259, \"LABEL\": \"monolingual\"}, {\"X\": -3.648942708969116, \"Y\": -3.9206600189208984, \"LABEL\": \"marburg\"}, {\"X\": -0.05739285424351692, \"Y\": -2.022557258605957, \"LABEL\": \"danish-norwegian\"}, {\"X\": 1.7043529748916626, \"Y\": 2.906785726547241, \"LABEL\": \"mordue\"}, {\"X\": -5.075143337249756, \"Y\": -1.644453525543213, \"LABEL\": \"yens\"}, {\"X\": 0.3012763261795044, \"Y\": 2.0685224533081055, \"LABEL\": \"huaiyi\"}, {\"X\": 1.0947548151016235, \"Y\": 0.4780854880809784, \"LABEL\": \"barringtonia\"}, {\"X\": 0.39884063601493835, \"Y\": 4.245619297027588, \"LABEL\": \"kvamme\"}, {\"X\": 1.9933421611785889, \"Y\": -3.1042473316192627, \"LABEL\": \"resistol\"}, {\"X\": 0.001281855395063758, \"Y\": -4.561442852020264, \"LABEL\": \"pip2\"}, {\"X\": 1.9752277135849, \"Y\": -2.0906660556793213, \"LABEL\": \"cibola\"}, {\"X\": -3.309156894683838, \"Y\": 4.320576190948486, \"LABEL\": \"vih\"}, {\"X\": 4.016912937164307, \"Y\": 3.287414789199829, \"LABEL\": \"zta\"}, {\"X\": 2.405322551727295, \"Y\": -0.551611602306366, \"LABEL\": \"bibury\"}, {\"X\": 4.584494590759277, \"Y\": -0.9640920162200928, \"LABEL\": \"s-train\"}, {\"X\": 1.8296866416931152, \"Y\": 3.0768754482269287, \"LABEL\": \"yuel\"}, {\"X\": 1.2502496242523193, \"Y\": -1.5392094850540161, \"LABEL\": \"spinous\"}, {\"X\": -0.8943789005279541, \"Y\": 0.5655716061592102, \"LABEL\": \"ipe\"}, {\"X\": -0.5633987188339233, \"Y\": 2.2412619590759277, \"LABEL\": \"balram\"}, {\"X\": -2.3990859985351562, \"Y\": 1.0372079610824585, \"LABEL\": \"electrocuting\"}, {\"X\": 0.697248637676239, \"Y\": 6.793210506439209, \"LABEL\": \"vygaudas\"}, {\"X\": -1.7133866548538208, \"Y\": -6.406035423278809, \"LABEL\": \"cautioned\"}, {\"X\": 2.6049861907958984, \"Y\": 1.407030463218689, \"LABEL\": \"rusland\"}, {\"X\": -2.0680716037750244, \"Y\": -2.920229434967041, \"LABEL\": \"grounders\"}, {\"X\": -0.4166671931743622, \"Y\": 4.783846378326416, \"LABEL\": \"hoffmann\"}, {\"X\": 2.817403554916382, \"Y\": -3.7412431240081787, \"LABEL\": \"op\"}, {\"X\": -0.6845284104347229, \"Y\": -1.064853310585022, \"LABEL\": \"supersensitive\"}, {\"X\": 0.8896921277046204, \"Y\": 4.13554573059082, \"LABEL\": \"bohnett\"}, {\"X\": -1.0415118932724, \"Y\": 1.8682529926300049, \"LABEL\": \"yong-min\"}, {\"X\": 0.007511958479881287, \"Y\": -2.8939781188964844, \"LABEL\": \"disquisition\"}, {\"X\": -3.9166793823242188, \"Y\": -2.8462374210357666, \"LABEL\": \"2,628\"}, {\"X\": 1.3045594692230225, \"Y\": -4.370767593383789, \"LABEL\": \"s-76\"}, {\"X\": 1.2596306800842285, \"Y\": 3.981947660446167, \"LABEL\": \"sklansky\"}, {\"X\": -5.907724380493164, \"Y\": -0.17584146559238434, \"LABEL\": \"110-103\"}, {\"X\": 3.8812642097473145, \"Y\": -1.09468412399292, \"LABEL\": \"ullensaker\"}, {\"X\": -1.029335379600525, \"Y\": -3.6667239665985107, \"LABEL\": \"immortal\"}, {\"X\": -0.5721047520637512, \"Y\": -5.607710361480713, \"LABEL\": \"lured\"}, {\"X\": -1.9780679941177368, \"Y\": 5.749257564544678, \"LABEL\": \"ib\\u00e1\\u00f1ez\"}, {\"X\": 6.046210765838623, \"Y\": -0.9605357646942139, \"LABEL\": \"velayat\"}, {\"X\": -0.30985984206199646, \"Y\": -2.2652552127838135, \"LABEL\": \"bumbler\"}, {\"X\": 0.9358075261116028, \"Y\": 0.6416646242141724, \"LABEL\": \"devilman\"}, {\"X\": -2.0909035205841064, \"Y\": 2.171335458755493, \"LABEL\": \"meselech\"}, {\"X\": -1.5750627517700195, \"Y\": 2.584534168243408, \"LABEL\": \"karademir\"}, {\"X\": 1.8892371654510498, \"Y\": 6.355302810668945, \"LABEL\": \"montague\"}, {\"X\": 5.756342887878418, \"Y\": -0.8455529808998108, \"LABEL\": \"sabak\"}, {\"X\": 5.031430721282959, \"Y\": -3.387355327606201, \"LABEL\": \"curlew\"}, {\"X\": 0.7463045716285706, \"Y\": 6.947405815124512, \"LABEL\": \"kaliopate\"}, {\"X\": 1.4062609672546387, \"Y\": 1.970537781715393, \"LABEL\": \"takenouchi\"}, {\"X\": 0.37944838404655457, \"Y\": 1.138419508934021, \"LABEL\": \"ro\\u00ed\"}, {\"X\": -4.198785781860352, \"Y\": -2.9763572216033936, \"LABEL\": \"receptions\"}, {\"X\": 0.7304072976112366, \"Y\": -3.940927267074585, \"LABEL\": \"hewing\"}, {\"X\": -3.4587807655334473, \"Y\": 2.1774020195007324, \"LABEL\": \"merv\"}, {\"X\": 2.2174265384674072, \"Y\": 3.6545379161834717, \"LABEL\": \"horner\"}, {\"X\": -0.09034573286771774, \"Y\": -1.978925347328186, \"LABEL\": \"366th\"}, {\"X\": 1.673880696296692, \"Y\": -2.3192026615142822, \"LABEL\": \"wjc\"}, {\"X\": 2.672041416168213, \"Y\": 1.7264018058776855, \"LABEL\": \"peignot\"}, {\"X\": -0.6308764815330505, \"Y\": 2.560429811477661, \"LABEL\": \"ocko\"}, {\"X\": -0.8526181578636169, \"Y\": 3.808871269226074, \"LABEL\": \"farwell\"}, {\"X\": 1.8003416061401367, \"Y\": 6.576667308807373, \"LABEL\": \"katzman\"}, {\"X\": 3.021326780319214, \"Y\": -0.4376433789730072, \"LABEL\": \"raincy\"}, {\"X\": 2.455650806427002, \"Y\": 4.46368408203125, \"LABEL\": \"khassan\"}, {\"X\": 4.967187881469727, \"Y\": 1.1704893112182617, \"LABEL\": \"regionalligas\"}, {\"X\": 2.375650644302368, \"Y\": 3.2159459590911865, \"LABEL\": \"lebas\"}, {\"X\": -1.3223336935043335, \"Y\": -3.7421135902404785, \"LABEL\": \"unblemished\"}, {\"X\": 0.15655210614204407, \"Y\": -0.3608507215976715, \"LABEL\": \"imperium\"}, {\"X\": 2.5757346153259277, \"Y\": 3.895517349243164, \"LABEL\": \"dupuy\"}, {\"X\": -0.5399155020713806, \"Y\": -0.3151606023311615, \"LABEL\": \"steeleye\"}, {\"X\": -2.296064853668213, \"Y\": -0.6031171679496765, \"LABEL\": \"3,000-kilometer\"}, {\"X\": 5.309158802032471, \"Y\": 2.472219944000244, \"LABEL\": \"licencia\"}, {\"X\": -1.7854912281036377, \"Y\": -0.5511234998703003, \"LABEL\": \"iaps\"}, {\"X\": 1.05715811252594, \"Y\": -2.5580153465270996, \"LABEL\": \"pla\"}, {\"X\": 0.16583386063575745, \"Y\": 6.722297191619873, \"LABEL\": \"sirchia\"}, {\"X\": -4.826889514923096, \"Y\": -3.2541298866271973, \"LABEL\": \"116,000\"}, {\"X\": 0.2685548663139343, \"Y\": -3.0563952922821045, \"LABEL\": \"sires\"}, {\"X\": -1.3101136684417725, \"Y\": -2.310978889465332, \"LABEL\": \"pillagers\"}, {\"X\": 0.5616827607154846, \"Y\": 3.0422887802124023, \"LABEL\": \"harenberg\"}, {\"X\": 5.467261791229248, \"Y\": 2.214053153991699, \"LABEL\": \"significa\"}, {\"X\": 3.336393356323242, \"Y\": -1.9571627378463745, \"LABEL\": \"sonnenstein\"}, {\"X\": 1.316734790802002, \"Y\": 0.14403261244297028, \"LABEL\": \"numerus\"}, {\"X\": 3.7475593090057373, \"Y\": 4.6338653564453125, \"LABEL\": \"j\\u00e1n\"}, {\"X\": -1.0539803504943848, \"Y\": 2.2643802165985107, \"LABEL\": \"regenerates\"}, {\"X\": 1.471402883529663, \"Y\": 1.795513391494751, \"LABEL\": \"trouillaud\"}, {\"X\": -3.5500664710998535, \"Y\": -2.257253885269165, \"LABEL\": \"3,324\"}, {\"X\": 0.5055855512619019, \"Y\": -1.7819554805755615, \"LABEL\": \"pumpernickel\"}, {\"X\": 2.879274845123291, \"Y\": 1.0638189315795898, \"LABEL\": \"sobh\"}, {\"X\": -5.236675262451172, \"Y\": 0.8414791226387024, \"LABEL\": \"jochem\"}, {\"X\": -2.4487709999084473, \"Y\": 1.4517662525177002, \"LABEL\": \"pleged\"}, {\"X\": 2.1931679248809814, \"Y\": -0.14807184040546417, \"LABEL\": \"agr\\u00e9gation\"}, {\"X\": 1.8924541473388672, \"Y\": -1.2164113521575928, \"LABEL\": \"beguinage\"}, {\"X\": -0.4638700783252716, \"Y\": 6.805858135223389, \"LABEL\": \"davitamon\"}, {\"X\": -2.50162672996521, \"Y\": -2.080314874649048, \"LABEL\": \"equates\"}, {\"X\": -0.47591832280158997, \"Y\": -2.238136053085327, \"LABEL\": \"prefiguration\"}, {\"X\": -3.9203178882598877, \"Y\": 0.5638137459754944, \"LABEL\": \"1983-88\"}, {\"X\": 0.4558611214160919, \"Y\": -4.736464500427246, \"LABEL\": \"sunroom\"}, {\"X\": -2.7315287590026855, \"Y\": -1.8032422065734863, \"LABEL\": \"124-billion\"}, {\"X\": 1.4715356826782227, \"Y\": -3.07743501663208, \"LABEL\": \"carmelites\"}, {\"X\": 4.292532920837402, \"Y\": -0.6686360836029053, \"LABEL\": \"gummidipoondi\"}, {\"X\": 1.0026755332946777, \"Y\": -0.9060705304145813, \"LABEL\": \"kltm\"}, {\"X\": -1.8267844915390015, \"Y\": 6.275962829589844, \"LABEL\": \"joycelyn\"}, {\"X\": -6.25808572769165, \"Y\": -0.31249985098838806, \"LABEL\": \"7/5\"}, {\"X\": -0.37095168232917786, \"Y\": -2.7969889640808105, \"LABEL\": \"posturings\"}, {\"X\": -4.031467437744141, \"Y\": 4.86801290512085, \"LABEL\": \"eriq\"}, {\"X\": 3.644117593765259, \"Y\": -2.657287836074829, \"LABEL\": \"areopolis\"}, {\"X\": -0.4064937233924866, \"Y\": -1.1493180990219116, \"LABEL\": \"turn-taking\"}, {\"X\": -4.031386852264404, \"Y\": -1.3943740129470825, \"LABEL\": \"44-5\"}, {\"X\": 3.3748860359191895, \"Y\": 3.1632063388824463, \"LABEL\": \"aitzaz\"}, {\"X\": -3.8744685649871826, \"Y\": -2.542637348175049, \"LABEL\": \"2,636\"}, {\"X\": 4.001797676086426, \"Y\": 0.4298775792121887, \"LABEL\": \"cloudland\"}, {\"X\": -5.302472114562988, \"Y\": -2.048213243484497, \"LABEL\": \"287.5\"}, {\"X\": 1.1846204996109009, \"Y\": 1.9201542139053345, \"LABEL\": \"sathiah\"}, {\"X\": -0.45195356011390686, \"Y\": 3.7893335819244385, \"LABEL\": \"rognoni\"}, {\"X\": 0.57712721824646, \"Y\": -1.9497286081314087, \"LABEL\": \"100-gun\"}, {\"X\": 1.0656646490097046, \"Y\": 0.6762267351150513, \"LABEL\": \"decumbens\"}, {\"X\": -0.9309497475624084, \"Y\": 3.1078860759735107, \"LABEL\": \"kristensen\"}, {\"X\": 2.455493211746216, \"Y\": -4.695106029510498, \"LABEL\": \"waltrick\"}, {\"X\": -4.946902275085449, \"Y\": 0.6659126877784729, \"LABEL\": \"foodstuff\"}, {\"X\": 2.817880630493164, \"Y\": -0.5876708030700684, \"LABEL\": \"burghfield\"}, {\"X\": -0.001998122315853834, \"Y\": 1.7208399772644043, \"LABEL\": \"aist\"}, {\"X\": 1.7961534261703491, \"Y\": 6.360050678253174, \"LABEL\": \"mincey\"}, {\"X\": 2.437303304672241, \"Y\": -0.8513261675834656, \"LABEL\": \"multi-sports\"}, {\"X\": 0.26340994238853455, \"Y\": 0.39143630862236023, \"LABEL\": \"asphaltum\"}, {\"X\": 3.193716526031494, \"Y\": 1.2791244983673096, \"LABEL\": \"karoo\"}, {\"X\": -2.2613348960876465, \"Y\": -4.209473133087158, \"LABEL\": \"thickness\"}, {\"X\": 5.7262372970581055, \"Y\": -0.9647931456565857, \"LABEL\": \"wakiso\"}, {\"X\": -4.848967552185059, \"Y\": -3.339970350265503, \"LABEL\": \"6,200\"}, {\"X\": 2.5872299671173096, \"Y\": -0.08229467272758484, \"LABEL\": \"birker\"}, {\"X\": -1.5068023204803467, \"Y\": -6.514584064483643, \"LABEL\": \"expertise\"}, {\"X\": -2.128805160522461, \"Y\": 0.37529584765434265, \"LABEL\": \"6,000-acre\"}, {\"X\": 3.5802996158599854, \"Y\": 7.086812496185303, \"LABEL\": \"producer\"}, {\"X\": 2.160682439804077, \"Y\": -1.25246000289917, \"LABEL\": \"lexton\"}, {\"X\": 5.099136829376221, \"Y\": -4.387190341949463, \"LABEL\": \"fyodorovich\"}, {\"X\": 4.582263469696045, \"Y\": 1.880804181098938, \"LABEL\": \"telenor\"}, {\"X\": -0.7743650078773499, \"Y\": -3.5206775665283203, \"LABEL\": \"entrancing\"}, {\"X\": 4.217233657836914, \"Y\": 4.869855880737305, \"LABEL\": \"tsiolkovsky\"}, {\"X\": -0.2369120717048645, \"Y\": 0.13969649374485016, \"LABEL\": \"cloudscape\"}, {\"X\": 0.5267013907432556, \"Y\": 1.6389307975769043, \"LABEL\": \"heick\"}, {\"X\": 1.8282414674758911, \"Y\": -3.415323257446289, \"LABEL\": \"magmatism\"}, {\"X\": -4.382731914520264, \"Y\": -1.0836540460586548, \"LABEL\": \"89.55\"}, {\"X\": 1.1300382614135742, \"Y\": 1.3153276443481445, \"LABEL\": \"steidl\"}, {\"X\": -2.1338353157043457, \"Y\": 1.944483995437622, \"LABEL\": \"sinndar\"}, {\"X\": 0.8920444846153259, \"Y\": 3.801344871520996, \"LABEL\": \"carretta\"}, {\"X\": 0.8753851056098938, \"Y\": 4.948059558868408, \"LABEL\": \"faqiryar\"}, {\"X\": -0.6004058122634888, \"Y\": 1.738161563873291, \"LABEL\": \"rajveer\"}, {\"X\": 2.7250959873199463, \"Y\": 0.5858160853385925, \"LABEL\": \"worde\"}, {\"X\": -3.5484554767608643, \"Y\": 2.9867286682128906, \"LABEL\": \"nounou\"}, {\"X\": 1.3412916660308838, \"Y\": 0.4048483073711395, \"LABEL\": \"yellamma\"}, {\"X\": -4.928579330444336, \"Y\": -1.1014864444732666, \"LABEL\": \"7.08\"}, {\"X\": -2.961972713470459, \"Y\": -5.505221366882324, \"LABEL\": \"teeth\"}, {\"X\": -3.9211878776550293, \"Y\": -2.5976405143737793, \"LABEL\": \"2,319\"}, {\"X\": -3.3181211948394775, \"Y\": 1.810122013092041, \"LABEL\": \"paleoclimatologist\"}, {\"X\": -5.222714900970459, \"Y\": 0.10607772320508957, \"LABEL\": \"turnovers\"}, {\"X\": -1.3040059804916382, \"Y\": -5.117013931274414, \"LABEL\": \"positioning\"}, {\"X\": 4.274844646453857, \"Y\": -0.3451537489891052, \"LABEL\": \"thoppigala\"}, {\"X\": -4.234315872192383, \"Y\": 1.284879446029663, \"LABEL\": \"1987/88\"}, {\"X\": -1.0529943704605103, \"Y\": -4.13674783706665, \"LABEL\": \"headlight\"}, {\"X\": -4.2410054206848145, \"Y\": -0.8377761840820312, \"LABEL\": \"1.2927\"}, {\"X\": 0.8266724944114685, \"Y\": 3.3899428844451904, \"LABEL\": \"lutze\"}, {\"X\": -2.3500008583068848, \"Y\": -3.490489959716797, \"LABEL\": \"sofas\"}, {\"X\": -1.5417687892913818, \"Y\": -0.5908432602882385, \"LABEL\": \"carburetion\"}, {\"X\": 2.1308534145355225, \"Y\": 2.03092360496521, \"LABEL\": \"basile\"}, {\"X\": 2.2077114582061768, \"Y\": 3.8405659198760986, \"LABEL\": \"bouka\"}, {\"X\": -1.944474458694458, \"Y\": -1.3842973709106445, \"LABEL\": \"debriefing\"}, {\"X\": -2.959296464920044, \"Y\": 4.2936320304870605, \"LABEL\": \"gooly\"}, {\"X\": -4.15456485748291, \"Y\": -2.787092685699463, \"LABEL\": \"1,232\"}, {\"X\": -4.25177526473999, \"Y\": -0.8780024647712708, \"LABEL\": \"121.70\"}, {\"X\": -5.461756229400635, \"Y\": -1.9666601419448853, \"LABEL\": \"125.4\"}, {\"X\": -1.7623569965362549, \"Y\": 0.28280630707740784, \"LABEL\": \"briseis\"}, {\"X\": -3.8945109844207764, \"Y\": -2.505610942840576, \"LABEL\": \"3,281\"}, {\"X\": -2.558455228805542, \"Y\": 3.0234038829803467, \"LABEL\": \"curtails\"}, {\"X\": 1.7979060411453247, \"Y\": 3.002246379852295, \"LABEL\": \"jonathon\"}, {\"X\": -3.1554884910583496, \"Y\": -1.8562639951705933, \"LABEL\": \"4,121\"}, {\"X\": -0.5963294506072998, \"Y\": -3.254037618637085, \"LABEL\": \"spareness\"}, {\"X\": 1.354068636894226, \"Y\": 3.1611294746398926, \"LABEL\": \"dweck\"}, {\"X\": -6.822629928588867, \"Y\": -3.92134165763855, \"LABEL\": \"wacotrib.com\"}, {\"X\": 0.35530295968055725, \"Y\": 3.2686545848846436, \"LABEL\": \"crosslin\"}, {\"X\": 0.28793007135391235, \"Y\": 2.0916097164154053, \"LABEL\": \"jajang\"}, {\"X\": 3.0654635429382324, \"Y\": 3.6713030338287354, \"LABEL\": \"colavito\"}, {\"X\": -0.7819266319274902, \"Y\": -1.5106292963027954, \"LABEL\": \"unscriptural\"}, {\"X\": 6.830411434173584, \"Y\": 0.3410165309906006, \"LABEL\": \"stonewalled\"}, {\"X\": 3.994908332824707, \"Y\": 3.0727360248565674, \"LABEL\": \"urc\"}, {\"X\": -1.3957493305206299, \"Y\": 7.365139484405518, \"LABEL\": \"trackage\"}, {\"X\": -1.690016269683838, \"Y\": 1.223793387413025, \"LABEL\": \"oboes\"}, {\"X\": 5.01585054397583, \"Y\": -2.198807716369629, \"LABEL\": \"janon\"}, {\"X\": -0.5903539657592773, \"Y\": -7.325730323791504, \"LABEL\": \"mystery\"}, {\"X\": -0.4692864716053009, \"Y\": 2.8181867599487305, \"LABEL\": \"sillett\"}, {\"X\": -1.0554184913635254, \"Y\": 2.4574525356292725, \"LABEL\": \"raj\"}, {\"X\": -0.31882697343826294, \"Y\": -5.331989765167236, \"LABEL\": \"masterminded\"}, {\"X\": 1.7422699928283691, \"Y\": -1.7260442972183228, \"LABEL\": \"tabar\\u00e9\"}, {\"X\": -1.101717472076416, \"Y\": -0.9721282124519348, \"LABEL\": \"actor/actress\"}, {\"X\": -7.586452007293701, \"Y\": -1.1921958923339844, \"LABEL\": \".215\"}, {\"X\": 2.021117687225342, \"Y\": 0.4819521903991699, \"LABEL\": \"ratione\"}, {\"X\": 1.2894195318222046, \"Y\": 3.5349836349487305, \"LABEL\": \"bazoum\"}, {\"X\": -0.12124724686145782, \"Y\": 2.0907950401306152, \"LABEL\": \"polyneices\"}, {\"X\": 3.6608190536499023, \"Y\": -2.89770245552063, \"LABEL\": \"peloponnese\"}, {\"X\": 3.0233747959136963, \"Y\": -1.6311924457550049, \"LABEL\": \"ystalyfera\"}, {\"X\": -0.6988768577575684, \"Y\": -4.664481163024902, \"LABEL\": \"judgments\"}, {\"X\": -1.585123896598816, \"Y\": -2.1954338550567627, \"LABEL\": \"profiteer\"}, {\"X\": 1.366675853729248, \"Y\": 0.5327690243721008, \"LABEL\": \"queimada\"}, {\"X\": -0.5595054626464844, \"Y\": 3.5084712505340576, \"LABEL\": \"youmans\"}, {\"X\": -3.8493213653564453, \"Y\": 0.6493184566497803, \"LABEL\": \"1987-1992\"}, {\"X\": 0.5689823031425476, \"Y\": 1.1964198350906372, \"LABEL\": \"ger\\u00f0r\"}, {\"X\": -1.6924771070480347, \"Y\": -6.53685998916626, \"LABEL\": \"athletes\"}, {\"X\": 4.886223316192627, \"Y\": 4.100514888763428, \"LABEL\": \"strombergs\"}, {\"X\": 4.14408016204834, \"Y\": -0.9998959302902222, \"LABEL\": \"mooresville\"}, {\"X\": -5.568302631378174, \"Y\": -2.0623037815093994, \"LABEL\": \"euro618\"}, {\"X\": -3.2910704612731934, \"Y\": -4.994430065155029, \"LABEL\": \"sherds\"}, {\"X\": -0.19207938015460968, \"Y\": 0.12812243402004242, \"LABEL\": \"gsci\"}, {\"X\": -3.2240121364593506, \"Y\": 3.939603328704834, \"LABEL\": \"cagnes\"}, {\"X\": -2.579202175140381, \"Y\": 0.19039298593997955, \"LABEL\": \"russia-1\"}, {\"X\": -1.1814476251602173, \"Y\": -2.7492849826812744, \"LABEL\": \"crusting\"}, {\"X\": -1.1408672332763672, \"Y\": 1.8979814052581787, \"LABEL\": \"mbami\"}, {\"X\": -0.30487000942230225, \"Y\": -1.3758394718170166, \"LABEL\": \"blonde-haired\"}, {\"X\": 0.5100669860839844, \"Y\": -4.09489107131958, \"LABEL\": \"patchworks\"}, {\"X\": -5.040509223937988, \"Y\": 4.542138576507568, \"LABEL\": \"emmylou\"}, {\"X\": 4.177653789520264, \"Y\": -1.190713882446289, \"LABEL\": \"verdal\"}, {\"X\": -5.4654717445373535, \"Y\": -0.3890540897846222, \"LABEL\": \"4-16\"}, {\"X\": -0.44834089279174805, \"Y\": 1.6226990222930908, \"LABEL\": \"chandrashekhar\"}, {\"X\": -0.9215325713157654, \"Y\": -6.037036418914795, \"LABEL\": \"trades\"}, {\"X\": 1.0808167457580566, \"Y\": 2.9551568031311035, \"LABEL\": \"woolfson\"}, {\"X\": -0.30384203791618347, \"Y\": 1.317916989326477, \"LABEL\": \"olympius\"}, {\"X\": 2.9339377880096436, \"Y\": -1.4161930084228516, \"LABEL\": \"leix\\u00f5es\"}, {\"X\": 0.6678705215454102, \"Y\": -0.5694807171821594, \"LABEL\": \"xlviii\"}, {\"X\": -2.293888807296753, \"Y\": 1.0576770305633545, \"LABEL\": \"eardrums\"}, {\"X\": 0.017335643991827965, \"Y\": 5.112030506134033, \"LABEL\": \"gunthardt\"}, {\"X\": -3.7703182697296143, \"Y\": 0.46424853801727295, \"LABEL\": \"1995-2006\"}, {\"X\": -2.3074963092803955, \"Y\": -0.2195073664188385, \"LABEL\": \"miserables\"}, {\"X\": -1.5287901163101196, \"Y\": 2.4281795024871826, \"LABEL\": \"trenet\"}, {\"X\": -1.1870315074920654, \"Y\": 6.662940502166748, \"LABEL\": \"scammon\"}, {\"X\": 0.6717162728309631, \"Y\": 0.6742922067642212, \"LABEL\": \"58-60\"}, {\"X\": 0.533917248249054, \"Y\": 0.42721977829933167, \"LABEL\": \"kishkindha\"}, {\"X\": -0.31816205382347107, \"Y\": 0.5393040776252747, \"LABEL\": \"10-cylinder\"}, {\"X\": -3.372891426086426, \"Y\": -4.730821132659912, \"LABEL\": \"vertebrate\"}, {\"X\": -1.2474336624145508, \"Y\": 3.539135217666626, \"LABEL\": \"kyriakos\"}, {\"X\": -3.7114758491516113, \"Y\": -2.440582275390625, \"LABEL\": \"4,004\"}, {\"X\": -5.243569374084473, \"Y\": -1.9680534601211548, \"LABEL\": \"257.4\"}, {\"X\": -1.8807939291000366, \"Y\": -0.38987186551094055, \"LABEL\": \"sma\"}, {\"X\": -0.8036916851997375, \"Y\": -2.8039536476135254, \"LABEL\": \"bunching\"}, {\"X\": 2.0064597129821777, \"Y\": 0.37521979212760925, \"LABEL\": \"kwe\"}, {\"X\": -7.468425750732422, \"Y\": 2.604368209838867, \"LABEL\": \"1819\"}, {\"X\": -4.196304798126221, \"Y\": -2.492053747177124, \"LABEL\": \"2,880\"}, {\"X\": 0.25378623604774475, \"Y\": 0.2798044979572296, \"LABEL\": \"arisaka\"}, {\"X\": -6.766482830047607, \"Y\": -3.91144061088562, \"LABEL\": \"halmccoy\"}, {\"X\": 2.761002540588379, \"Y\": -3.4594480991363525, \"LABEL\": \"optimedia\"}, {\"X\": -1.2653772830963135, \"Y\": -2.068664789199829, \"LABEL\": \"cyber-bullying\"}, {\"X\": 0.33882781863212585, \"Y\": -1.5027168989181519, \"LABEL\": \"tiamat\"}, {\"X\": 3.4771296977996826, \"Y\": 1.030648946762085, \"LABEL\": \"mocama\"}, {\"X\": -4.147546768188477, \"Y\": -1.1789116859436035, \"LABEL\": \"74.34\"}, {\"X\": -0.30865028500556946, \"Y\": -0.7105736136436462, \"LABEL\": \"must-read\"}, {\"X\": 1.8755614757537842, \"Y\": 4.012731552124023, \"LABEL\": \"gilmartin\"}, {\"X\": 1.343445897102356, \"Y\": 0.2680553197860718, \"LABEL\": \"hanggan\"}, {\"X\": -0.7852455377578735, \"Y\": -3.1372339725494385, \"LABEL\": \"formula_13\"}, {\"X\": 2.7186696529388428, \"Y\": -1.6930367946624756, \"LABEL\": \"parkside\"}, {\"X\": -1.9123166799545288, \"Y\": -1.203402042388916, \"LABEL\": \"64m\"}, {\"X\": 0.06908941268920898, \"Y\": 1.0018724203109741, \"LABEL\": \"arqa\"}, {\"X\": 2.3496718406677246, \"Y\": 3.2075045108795166, \"LABEL\": \"simard\"}, {\"X\": 1.7855472564697266, \"Y\": 1.9943397045135498, \"LABEL\": \"lindemans\"}, {\"X\": 0.9903056025505066, \"Y\": 0.9787611365318298, \"LABEL\": \"nathar\"}, {\"X\": -0.9807329177856445, \"Y\": 5.761176586151123, \"LABEL\": \"pellnas\"}, {\"X\": 0.9816374778747559, \"Y\": 2.25154185295105, \"LABEL\": \"morejon\"}, {\"X\": 0.7620837092399597, \"Y\": 2.492222785949707, \"LABEL\": \"hinkson\"}, {\"X\": 0.871763288974762, \"Y\": 1.373428225517273, \"LABEL\": \"ianthe\"}, {\"X\": 0.6835405230522156, \"Y\": 3.582815408706665, \"LABEL\": \"marife\"}, {\"X\": -1.496787428855896, \"Y\": -6.448486328125, \"LABEL\": \"fundamental\"}, {\"X\": -0.4770636558532715, \"Y\": -1.6055641174316406, \"LABEL\": \"flyrod\"}, {\"X\": 1.9213470220565796, \"Y\": 2.4990925788879395, \"LABEL\": \"kranenburg\"}, {\"X\": -0.3187463879585266, \"Y\": -3.0385072231292725, \"LABEL\": \"slyness\"}, {\"X\": -5.664498329162598, \"Y\": -2.0915589332580566, \"LABEL\": \"euro163\"}, {\"X\": 0.19216369092464447, \"Y\": 2.8508102893829346, \"LABEL\": \"lohr\"}, {\"X\": 2.2180731296539307, \"Y\": 0.254144549369812, \"LABEL\": \"wismut\"}, {\"X\": -2.508146286010742, \"Y\": -1.5358316898345947, \"LABEL\": \"undemarcated\"}, {\"X\": 1.0718848705291748, \"Y\": -0.542174756526947, \"LABEL\": \"kraton\"}, {\"X\": 1.7328085899353027, \"Y\": 4.273179531097412, \"LABEL\": \"verolme\"}, {\"X\": -1.9864548444747925, \"Y\": -6.285579204559326, \"LABEL\": \"voter\"}, {\"X\": 2.74863338470459, \"Y\": -1.8634012937545776, \"LABEL\": \"winchendon\"}, {\"X\": 0.14176902174949646, \"Y\": 1.4656438827514648, \"LABEL\": \"chelsy\"}, {\"X\": 3.522127628326416, \"Y\": 7.009819507598877, \"LABEL\": \"assistant\"}, {\"X\": -4.297441005706787, \"Y\": 2.6471893787384033, \"LABEL\": \"pti\"}, {\"X\": -1.3820332288742065, \"Y\": -2.730053424835205, \"LABEL\": \"backlit\"}, {\"X\": -1.4374973773956299, \"Y\": -6.608609676361084, \"LABEL\": \"web\"}, {\"X\": -3.164677143096924, \"Y\": -1.7328253984451294, \"LABEL\": \"rostral\"}, {\"X\": -1.9427778720855713, \"Y\": -3.3095765113830566, \"LABEL\": \"p3b\"}, {\"X\": 1.5638518333435059, \"Y\": -1.9101330041885376, \"LABEL\": \"yanomami\"}, {\"X\": -0.30344802141189575, \"Y\": 4.345692157745361, \"LABEL\": \"ploetz\"}, {\"X\": 0.9063016772270203, \"Y\": 2.0003840923309326, \"LABEL\": \"uvarova\"}, {\"X\": -4.871540546417236, \"Y\": 3.301527976989746, \"LABEL\": \"kurier\"}, {\"X\": 2.4181442260742188, \"Y\": -0.7885221838951111, \"LABEL\": \"tintinhull\"}, {\"X\": -1.4597383737564087, \"Y\": -0.16043348610401154, \"LABEL\": \"qualis\"}, {\"X\": 1.4102094173431396, \"Y\": -1.578334927558899, \"LABEL\": \"fiatach\"}, {\"X\": -1.5267724990844727, \"Y\": 0.5331829190254211, \"LABEL\": \"nlos\"}, {\"X\": 2.8422257900238037, \"Y\": 5.4792160987854, \"LABEL\": \"arista\"}, {\"X\": 0.5304328799247742, \"Y\": 1.3849059343338013, \"LABEL\": \"mescheriakova\"}, {\"X\": 1.3474817276000977, \"Y\": 2.129301071166992, \"LABEL\": \"vyjayanthimala\"}, {\"X\": 1.5536489486694336, \"Y\": 4.000104904174805, \"LABEL\": \"doerig\"}, {\"X\": 1.422691822052002, \"Y\": -2.3855247497558594, \"LABEL\": \"unlf\"}, {\"X\": -1.749647855758667, \"Y\": 3.79532527923584, \"LABEL\": \"feltman\"}, {\"X\": 0.3384338617324829, \"Y\": -1.3482375144958496, \"LABEL\": \"biopsychosocial\"}, {\"X\": 1.3779208660125732, \"Y\": 2.8800904750823975, \"LABEL\": \"gane\"}, {\"X\": 4.270576000213623, \"Y\": 0.5146393179893494, \"LABEL\": \"caprock\"}, {\"X\": 5.204737186431885, \"Y\": 1.4236226081848145, \"LABEL\": \"percenters\"}, {\"X\": -0.2698369026184082, \"Y\": 1.6557774543762207, \"LABEL\": \"davidovsky\"}, {\"X\": 3.0713229179382324, \"Y\": -1.9603455066680908, \"LABEL\": \"caerleon\"}, {\"X\": -5.816298961639404, \"Y\": -0.06787492334842682, \"LABEL\": \"17-second\"}, {\"X\": 4.487124443054199, \"Y\": -0.945371687412262, \"LABEL\": \"wanne-eickel\"}, {\"X\": 1.8167380094528198, \"Y\": 2.8632287979125977, \"LABEL\": \"rawcliffe\"}, {\"X\": -3.5409433841705322, \"Y\": -3.5163872241973877, \"LABEL\": \"box-office\"}, {\"X\": -0.6984370946884155, \"Y\": 0.6374652981758118, \"LABEL\": \"gretai\"}, {\"X\": -4.3046112060546875, \"Y\": 6.680871963500977, \"LABEL\": \"jenson\"}, {\"X\": -1.4356439113616943, \"Y\": -4.2532219886779785, \"LABEL\": \"laughter\"}, {\"X\": 3.1048295497894287, \"Y\": -0.8844076991081238, \"LABEL\": \"narellan\"}, {\"X\": 4.562843322753906, \"Y\": 3.7597293853759766, \"LABEL\": \"dorien\"}, {\"X\": -3.1502108573913574, \"Y\": -2.2346739768981934, \"LABEL\": \"18-to\"}, {\"X\": -4.0432915687561035, \"Y\": -7.140585422515869, \"LABEL\": \"walkway\"}, {\"X\": 2.9473965167999268, \"Y\": -0.3248390555381775, \"LABEL\": \"\\u00e9glise\"}, {\"X\": 2.9742541313171387, \"Y\": 6.357918739318848, \"LABEL\": \"antony\"}, {\"X\": -1.3042824268341064, \"Y\": -4.023340702056885, \"LABEL\": \"grunts\"}, {\"X\": -1.645735502243042, \"Y\": -6.469205379486084, \"LABEL\": \"abuses\"}, {\"X\": -0.055001843720674515, \"Y\": 4.1232452392578125, \"LABEL\": \"borer\"}, {\"X\": -1.6499004364013672, \"Y\": -6.255029678344727, \"LABEL\": \"presenting\"}, {\"X\": -5.742720127105713, \"Y\": -1.9666509628295898, \"LABEL\": \"104.9\"}, {\"X\": 0.014196083880960941, \"Y\": -4.001285552978516, \"LABEL\": \"backfires\"}, {\"X\": 4.014240741729736, \"Y\": 4.163486003875732, \"LABEL\": \"wargny\"}, {\"X\": 1.6292482614517212, \"Y\": 3.0718047618865967, \"LABEL\": \"dasmunshi\"}, {\"X\": 0.24916835129261017, \"Y\": 1.5407779216766357, \"LABEL\": \"philotas\"}, {\"X\": 3.3527026176452637, \"Y\": -3.9645614624023438, \"LABEL\": \"aleksandras\"}, {\"X\": -1.5364717245101929, \"Y\": -6.523622035980225, \"LABEL\": \"subjects\"}, {\"X\": 3.877957582473755, \"Y\": -2.520808219909668, \"LABEL\": \"vlore\"}, {\"X\": 0.8388885855674744, \"Y\": 1.2545607089996338, \"LABEL\": \"lankesh\"}, {\"X\": -0.9237035512924194, \"Y\": -1.6685256958007812, \"LABEL\": \"roundly\"}, {\"X\": 2.264538526535034, \"Y\": 0.18918681144714355, \"LABEL\": \"unirii\"}, {\"X\": -1.6012177467346191, \"Y\": -6.532229423522949, \"LABEL\": \"task\"}, {\"X\": 0.17557109892368317, \"Y\": -0.24225609004497528, \"LABEL\": \"m-22\"}, {\"X\": -2.148702383041382, \"Y\": -3.8878564834594727, \"LABEL\": \"unintentional\"}, {\"X\": 6.655385971069336, \"Y\": 1.0974757671356201, \"LABEL\": \"prebiotic\"}, {\"X\": 0.30119794607162476, \"Y\": 0.6322625875473022, \"LABEL\": \"ayat\"}, {\"X\": 0.1089649349451065, \"Y\": 4.5833964347839355, \"LABEL\": \"felten\"}, {\"X\": -0.5569761991500854, \"Y\": -3.267590284347534, \"LABEL\": \"selfishness\"}, {\"X\": 0.5319772958755493, \"Y\": 0.33482247591018677, \"LABEL\": \"illuminatus\"}, {\"X\": -3.443408966064453, \"Y\": 3.880845546722412, \"LABEL\": \"muzahim\"}, {\"X\": -1.4684882164001465, \"Y\": 5.007636070251465, \"LABEL\": \"mathie\"}, {\"X\": 4.428638935089111, \"Y\": -0.00524548115208745, \"LABEL\": \"pokhara\"}, {\"X\": 0.7817288041114807, \"Y\": 4.2047929763793945, \"LABEL\": \"perricos\"}, {\"X\": 2.053314208984375, \"Y\": -2.3377747535705566, \"LABEL\": \"gurdwara\"}, {\"X\": -4.240405559539795, \"Y\": -0.8020551204681396, \"LABEL\": \"1.4205\"}, {\"X\": 1.635524034500122, \"Y\": 5.310762882232666, \"LABEL\": \"bisi\"}, {\"X\": 1.0076950788497925, \"Y\": 5.310148239135742, \"LABEL\": \"husam\"}, {\"X\": 0.638773500919342, \"Y\": 3.531987190246582, \"LABEL\": \"paprocki\"}, {\"X\": 3.7680346965789795, \"Y\": 1.440186619758606, \"LABEL\": \"waart\"}, {\"X\": -4.626597881317139, \"Y\": -0.8306002020835876, \"LABEL\": \"30.11\"}, {\"X\": 2.001415967941284, \"Y\": 0.8399917483329773, \"LABEL\": \"wbcn\"}, {\"X\": 1.2690783739089966, \"Y\": 1.7902542352676392, \"LABEL\": \"zulma\"}, {\"X\": 0.5581320524215698, \"Y\": -2.5511460304260254, \"LABEL\": \"clitoris\"}, {\"X\": 0.705920398235321, \"Y\": 0.01946808397769928, \"LABEL\": \"r.e.d.\"}, {\"X\": 1.2560081481933594, \"Y\": 3.542628288269043, \"LABEL\": \"lebrecht\"}, {\"X\": 1.2994087934494019, \"Y\": 6.091761112213135, \"LABEL\": \"roed\"}, {\"X\": 0.7495995759963989, \"Y\": -2.65610408782959, \"LABEL\": \"trotting\"}, {\"X\": 1.2960680723190308, \"Y\": -0.31825077533721924, \"LABEL\": \"aktif\"}, {\"X\": 1.253426194190979, \"Y\": 4.537588119506836, \"LABEL\": \"malikha\"}, {\"X\": -1.0184941291809082, \"Y\": 2.9089548587799072, \"LABEL\": \"kutta\"}, {\"X\": 3.2772459983825684, \"Y\": -5.388905048370361, \"LABEL\": \"photoacoustic\"}, {\"X\": 0.35075798630714417, \"Y\": -0.9332445859909058, \"LABEL\": \"gujarati\"}, {\"X\": 4.205495357513428, \"Y\": 0.29566502571105957, \"LABEL\": \"cusco\"}, {\"X\": 0.8697137832641602, \"Y\": -3.0889148712158203, \"LABEL\": \"benzalkonium\"}, {\"X\": 0.34164664149284363, \"Y\": 5.0287275314331055, \"LABEL\": \"poncino\"}, {\"X\": -6.082117557525635, \"Y\": -0.15101027488708496, \"LABEL\": \"33-28\"}, {\"X\": -5.890989780426025, \"Y\": -2.3042473793029785, \"LABEL\": \"writedown\"}, {\"X\": -4.1762919425964355, \"Y\": -1.076662540435791, \"LABEL\": \"53.46\"}, {\"X\": 1.8934662342071533, \"Y\": 4.314352512359619, \"LABEL\": \"choksi\"}, {\"X\": 0.30162712931632996, \"Y\": 3.124713659286499, \"LABEL\": \"krenn\"}, {\"X\": -0.25200751423835754, \"Y\": 3.180025577545166, \"LABEL\": \"holtzclaw\"}, {\"X\": -1.4879504442214966, \"Y\": -0.7489076256752014, \"LABEL\": \"nulcear\"}, {\"X\": -2.6385388374328613, \"Y\": 2.0685794353485107, \"LABEL\": \"casspi\"}, {\"X\": 0.844614565372467, \"Y\": 3.3198554515838623, \"LABEL\": \"ellena\"}, {\"X\": -7.416344165802002, \"Y\": 2.5491812229156494, \"LABEL\": \"1154\"}, {\"X\": 0.9460113644599915, \"Y\": 0.5972698330879211, \"LABEL\": \"eglantine\"}, {\"X\": -3.9810218811035156, \"Y\": 0.7255771160125732, \"LABEL\": \"committeewoman\"}, {\"X\": 4.402213096618652, \"Y\": 5.157804012298584, \"LABEL\": \"tex.\"}, {\"X\": 4.757976531982422, \"Y\": -0.30392658710479736, \"LABEL\": \"bujanovac\"}, {\"X\": -0.9327129125595093, \"Y\": -1.798291802406311, \"LABEL\": \"anti-white\"}, {\"X\": 1.450936198234558, \"Y\": 3.436980724334717, \"LABEL\": \"khalib\"}, {\"X\": 3.212677240371704, \"Y\": 2.516474723815918, \"LABEL\": \"amena\"}, {\"X\": 6.692416667938232, \"Y\": 0.20123730599880219, \"LABEL\": \"machine-gunned\"}, {\"X\": -1.4619961977005005, \"Y\": -1.5391689538955688, \"LABEL\": \"handshaking\"}, {\"X\": -1.5725042819976807, \"Y\": 4.845310211181641, \"LABEL\": \"marouane\"}, {\"X\": 0.5908697843551636, \"Y\": 3.8155040740966797, \"LABEL\": \"disgustedly\"}, {\"X\": -3.4511022567749023, \"Y\": 4.23668098449707, \"LABEL\": \"tungar\"}, {\"X\": 3.008793354034424, \"Y\": 3.220454454421997, \"LABEL\": \"pahadi\"}, {\"X\": 4.3869547843933105, \"Y\": -1.9532766342163086, \"LABEL\": \"patea\"}, {\"X\": -4.63763427734375, \"Y\": 1.2208495140075684, \"LABEL\": \"zinifex\"}, {\"X\": -0.5538192987442017, \"Y\": 2.6257882118225098, \"LABEL\": \"feizi\"}, {\"X\": -3.4358925819396973, \"Y\": -2.164620876312256, \"LABEL\": \"5,312\"}, {\"X\": 2.684979200363159, \"Y\": -2.4565584659576416, \"LABEL\": \"shapinsay\"}, {\"X\": 0.9860987663269043, \"Y\": -1.3542110919952393, \"LABEL\": \"cochlear\"}, {\"X\": 0.29603904485702515, \"Y\": 3.975151538848877, \"LABEL\": \"sapoznik\"}, {\"X\": -1.608170509338379, \"Y\": 3.3516581058502197, \"LABEL\": \"ibraimov\"}, {\"X\": 1.2351899147033691, \"Y\": 2.767551898956299, \"LABEL\": \"cotterell\"}, {\"X\": -4.034062385559082, \"Y\": -2.73197340965271, \"LABEL\": \"2,199\"}, {\"X\": 0.7458032369613647, \"Y\": -0.3006262481212616, \"LABEL\": \"neo-vedanta\"}, {\"X\": 1.8992657661437988, \"Y\": 1.2000670433044434, \"LABEL\": \"aundra\"}, {\"X\": -0.7547206282615662, \"Y\": 6.155946731567383, \"LABEL\": \"zuendel\"}, {\"X\": -1.1293559074401855, \"Y\": -3.9771671295166016, \"LABEL\": \"chrome-plated\"}, {\"X\": 2.3280320167541504, \"Y\": -6.159022808074951, \"LABEL\": \"0-for-21\"}, {\"X\": 1.0614056587219238, \"Y\": 1.009429693222046, \"LABEL\": \"bushbaby\"}, {\"X\": 2.654714345932007, \"Y\": -0.05221828445792198, \"LABEL\": \"gornje\"}, {\"X\": -0.32072073221206665, \"Y\": 4.207300662994385, \"LABEL\": \"mbah\"}, {\"X\": 0.016742102801799774, \"Y\": 2.8844242095947266, \"LABEL\": \"arnison\"}, {\"X\": -2.7803733348846436, \"Y\": 5.518004417419434, \"LABEL\": \"kodro\"}, {\"X\": 4.021331787109375, \"Y\": -4.167285919189453, \"LABEL\": \"proteus\"}, {\"X\": 1.4656188488006592, \"Y\": -0.006635617930442095, \"LABEL\": \"pixeljunk\"}, {\"X\": -1.4998618364334106, \"Y\": -6.41180419921875, \"LABEL\": \"preserving\"}, {\"X\": 2.4216525554656982, \"Y\": -0.858615517616272, \"LABEL\": \"s.c\"}, {\"X\": 0.3768294155597687, \"Y\": 3.3281025886535645, \"LABEL\": \"lapinski\"}, {\"X\": -1.553665280342102, \"Y\": 1.428961157798767, \"LABEL\": \"khalsa\"}, {\"X\": 1.4373507499694824, \"Y\": 1.4301531314849854, \"LABEL\": \"jakopi\\u010d\"}, {\"X\": -2.374077796936035, \"Y\": -3.1762094497680664, \"LABEL\": \"pinkish\"}, {\"X\": -0.2931973934173584, \"Y\": 1.5436676740646362, \"LABEL\": \"w.b.\"}, {\"X\": -1.255340576171875, \"Y\": -1.9186952114105225, \"LABEL\": \"embroiled\"}, {\"X\": -2.261948823928833, \"Y\": 4.4878644943237305, \"LABEL\": \"ndabaningi\"}, {\"X\": 2.248040199279785, \"Y\": 5.678334712982178, \"LABEL\": \"tollefson\"}, {\"X\": -1.824782133102417, \"Y\": 5.31970739364624, \"LABEL\": \"stamboli\\u0107\"}, {\"X\": 1.766278862953186, \"Y\": 1.541805624961853, \"LABEL\": \"editori\"}, {\"X\": -1.02260160446167, \"Y\": 0.9007568955421448, \"LABEL\": \"unity08\"}, {\"X\": -0.2328198105096817, \"Y\": 3.8835391998291016, \"LABEL\": \"loftin\"}, {\"X\": -0.39929884672164917, \"Y\": 7.918664455413818, \"LABEL\": \"macy\"}, {\"X\": 5.379134654998779, \"Y\": 2.4068191051483154, \"LABEL\": \"eien\"}, {\"X\": -0.288882315158844, \"Y\": -2.3515186309814453, \"LABEL\": \"disturber\"}, {\"X\": 1.413720965385437, \"Y\": -0.9162845015525818, \"LABEL\": \"r\\u014dj\\u016b\"}, {\"X\": -0.7109895348548889, \"Y\": -1.0258235931396484, \"LABEL\": \"alkoxy\"}, {\"X\": -1.0778461694717407, \"Y\": -2.4054479598999023, \"LABEL\": \"mown\"}, {\"X\": 5.326977252960205, \"Y\": -1.082203984260559, \"LABEL\": \"koraput\"}, {\"X\": 3.288917303085327, \"Y\": 6.615772247314453, \"LABEL\": \"craig\"}, {\"X\": -4.022002220153809, \"Y\": -4.676542282104492, \"LABEL\": \"kotzebue\"}, {\"X\": -4.554810047149658, \"Y\": -2.3003265857696533, \"LABEL\": \"16,250\"}, {\"X\": -3.984130382537842, \"Y\": 0.6544055342674255, \"LABEL\": \"decoupled\"}, {\"X\": -0.235978901386261, \"Y\": 2.732724666595459, \"LABEL\": \"chernyshov\"}, {\"X\": -7.461356163024902, \"Y\": 2.590864658355713, \"LABEL\": \"1743\"}, {\"X\": 3.9309117794036865, \"Y\": -4.704222679138184, \"LABEL\": \"propulsid\"}, {\"X\": 0.18228913843631744, \"Y\": 0.2078445702791214, \"LABEL\": \"8/8\"}, {\"X\": 3.4156150817871094, \"Y\": -2.224799156188965, \"LABEL\": \"beaumaris\"}, {\"X\": -3.8028757572174072, \"Y\": -0.123846635222435, \"LABEL\": \"640x480\"}, {\"X\": -0.11516737192869186, \"Y\": 0.5943670272827148, \"LABEL\": \"vvt\"}, {\"X\": 3.5743210315704346, \"Y\": 7.075913906097412, \"LABEL\": \"owner\"}, {\"X\": -1.8139537572860718, \"Y\": -5.2292890548706055, \"LABEL\": \"catwalk\"}, {\"X\": -3.9421820640563965, \"Y\": -2.789942979812622, \"LABEL\": \"2,422\"}, {\"X\": 0.9073460102081299, \"Y\": -5.291321754455566, \"LABEL\": \"pepitas\"}, {\"X\": -2.8408374786376953, \"Y\": -4.314259052276611, \"LABEL\": \"wantagh\"}, {\"X\": -0.561316728591919, \"Y\": 0.5584275126457214, \"LABEL\": \"tfe\"}, {\"X\": 2.9533274173736572, \"Y\": 2.6758387088775635, \"LABEL\": \"edgardo\"}, {\"X\": 5.958603382110596, \"Y\": 3.0154266357421875, \"LABEL\": \"viajes\"}, {\"X\": -5.749955654144287, \"Y\": 1.4496738910675049, \"LABEL\": \"nerchinsk\"}, {\"X\": 2.791642904281616, \"Y\": -0.9436494708061218, \"LABEL\": \"belsize\"}, {\"X\": 1.9164773225784302, \"Y\": -1.497007131576538, \"LABEL\": \"mnl\"}, {\"X\": 0.15914617478847504, \"Y\": -2.119220495223999, \"LABEL\": \"monospecific\"}, {\"X\": -5.256027698516846, \"Y\": 1.302839756011963, \"LABEL\": \"democratica\"}, {\"X\": -5.52147102355957, \"Y\": -4.438582420349121, \"LABEL\": \"gusta\"}, {\"X\": 0.6448510885238647, \"Y\": 0.4842141270637512, \"LABEL\": \"feverfew\"}, {\"X\": -5.007630348205566, \"Y\": 4.475305080413818, \"LABEL\": \"c.z.\"}, {\"X\": 0.006754470057785511, \"Y\": -0.7235528826713562, \"LABEL\": \"la-z-boy\"}, {\"X\": 4.13307523727417, \"Y\": -2.2014262676239014, \"LABEL\": \"skowhegan\"}, {\"X\": 1.1832281351089478, \"Y\": 1.108353853225708, \"LABEL\": \"sol\\u00e9\"}, {\"X\": -3.3744633197784424, \"Y\": -2.2313315868377686, \"LABEL\": \"5,198\"}, {\"X\": -4.689788818359375, \"Y\": -0.9902364611625671, \"LABEL\": \"26.73\"}, {\"X\": 3.450007438659668, \"Y\": 1.5437496900558472, \"LABEL\": \"chena\"}, {\"X\": 2.362287759780884, \"Y\": -3.706029176712036, \"LABEL\": \"sinan\"}, {\"X\": -0.3580167591571808, \"Y\": -2.4252359867095947, \"LABEL\": \"fluorinated\"}, {\"X\": 1.347459077835083, \"Y\": 1.9978440999984741, \"LABEL\": \"gurukiran\"}, {\"X\": 0.34347671270370483, \"Y\": 1.6605117321014404, \"LABEL\": \"harue\"}, {\"X\": -1.717286467552185, \"Y\": -3.3802244663238525, \"LABEL\": \"lobsters\"}, {\"X\": -3.957296371459961, \"Y\": -1.5379160642623901, \"LABEL\": \"833,000\"}, {\"X\": 5.857883453369141, \"Y\": -0.9385608434677124, \"LABEL\": \"dadeldhura\"}, {\"X\": -4.757223129272461, \"Y\": 3.223944664001465, \"LABEL\": \"bhorer\"}, {\"X\": 3.400444507598877, \"Y\": 0.5070397853851318, \"LABEL\": \"macdonald-cartier\"}, {\"X\": -1.5913797616958618, \"Y\": -6.500370979309082, \"LABEL\": \"especially\"}, {\"X\": -2.698624849319458, \"Y\": 0.712303102016449, \"LABEL\": \"confidentially\"}, {\"X\": -3.6100540161132812, \"Y\": -0.05557796731591225, \"LABEL\": \"19:47\"}, {\"X\": -0.5411399602890015, \"Y\": -4.237624168395996, \"LABEL\": \"dispositions\"}, {\"X\": -0.5711542367935181, \"Y\": -3.3373525142669678, \"LABEL\": \"sublimity\"}, {\"X\": 1.1279306411743164, \"Y\": 3.555875778198242, \"LABEL\": \"helmut\"}, {\"X\": 0.4781818389892578, \"Y\": 6.659052848815918, \"LABEL\": \"darabos\"}, {\"X\": 3.019728660583496, \"Y\": -1.4923511743545532, \"LABEL\": \"inner-city\"}, {\"X\": 2.0788862705230713, \"Y\": -4.858784198760986, \"LABEL\": \"cetoniinae\"}, {\"X\": -0.6220438480377197, \"Y\": -0.8463001847267151, \"LABEL\": \"wraparounds\"}, {\"X\": -0.24795317649841309, \"Y\": 5.232926368713379, \"LABEL\": \"jovanka\"}, {\"X\": 0.49259209632873535, \"Y\": 0.890393853187561, \"LABEL\": \"kiga\"}, {\"X\": -0.70025235414505, \"Y\": 1.4278408288955688, \"LABEL\": \"commagene\"}, {\"X\": 3.2413151264190674, \"Y\": 5.138324737548828, \"LABEL\": \"ostin\"}, {\"X\": 2.1456499099731445, \"Y\": 4.943078517913818, \"LABEL\": \"yoshiki\"}, {\"X\": 1.5704045295715332, \"Y\": -0.6163040399551392, \"LABEL\": \"cacao\"}, {\"X\": 3.5670623779296875, \"Y\": -1.3623203039169312, \"LABEL\": \"milltown\"}, {\"X\": -2.402376651763916, \"Y\": -6.071774005889893, \"LABEL\": \"blocks\"}, {\"X\": -3.507343292236328, \"Y\": 5.470800399780273, \"LABEL\": \"culinaire\"}, {\"X\": 4.196590423583984, \"Y\": -1.7097747325897217, \"LABEL\": \"kumeu\"}, {\"X\": 3.3740429878234863, \"Y\": 6.333901405334473, \"LABEL\": \"ronnie\"}, {\"X\": -3.01391339302063, \"Y\": -3.0692241191864014, \"LABEL\": \"pre-20th\"}, {\"X\": 0.15339301526546478, \"Y\": -0.7886086106300354, \"LABEL\": \"s\\u00f6\"}, {\"X\": 1.9702814817428589, \"Y\": 3.482522964477539, \"LABEL\": \"demirchyan\"}, {\"X\": 3.9489166736602783, \"Y\": -3.070983648300171, \"LABEL\": \"lezh\\u00eb\"}, {\"X\": 2.094935655593872, \"Y\": -4.921571731567383, \"LABEL\": \"discodorididae\"}, {\"X\": 0.7299563884735107, \"Y\": 4.189367771148682, \"LABEL\": \"lindner\"}, {\"X\": -0.4334982931613922, \"Y\": 2.2169954776763916, \"LABEL\": \"filipovi\\u0107\"}, {\"X\": 3.0766046047210693, \"Y\": -2.859212875366211, \"LABEL\": \"absheron\"}, {\"X\": 0.3085429072380066, \"Y\": 3.106713056564331, \"LABEL\": \"pharris\"}, {\"X\": 0.7716776728630066, \"Y\": 0.8680321574211121, \"LABEL\": \"amma\"}, {\"X\": 0.3473607897758484, \"Y\": -1.7052289247512817, \"LABEL\": \"udl\"}, {\"X\": -5.9418206214904785, \"Y\": -0.1959381103515625, \"LABEL\": \"30-9\"}, {\"X\": 2.891681432723999, \"Y\": 3.0356245040893555, \"LABEL\": \"brooksley\"}, {\"X\": 0.03992221876978874, \"Y\": -1.7088459730148315, \"LABEL\": \"hydronic\"}, {\"X\": -3.044372320175171, \"Y\": -5.4160332679748535, \"LABEL\": \"fragments\"}, {\"X\": -3.1031880378723145, \"Y\": 0.20052240788936615, \"LABEL\": \"3:14\"}, {\"X\": -0.8900588154792786, \"Y\": 0.005421046633273363, \"LABEL\": \"mudhoney\"}, {\"X\": -1.8395298719406128, \"Y\": -1.3695001602172852, \"LABEL\": \"103-day\"}, {\"X\": -7.434380531311035, \"Y\": 2.55911922454834, \"LABEL\": \"1388\"}, {\"X\": 0.932146430015564, \"Y\": 0.03682481497526169, \"LABEL\": \"bereshit\"}, {\"X\": 0.16215689480304718, \"Y\": 1.3615840673446655, \"LABEL\": \"pyrrho\"}, {\"X\": -0.584665834903717, \"Y\": -1.5009846687316895, \"LABEL\": \"indianness\"}, {\"X\": -0.26635608077049255, \"Y\": 6.517177104949951, \"LABEL\": \"jodar\"}, {\"X\": 0.4346732497215271, \"Y\": 2.0244081020355225, \"LABEL\": \"costel\"}, {\"X\": -3.2179455757141113, \"Y\": -0.3676966428756714, \"LABEL\": \"blacklist\"}, {\"X\": 5.965641021728516, \"Y\": 3.0652010440826416, \"LABEL\": \"basicos\"}, {\"X\": -1.5105701684951782, \"Y\": 4.21089506149292, \"LABEL\": \"nukaga\"}, {\"X\": -4.1992082595825195, \"Y\": -0.8858769536018372, \"LABEL\": \"65.33\"}, {\"X\": 1.6032419204711914, \"Y\": -3.767486572265625, \"LABEL\": \"bronchiolitis\"}, {\"X\": 1.042187213897705, \"Y\": 2.2412607669830322, \"LABEL\": \"bergstr\\u00f6m\"}, {\"X\": 1.1460565328598022, \"Y\": 0.6330808401107788, \"LABEL\": \"lenga\"}, {\"X\": -2.156205415725708, \"Y\": 3.4235403537750244, \"LABEL\": \"kolobnev\"}, {\"X\": 0.19898483157157898, \"Y\": 1.202675461769104, \"LABEL\": \"cumple\"}, {\"X\": 4.2191481590271, \"Y\": -2.0831525325775146, \"LABEL\": \"vrbas\"}, {\"X\": -0.44173794984817505, \"Y\": -0.35422173142433167, \"LABEL\": \"omnium\"}, {\"X\": -0.06328596919775009, \"Y\": 3.849374532699585, \"LABEL\": \"m.n.\"}, {\"X\": 0.5183677673339844, \"Y\": -1.2180930376052856, \"LABEL\": \"baklava\"}, {\"X\": 4.18161678314209, \"Y\": 1.7657781839370728, \"LABEL\": \"wrenchingly\"}, {\"X\": 4.2141337394714355, \"Y\": -1.2536927461624146, \"LABEL\": \"\\u0161abac\"}, {\"X\": -2.9547364711761475, \"Y\": -0.2944733500480652, \"LABEL\": \"bio-weapons\"}, {\"X\": 2.882920742034912, \"Y\": -0.265492707490921, \"LABEL\": \"spirituel\"}, {\"X\": 0.661001980304718, \"Y\": 4.638207912445068, \"LABEL\": \"marter\"}, {\"X\": -0.502930223941803, \"Y\": 0.5338069796562195, \"LABEL\": \"xiaoli\"}, {\"X\": 2.005990505218506, \"Y\": -0.506398618221283, \"LABEL\": \"graphique\"}, {\"X\": 4.020852088928223, \"Y\": 3.8278889656066895, \"LABEL\": \"paoa\"}, {\"X\": 0.0971592366695404, \"Y\": -0.39274320006370544, \"LABEL\": \"802.15.4\"}, {\"X\": -2.0295321941375732, \"Y\": 3.9179952144622803, \"LABEL\": \"heung\"}, {\"X\": -4.198530673980713, \"Y\": 2.3096060752868652, \"LABEL\": \"swayambhu\"}, {\"X\": -2.800729274749756, \"Y\": 6.77996826171875, \"LABEL\": \"masahiro\"}, {\"X\": 0.42876145243644714, \"Y\": -3.3303024768829346, \"LABEL\": \"prolix\"}, {\"X\": -0.23004032671451569, \"Y\": 2.3171184062957764, \"LABEL\": \"tancharoen\"}, {\"X\": -1.3620696067810059, \"Y\": 4.23930025100708, \"LABEL\": \"barkett\"}, {\"X\": -1.0134371519088745, \"Y\": 1.1590832471847534, \"LABEL\": \"accordionist\"}, {\"X\": -0.35138723254203796, \"Y\": -3.8252782821655273, \"LABEL\": \"preservers\"}, {\"X\": 0.0021894946694374084, \"Y\": 1.2902743816375732, \"LABEL\": \"macrocephaly\"}, {\"X\": 1.177488923072815, \"Y\": -2.673088550567627, \"LABEL\": \"decorators\"}, {\"X\": -1.5704632997512817, \"Y\": -0.3934870958328247, \"LABEL\": \"homebuying\"}, {\"X\": 1.994243860244751, \"Y\": 4.853201389312744, \"LABEL\": \"chunzheng\"}, {\"X\": 3.5493040084838867, \"Y\": -0.8961758017539978, \"LABEL\": \"rheingau\"}, {\"X\": 1.0105032920837402, \"Y\": -1.1234290599822998, \"LABEL\": \"microcanonical\"}, {\"X\": -1.3686543703079224, \"Y\": -6.101625919342041, \"LABEL\": \"binding\"}, {\"X\": 4.655353546142578, \"Y\": -0.301785945892334, \"LABEL\": \"kabgayi\"}, {\"X\": 0.6055669188499451, \"Y\": 1.0714232921600342, \"LABEL\": \"cyanopepla\"}, {\"X\": 3.8523709774017334, \"Y\": 2.283905506134033, \"LABEL\": \"davids\"}, {\"X\": -0.6884158849716187, \"Y\": -1.3249061107635498, \"LABEL\": \"eight-team\"}, {\"X\": 3.423919916152954, \"Y\": -3.7707602977752686, \"LABEL\": \"cannabidiol\"}, {\"X\": -1.6322578191757202, \"Y\": -2.4045026302337646, \"LABEL\": \"indignities\"}, {\"X\": 3.944270133972168, \"Y\": -1.8638784885406494, \"LABEL\": \"saroma\"}, {\"X\": -4.110910892486572, \"Y\": -2.655947208404541, \"LABEL\": \"waterboarded\"}, {\"X\": 0.8380677700042725, \"Y\": 2.306412696838379, \"LABEL\": \"egelko\"}, {\"X\": -0.2186739146709442, \"Y\": 1.7319562435150146, \"LABEL\": \"potulny\"}, {\"X\": 0.9704009890556335, \"Y\": -5.279729843139648, \"LABEL\": \"breadcrumbs\"}, {\"X\": -1.2863202095031738, \"Y\": 3.1323325634002686, \"LABEL\": \"fabrikant\"}, {\"X\": -1.8605331182479858, \"Y\": -6.371087551116943, \"LABEL\": \"await\"}, {\"X\": 0.8091006278991699, \"Y\": -0.12656144797801971, \"LABEL\": \"bayad\"}, {\"X\": 1.5021908283233643, \"Y\": -1.7138652801513672, \"LABEL\": \"unido\"}, {\"X\": 6.0457353591918945, \"Y\": 3.105625867843628, \"LABEL\": \"niveles\"}, {\"X\": 4.943387508392334, \"Y\": -0.17379623651504517, \"LABEL\": \"drenica\"}, {\"X\": -1.023027777671814, \"Y\": -0.6151145696640015, \"LABEL\": \"cognitive-behavioral\"}, {\"X\": 3.399859666824341, \"Y\": 1.799537181854248, \"LABEL\": \"waele\"}, {\"X\": -4.565952301025391, \"Y\": -1.0552692413330078, \"LABEL\": \"75.50\"}, {\"X\": 4.727248668670654, \"Y\": 3.0521633625030518, \"LABEL\": \"witheringly\"}, {\"X\": 0.27349531650543213, \"Y\": 6.612804412841797, \"LABEL\": \"pourmohammadi\"}, {\"X\": -1.8983066082000732, \"Y\": 1.612220048904419, \"LABEL\": \"gleaves\"}, {\"X\": -1.6734187602996826, \"Y\": -1.3346900939941406, \"LABEL\": \"re-training\"}, {\"X\": 1.6399399042129517, \"Y\": -2.3233821392059326, \"LABEL\": \"fnm\"}, {\"X\": 0.497167706489563, \"Y\": 3.932518720626831, \"LABEL\": \"borrelli\"}, {\"X\": -0.6055610179901123, \"Y\": -7.331160545349121, \"LABEL\": \"adventure\"}, {\"X\": 2.9508919715881348, \"Y\": 2.286942720413208, \"LABEL\": \"tolosa\"}, {\"X\": -4.612579345703125, \"Y\": -0.9928652048110962, \"LABEL\": \"31.02\"}, {\"X\": -4.133425712585449, \"Y\": -1.1091023683547974, \"LABEL\": \"2.390\"}, {\"X\": 3.523721933364868, \"Y\": -1.0582809448242188, \"LABEL\": \"holbeach\"}, {\"X\": 1.1826785802841187, \"Y\": 0.31948360800743103, \"LABEL\": \"tenuis\"}, {\"X\": 2.259305238723755, \"Y\": 5.951606273651123, \"LABEL\": \"donnie\"}, {\"X\": -6.703404903411865, \"Y\": -3.915257453918457, \"LABEL\": \"mlanghenry\"}, {\"X\": 4.905426502227783, \"Y\": -2.0463409423828125, \"LABEL\": \"schweber\"}, {\"X\": 5.176905632019043, \"Y\": -2.811901569366455, \"LABEL\": \"washita\"}, {\"X\": 2.1978302001953125, \"Y\": 3.32379150390625, \"LABEL\": \"hallford\"}, {\"X\": -2.4404866695404053, \"Y\": -1.1982722282409668, \"LABEL\": \"chromosomal\"}, {\"X\": -0.823937714099884, \"Y\": 1.5456256866455078, \"LABEL\": \"coleridge-taylor\"}, {\"X\": -2.413045883178711, \"Y\": 1.936437964439392, \"LABEL\": \"menifee\"}, {\"X\": 1.117558240890503, \"Y\": 0.4190768301486969, \"LABEL\": \"ruficollis\"}, {\"X\": 0.8993608951568604, \"Y\": 3.918768882751465, \"LABEL\": \"rentzer\"}, {\"X\": 1.8076143264770508, \"Y\": 3.376943826675415, \"LABEL\": \"khoza\"}, {\"X\": -1.6578712463378906, \"Y\": 1.0983892679214478, \"LABEL\": \"altria\"}, {\"X\": -3.344895362854004, \"Y\": -2.0013046264648438, \"LABEL\": \"46-million\"}, {\"X\": 2.3658275604248047, \"Y\": 2.6966116428375244, \"LABEL\": \"fonix\"}, {\"X\": 4.228843688964844, \"Y\": -0.8983476161956787, \"LABEL\": \"meppel\"}, {\"X\": 1.203170657157898, \"Y\": -1.668529987335205, \"LABEL\": \"vympel\"}, {\"X\": 0.9241385459899902, \"Y\": 2.685140609741211, \"LABEL\": \"salvat\"}, {\"X\": -3.621347188949585, \"Y\": -1.7393677234649658, \"LABEL\": \"40b\"}, {\"X\": -3.1989991664886475, \"Y\": -3.263131618499756, \"LABEL\": \"melilla\"}, {\"X\": -7.452267646789551, \"Y\": 2.5797903537750244, \"LABEL\": \"1686\"}, {\"X\": -2.112089157104492, \"Y\": 0.8061411380767822, \"LABEL\": \"bodybuilder\"}, {\"X\": 0.7024179100990295, \"Y\": 3.124779462814331, \"LABEL\": \"chicoine\"}, {\"X\": 1.413185954093933, \"Y\": 5.297164440155029, \"LABEL\": \"lanni\"}, {\"X\": 1.5347753763198853, \"Y\": -2.1925785541534424, \"LABEL\": \"kprp\"}, {\"X\": -1.6419329643249512, \"Y\": -1.7241086959838867, \"LABEL\": \"downscaling\"}, {\"X\": 3.7753794193267822, \"Y\": -0.4522239565849304, \"LABEL\": \"montrouge\"}, {\"X\": 3.345492124557495, \"Y\": 4.13253927230835, \"LABEL\": \"glockner\"}, {\"X\": -4.391262531280518, \"Y\": -1.1443982124328613, \"LABEL\": \"8,570\"}, {\"X\": 2.458021640777588, \"Y\": -0.32010287046432495, \"LABEL\": \"marsonia\"}, {\"X\": -3.3548965454101562, \"Y\": -1.0463453531265259, \"LABEL\": \"trinitrotoluene\"}, {\"X\": 5.807266712188721, \"Y\": -0.9319196939468384, \"LABEL\": \"firuzeh\"}, {\"X\": 0.15644270181655884, \"Y\": 2.0132839679718018, \"LABEL\": \"subramani\"}, {\"X\": 3.5488781929016113, \"Y\": -1.032910704612732, \"LABEL\": \"fidenza\"}, {\"X\": -1.1713650226593018, \"Y\": 0.3774428069591522, \"LABEL\": \"amfar\"}, {\"X\": -0.2741372287273407, \"Y\": -1.9451109170913696, \"LABEL\": \"dunderheaded\"}, {\"X\": -1.0044673681259155, \"Y\": 1.5199384689331055, \"LABEL\": \"\\u00e7atl\\u0131\"}, {\"X\": 2.6112029552459717, \"Y\": 5.1379475593566895, \"LABEL\": \"macci\"}, {\"X\": -0.2665270268917084, \"Y\": 0.4714259207248688, \"LABEL\": \"evz\"}, {\"X\": 3.388570785522461, \"Y\": 0.46336522698402405, \"LABEL\": \"guanghua\"}, {\"X\": -0.1180834099650383, \"Y\": 1.9987952709197998, \"LABEL\": \"camm\"}, {\"X\": 1.4957200288772583, \"Y\": 4.457016944885254, \"LABEL\": \"saggar\"}, {\"X\": 1.9466568231582642, \"Y\": 2.1890716552734375, \"LABEL\": \"torie\"}, {\"X\": -0.17856521904468536, \"Y\": -0.7914535999298096, \"LABEL\": \"pullman-standard\"}, {\"X\": -4.090905666351318, \"Y\": -1.5874700546264648, \"LABEL\": \"3,610\"}, {\"X\": 5.222691535949707, \"Y\": -0.4109313189983368, \"LABEL\": \"\\u015f\\u0131rnak\"}, {\"X\": -4.312832832336426, \"Y\": -1.0786551237106323, \"LABEL\": \"66.45\"}, {\"X\": 1.8657457828521729, \"Y\": 6.789449691772461, \"LABEL\": \"littlewood\"}, {\"X\": 0.914379358291626, \"Y\": 1.6336387395858765, \"LABEL\": \"barashi\"}, {\"X\": 1.5524940490722656, \"Y\": -1.2087583541870117, \"LABEL\": \"peccary\"}, {\"X\": 1.5657382011413574, \"Y\": -1.6303268671035767, \"LABEL\": \"afn\"}, {\"X\": 0.02759709395468235, \"Y\": 4.70298957824707, \"LABEL\": \"dunuwille\"}, {\"X\": 2.8338241577148438, \"Y\": 0.1619587540626526, \"LABEL\": \"chorro\"}, {\"X\": 3.4147186279296875, \"Y\": 5.880030155181885, \"LABEL\": \"chilcutt\"}, {\"X\": 3.3995022773742676, \"Y\": 2.448089122772217, \"LABEL\": \"southam\"}, {\"X\": 0.3160058856010437, \"Y\": 1.2780077457427979, \"LABEL\": \"brainwashes\"}, {\"X\": 0.03162061423063278, \"Y\": -0.12458520382642746, \"LABEL\": \"t'ai\"}, {\"X\": 0.6727770566940308, \"Y\": -0.21026749908924103, \"LABEL\": \"diabolos\"}, {\"X\": -3.227400779724121, \"Y\": 0.8924667239189148, \"LABEL\": \"2069\"}, {\"X\": 3.3348543643951416, \"Y\": -0.32822901010513306, \"LABEL\": \"elefsina\"}, {\"X\": -0.5269487500190735, \"Y\": 1.264857530593872, \"LABEL\": \"pohjonen\"}, {\"X\": 0.4544438421726227, \"Y\": -0.6910552382469177, \"LABEL\": \"sekhmet\"}, {\"X\": -1.9292991161346436, \"Y\": -5.028271198272705, \"LABEL\": \"imprisonment\"}, {\"X\": -7.42979097366333, \"Y\": 2.5576887130737305, \"LABEL\": \"1236\"}, {\"X\": 0.6232296228408813, \"Y\": 1.412614107131958, \"LABEL\": \"hamoui\"}, {\"X\": -1.8367081880569458, \"Y\": 6.115380764007568, \"LABEL\": \"c\\u00e1ndido\"}, {\"X\": -0.21952949464321136, \"Y\": 3.655144214630127, \"LABEL\": \"albertz\"}, {\"X\": -3.577500343322754, \"Y\": -1.9939029216766357, \"LABEL\": \"mws\"}, {\"X\": -0.5547404289245605, \"Y\": 0.13373282551765442, \"LABEL\": \"jtb\"}, {\"X\": -1.1193082332611084, \"Y\": 1.7637113332748413, \"LABEL\": \"ronaldhino\"}, {\"X\": -4.671439170837402, \"Y\": -1.0274168252944946, \"LABEL\": \"89.00\"}, {\"X\": 0.6355153322219849, \"Y\": 5.553779125213623, \"LABEL\": \"kimanthi\"}, {\"X\": -1.5651315450668335, \"Y\": 2.020839214324951, \"LABEL\": \"entero\"}, {\"X\": 2.0649328231811523, \"Y\": 0.7698681354522705, \"LABEL\": \"skud\"}, {\"X\": -0.8449869751930237, \"Y\": 0.13587793707847595, \"LABEL\": \"stereolab\"}, {\"X\": 0.8833796977996826, \"Y\": 3.0154988765716553, \"LABEL\": \"dromgoole\"}, {\"X\": 0.4060751497745514, \"Y\": -0.6508128046989441, \"LABEL\": \"hypocorism\"}, {\"X\": -0.958476722240448, \"Y\": -1.674043893814087, \"LABEL\": \"feeble-minded\"}, {\"X\": 3.5735974311828613, \"Y\": -2.4117729663848877, \"LABEL\": \"penrith\"}, {\"X\": -0.19912602007389069, \"Y\": 2.56687068939209, \"LABEL\": \"al-said\"}, {\"X\": -1.8665103912353516, \"Y\": 3.5646963119506836, \"LABEL\": \"basbug\"}, {\"X\": -0.06270281225442886, \"Y\": 1.114816427230835, \"LABEL\": \"goni\"}, {\"X\": 0.17753994464874268, \"Y\": 0.5713231563568115, \"LABEL\": \"dest\"}, {\"X\": -5.512340545654297, \"Y\": -1.8977735042572021, \"LABEL\": \"57.8\"}, {\"X\": -4.7523016929626465, \"Y\": -3.2417988777160645, \"LABEL\": \"330\"}, {\"X\": 2.2255003452301025, \"Y\": 4.723818778991699, \"LABEL\": \"kikuo\"}, {\"X\": 2.1011416912078857, \"Y\": -2.014415740966797, \"LABEL\": \"d7\"}, {\"X\": -1.6542127132415771, \"Y\": 5.419701099395752, \"LABEL\": \"rios\"}, {\"X\": -0.8843289017677307, \"Y\": -0.4885609745979309, \"LABEL\": \"self-advocacy\"}, {\"X\": -0.16366253793239594, \"Y\": 2.2707338333129883, \"LABEL\": \"aivar\"}, {\"X\": -1.49801504611969, \"Y\": -2.28403902053833, \"LABEL\": \"snakebite\"}, {\"X\": -1.8516291379928589, \"Y\": 2.176199197769165, \"LABEL\": \"nhu\"}, {\"X\": 5.677759170532227, \"Y\": -0.8528543710708618, \"LABEL\": \"hezarjarib\"}, {\"X\": -0.5475430488586426, \"Y\": -0.5460083484649658, \"LABEL\": \"skatalites\"}, {\"X\": -5.745991230010986, \"Y\": -0.2783220410346985, \"LABEL\": \"1-1-2\"}, {\"X\": 2.5262417793273926, \"Y\": -2.972529411315918, \"LABEL\": \"coralline\"}, {\"X\": 3.4560279846191406, \"Y\": -0.347513884305954, \"LABEL\": \"mora\\u010da\"}, {\"X\": -4.9282660484313965, \"Y\": 3.3338029384613037, \"LABEL\": \"dagblad\"}, {\"X\": 3.4791157245635986, \"Y\": 0.909805953502655, \"LABEL\": \"pelabuhan\"}, {\"X\": -4.643506050109863, \"Y\": 1.8866214752197266, \"LABEL\": \"8pm\"}, {\"X\": 3.055575370788574, \"Y\": 2.1583847999572754, \"LABEL\": \"ra\\u0161ka\"}, {\"X\": 4.375154972076416, \"Y\": 1.0977730751037598, \"LABEL\": \"ant\\u00e1rtica\"}, {\"X\": -0.6934854984283447, \"Y\": -2.4229440689086914, \"LABEL\": \"two-tenths\"}, {\"X\": -2.978689432144165, \"Y\": 4.050018310546875, \"LABEL\": \"wuerttemburg\"}, {\"X\": 3.5327210426330566, \"Y\": -0.7219035029411316, \"LABEL\": \"gyeongbokgung\"}, {\"X\": 0.7056074738502502, \"Y\": 3.9112048149108887, \"LABEL\": \"struk\"}, {\"X\": -2.991410970687866, \"Y\": -5.551970958709717, \"LABEL\": \"littered\"}, {\"X\": -4.342311859130859, \"Y\": -5.53277587890625, \"LABEL\": \"baddesley\"}, {\"X\": 0.18807312846183777, \"Y\": 3.1291067600250244, \"LABEL\": \"horcher\"}, {\"X\": 1.269296646118164, \"Y\": -0.9074307680130005, \"LABEL\": \"prayerbook\"}, {\"X\": 2.1623122692108154, \"Y\": -1.1172086000442505, \"LABEL\": \"keenans\"}, {\"X\": 4.529615879058838, \"Y\": -0.9250697493553162, \"LABEL\": \"freudenstadt\"}, {\"X\": -0.14670449495315552, \"Y\": 3.0370514392852783, \"LABEL\": \"dalling\"}, {\"X\": -2.6628506183624268, \"Y\": -2.7775464057922363, \"LABEL\": \"footfall\"}, {\"X\": -2.024794340133667, \"Y\": 2.7487223148345947, \"LABEL\": \"1-aug\"}, {\"X\": -4.6331329345703125, \"Y\": 0.2518884539604187, \"LABEL\": \"snet\"}, {\"X\": -3.651001453399658, \"Y\": -1.5084545612335205, \"LABEL\": \"787,000\"}, {\"X\": 5.863266468048096, \"Y\": -2.022118091583252, \"LABEL\": \"th\\u00e1p\"}, {\"X\": 3.184652328491211, \"Y\": -4.467911243438721, \"LABEL\": \"mladi\\u0107\"}, {\"X\": 3.3478918075561523, \"Y\": -1.6926037073135376, \"LABEL\": \"hednesford\"}, {\"X\": -0.2718348503112793, \"Y\": -1.8445899486541748, \"LABEL\": \"politico-religious\"}, {\"X\": -3.472795248031616, \"Y\": 3.5467123985290527, \"LABEL\": \"avasin\"}, {\"X\": 3.7447280883789062, \"Y\": 3.3955914974212646, \"LABEL\": \"bva\"}, {\"X\": -1.4572620391845703, \"Y\": 3.597264051437378, \"LABEL\": \"meskini\"}, {\"X\": -3.9714670181274414, \"Y\": -2.7610490322113037, \"LABEL\": \"3,517\"}, {\"X\": -0.8180509805679321, \"Y\": 4.26654577255249, \"LABEL\": \"opfer\"}, {\"X\": -0.5521010756492615, \"Y\": -5.507750511169434, \"LABEL\": \"unknowingly\"}, {\"X\": -2.7812352180480957, \"Y\": -4.2169342041015625, \"LABEL\": \"hurstbourne\"}, {\"X\": 0.17101992666721344, \"Y\": 1.9729317426681519, \"LABEL\": \"essa\"}, {\"X\": 2.242009401321411, \"Y\": 2.478893756866455, \"LABEL\": \"jode\"}, {\"X\": 4.573306560516357, \"Y\": 0.615931510925293, \"LABEL\": \"verde\"}, {\"X\": -0.3210006356239319, \"Y\": -3.5170583724975586, \"LABEL\": \"quickness\"}, {\"X\": 0.6402909755706787, \"Y\": -1.2438583374023438, \"LABEL\": \"t-6\"}, {\"X\": 2.2279293537139893, \"Y\": 0.6016258597373962, \"LABEL\": \"masia\"}, {\"X\": 0.9420319199562073, \"Y\": 3.7275960445404053, \"LABEL\": \"wtoo\"}, {\"X\": -4.669874668121338, \"Y\": -4.637800693511963, \"LABEL\": \"tohir\"}, {\"X\": -1.2484972476959229, \"Y\": -1.7843539714813232, \"LABEL\": \"rambos\"}, {\"X\": 1.9920676946640015, \"Y\": -5.058928966522217, \"LABEL\": \"medium-sized\"}, {\"X\": 0.8397679328918457, \"Y\": 7.089618682861328, \"LABEL\": \"araoz\"}, {\"X\": -2.911407709121704, \"Y\": -1.123810887336731, \"LABEL\": \"naps\"}, {\"X\": 2.334446907043457, \"Y\": 1.9502458572387695, \"LABEL\": \"billard\"}, {\"X\": -4.1433868408203125, \"Y\": 2.9343864917755127, \"LABEL\": \"agencia\"}, {\"X\": -2.4233767986297607, \"Y\": -0.5663067102432251, \"LABEL\": \"muons\"}, {\"X\": -3.3758397102355957, \"Y\": 4.131444454193115, \"LABEL\": \"kalai\"}, {\"X\": 0.49973076581954956, \"Y\": -0.036151718348264694, \"LABEL\": \"3261\"}, {\"X\": 2.4085769653320312, \"Y\": 1.4509458541870117, \"LABEL\": \"brahima\"}, {\"X\": -0.3151857554912567, \"Y\": -2.4499547481536865, \"LABEL\": \"pan-arabist\"}, {\"X\": -0.5515539050102234, \"Y\": 3.9675467014312744, \"LABEL\": \"shalonda\"}, {\"X\": -4.752068519592285, \"Y\": -1.0240126848220825, \"LABEL\": \"21.38\"}, {\"X\": -5.888032913208008, \"Y\": -3.204486846923828, \"LABEL\": \"lc\"}, {\"X\": -0.8817310929298401, \"Y\": -3.219783067703247, \"LABEL\": \"unknowable\"}, {\"X\": 1.3170033693313599, \"Y\": 1.8580749034881592, \"LABEL\": \"shengping\"}, {\"X\": 2.970925807952881, \"Y\": 4.153968334197998, \"LABEL\": \"mizanur\"}, {\"X\": 1.7532429695129395, \"Y\": 0.3152710199356079, \"LABEL\": \"poedjangga\"}, {\"X\": -2.7759881019592285, \"Y\": 6.703592300415039, \"LABEL\": \"chono\"}, {\"X\": 0.7347766757011414, \"Y\": 1.4201710224151611, \"LABEL\": \"wazzan\"}, {\"X\": 1.4238401651382446, \"Y\": 1.347678542137146, \"LABEL\": \"manorama\"}, {\"X\": -1.7994152307510376, \"Y\": 5.103199005126953, \"LABEL\": \"ruef\"}, {\"X\": -2.1791608333587646, \"Y\": 3.470081090927124, \"LABEL\": \"uil\"}, {\"X\": -0.039295848459005356, \"Y\": 2.206578493118286, \"LABEL\": \"a.j\"}, {\"X\": -4.317159652709961, \"Y\": 2.2215189933776855, \"LABEL\": \"showtime\"}, {\"X\": -3.426652193069458, \"Y\": 4.154726505279541, \"LABEL\": \"chay\"}, {\"X\": -3.7006664276123047, \"Y\": -1.5524702072143555, \"LABEL\": \"7,020\"}, {\"X\": -4.597995281219482, \"Y\": -3.014115571975708, \"LABEL\": \"724\"}, {\"X\": -4.687491416931152, \"Y\": -0.9789361953735352, \"LABEL\": \"44.59\"}, {\"X\": 2.3272581100463867, \"Y\": 0.06363238394260406, \"LABEL\": \"marylhurst\"}, {\"X\": -1.660172700881958, \"Y\": 0.3271981477737427, \"LABEL\": \"tianwei\"}, {\"X\": 0.8179940581321716, \"Y\": 1.9314082860946655, \"LABEL\": \"baniszewski\"}, {\"X\": 2.881251573562622, \"Y\": -1.5341678857803345, \"LABEL\": \"ballinderry\"}, {\"X\": -3.2009918689727783, \"Y\": -4.831730842590332, \"LABEL\": \"assemblages\"}, {\"X\": -1.8154487609863281, \"Y\": 3.4010369777679443, \"LABEL\": \"dixit\"}, {\"X\": -3.695061445236206, \"Y\": -2.501439332962036, \"LABEL\": \"2,896\"}, {\"X\": 1.2013055086135864, \"Y\": 1.7806514501571655, \"LABEL\": \"barkman\"}, {\"X\": -0.21473917365074158, \"Y\": 0.6920617818832397, \"LABEL\": \"s/mileage\"}, {\"X\": -4.854204177856445, \"Y\": -3.363603353500366, \"LABEL\": \"5,500\"}, {\"X\": 0.4004885256290436, \"Y\": 0.9280962944030762, \"LABEL\": \"well-disposed\"}, {\"X\": -1.9993475675582886, \"Y\": 0.8195539116859436, \"LABEL\": \"basketballer\"}, {\"X\": 3.2251839637756348, \"Y\": -5.352944850921631, \"LABEL\": \"cardioprotective\"}, {\"X\": 2.673241376876831, \"Y\": -0.32554930448532104, \"LABEL\": \"macumba\"}, {\"X\": -0.8811262845993042, \"Y\": 4.686092376708984, \"LABEL\": \"isaiah\"}, {\"X\": -2.0665884017944336, \"Y\": -2.2212369441986084, \"LABEL\": \"abstinence\"}, {\"X\": 0.3286263346672058, \"Y\": 1.8490452766418457, \"LABEL\": \"aferwerki\"}, {\"X\": 2.6611998081207275, \"Y\": -2.9034578800201416, \"LABEL\": \"jindalee\"}, {\"X\": -0.37661099433898926, \"Y\": 1.1641827821731567, \"LABEL\": \"bijoy\"}, {\"X\": -0.2513246536254883, \"Y\": -2.9621059894561768, \"LABEL\": \"modernist\"}, {\"X\": -0.30504804849624634, \"Y\": -1.8181774616241455, \"LABEL\": \"regge\"}, {\"X\": -1.5480915307998657, \"Y\": 1.2136802673339844, \"LABEL\": \"rj\"}, {\"X\": 1.426699161529541, \"Y\": -1.6216065883636475, \"LABEL\": \"ncai\"}, {\"X\": 0.044943295419216156, \"Y\": 5.782005310058594, \"LABEL\": \"beamer\"}, {\"X\": 1.3404556512832642, \"Y\": -0.9705528616905212, \"LABEL\": \"kahun\"}, {\"X\": 1.013394832611084, \"Y\": -4.02754545211792, \"LABEL\": \"subtropical\"}, {\"X\": 3.1984148025512695, \"Y\": 2.1429505348205566, \"LABEL\": \"kempen\"}, {\"X\": 0.1333259791135788, \"Y\": 0.22291570901870728, \"LABEL\": \"anthranilate\"}, {\"X\": 0.26122039556503296, \"Y\": 2.8263819217681885, \"LABEL\": \"gansey\"}, {\"X\": 0.10617092996835709, \"Y\": 2.7048840522766113, \"LABEL\": \"mclee\"}, {\"X\": 2.010695219039917, \"Y\": 1.3812333345413208, \"LABEL\": \"64-year\"}, {\"X\": -1.5587246417999268, \"Y\": -6.586980819702148, \"LABEL\": \"generation\"}, {\"X\": -2.764429807662964, \"Y\": 2.675036907196045, \"LABEL\": \"ro\"}, {\"X\": 1.1135083436965942, \"Y\": 4.939592361450195, \"LABEL\": \"sopko\"}, {\"X\": -5.628089904785156, \"Y\": -1.9882786273956299, \"LABEL\": \"84.4\"}, {\"X\": -1.6576473712921143, \"Y\": -2.6805050373077393, \"LABEL\": \"sewers\"}, {\"X\": 3.742060661315918, \"Y\": -0.41061800718307495, \"LABEL\": \"portree\"}, {\"X\": 0.5123251080513, \"Y\": -5.796205520629883, \"LABEL\": \"tomato\"}, {\"X\": 0.3433811068534851, \"Y\": -6.013747215270996, \"LABEL\": \"recipes\"}, {\"X\": -0.4667779207229614, \"Y\": -3.407663345336914, \"LABEL\": \"quietude\"}, {\"X\": -3.491434335708618, \"Y\": -0.8633008003234863, \"LABEL\": \"rpi\"}, {\"X\": -0.5775434970855713, \"Y\": 3.1629674434661865, \"LABEL\": \"kensey\"}, {\"X\": -0.6060641407966614, \"Y\": 2.8016316890716553, \"LABEL\": \"c.ss.r\"}, {\"X\": 2.170647382736206, \"Y\": 2.513173818588257, \"LABEL\": \"calheiros\"}, {\"X\": 3.439511775970459, \"Y\": -0.8320602774620056, \"LABEL\": \"barcelonnette\"}, {\"X\": 3.3650920391082764, \"Y\": -0.6433925032615662, \"LABEL\": \"niyamgiri\"}, {\"X\": 0.6619861125946045, \"Y\": 3.0258684158325195, \"LABEL\": \"genis\"}, {\"X\": -4.703845024108887, \"Y\": -0.9845815300941467, \"LABEL\": \"61.78\"}, {\"X\": -1.3558646440505981, \"Y\": -1.500129222869873, \"LABEL\": \"moulting\"}, {\"X\": -0.5803216099739075, \"Y\": -1.3789420127868652, \"LABEL\": \"old-line\"}, {\"X\": -3.1125893592834473, \"Y\": -2.5325610637664795, \"LABEL\": \"fencers\"}, {\"X\": 0.8086843490600586, \"Y\": -7.089061260223389, \"LABEL\": \"30-180\"}, {\"X\": 1.3198738098144531, \"Y\": 2.474742889404297, \"LABEL\": \"barza\"}, {\"X\": 5.831934452056885, \"Y\": -0.9974708557128906, \"LABEL\": \"ph\\u00fa\"}, {\"X\": -0.70344477891922, \"Y\": 3.0898125171661377, \"LABEL\": \"muello\"}, {\"X\": 4.056872844696045, \"Y\": -1.8084510564804077, \"LABEL\": \"kwamakhutha\"}, {\"X\": -4.394976615905762, \"Y\": -5.568548202514648, \"LABEL\": \"ferriby\"}, {\"X\": -0.7874332666397095, \"Y\": 3.2631561756134033, \"LABEL\": \"skloot\"}, {\"X\": 1.6269975900650024, \"Y\": 0.04090827703475952, \"LABEL\": \"antiquus\"}, {\"X\": -1.9103035926818848, \"Y\": -1.3515584468841553, \"LABEL\": \"mother-to-child\"}, {\"X\": -0.7563278675079346, \"Y\": 5.75878381729126, \"LABEL\": \"maintaka\"}, {\"X\": -0.19698669016361237, \"Y\": 5.4497480392456055, \"LABEL\": \"barad\"}, {\"X\": 0.8114456534385681, \"Y\": -0.5451813340187073, \"LABEL\": \"sundew\"}, {\"X\": 5.684423923492432, \"Y\": 0.46585261821746826, \"LABEL\": \"mbanza\"}, {\"X\": -2.104910135269165, \"Y\": 5.403429985046387, \"LABEL\": \"obrador\"}, {\"X\": -1.8073114156723022, \"Y\": -6.618407249450684, \"LABEL\": \"falter\"}, {\"X\": 2.241636276245117, \"Y\": 3.666487455368042, \"LABEL\": \"hutcheson\"}, {\"X\": 0.8911027312278748, \"Y\": -2.0084354877471924, \"LABEL\": \"stockcar\"}, {\"X\": 2.863140344619751, \"Y\": 1.7331292629241943, \"LABEL\": \"12-ranked\"}, {\"X\": -4.097416877746582, \"Y\": -1.1103874444961548, \"LABEL\": \"61.62\"}, {\"X\": -0.39999449253082275, \"Y\": 0.08027084916830063, \"LABEL\": \"kirlian\"}, {\"X\": -1.6927298307418823, \"Y\": -1.4376122951507568, \"LABEL\": \"test-bed\"}, {\"X\": 0.9349678754806519, \"Y\": -1.243152141571045, \"LABEL\": \"pheasantry\"}, {\"X\": 4.669468879699707, \"Y\": 3.2502243518829346, \"LABEL\": \"chirasevenupraphand\"}, {\"X\": 3.2860772609710693, \"Y\": 4.809737682342529, \"LABEL\": \"hanno\"}, {\"X\": -0.07896825671195984, \"Y\": 0.06302465498447418, \"LABEL\": \"apro\"}, {\"X\": -0.35890477895736694, \"Y\": -0.18659630417823792, \"LABEL\": \"cfaa\"}, {\"X\": 3.5526716709136963, \"Y\": 3.3490049839019775, \"LABEL\": \"nzier\"}, {\"X\": -2.1456425189971924, \"Y\": -2.042128324508667, \"LABEL\": \"logograms\"}, {\"X\": 5.118646144866943, \"Y\": -2.863966703414917, \"LABEL\": \"cahaba\"}, {\"X\": -0.7539385557174683, \"Y\": 3.0576910972595215, \"LABEL\": \"christe\"}, {\"X\": -4.045970916748047, \"Y\": 1.0890880823135376, \"LABEL\": \"1960-1961\"}, {\"X\": 4.685543537139893, \"Y\": -2.269106149673462, \"LABEL\": \"somatosensory\"}, {\"X\": -6.763393402099609, \"Y\": -3.914860963821411, \"LABEL\": \"tbaxter\"}, {\"X\": 2.085242509841919, \"Y\": 1.0000041723251343, \"LABEL\": \"nariz\"}, {\"X\": 0.24069029092788696, \"Y\": 0.5069654583930969, \"LABEL\": \"korean-chinese\"}, {\"X\": 1.5857186317443848, \"Y\": -3.6092991828918457, \"LABEL\": \"galactosemia\"}, {\"X\": 0.17876935005187988, \"Y\": -3.3034205436706543, \"LABEL\": \"tilework\"}, {\"X\": 2.8920114040374756, \"Y\": 4.300528049468994, \"LABEL\": \"ranicki\"}, {\"X\": 0.530769944190979, \"Y\": -2.8919262886047363, \"LABEL\": \"foamy\"}, {\"X\": 4.618194103240967, \"Y\": -2.1996452808380127, \"LABEL\": \"mainstem\"}, {\"X\": -1.6504547595977783, \"Y\": 2.262265205383301, \"LABEL\": \"grage\"}, {\"X\": 2.0527026653289795, \"Y\": -3.726086139678955, \"LABEL\": \"h5ni\"}, {\"X\": 3.670633554458618, \"Y\": -1.274948239326477, \"LABEL\": \"arezzo\"}, {\"X\": -0.4370591342449188, \"Y\": -2.1249191761016846, \"LABEL\": \"benzophenone\"}, {\"X\": -5.369718551635742, \"Y\": 2.5687482357025146, \"LABEL\": \"ottey\"}, {\"X\": 0.7168267369270325, \"Y\": 3.4592559337615967, \"LABEL\": \"o'chester\"}, {\"X\": 3.500393867492676, \"Y\": -0.9199804067611694, \"LABEL\": \"ladysmith\"}, {\"X\": -1.0942752361297607, \"Y\": -0.32122349739074707, \"LABEL\": \"quatrain\"}, {\"X\": -1.8127362728118896, \"Y\": 3.2868459224700928, \"LABEL\": \"lugard\"}, {\"X\": -4.121970176696777, \"Y\": 2.6836183071136475, \"LABEL\": \"diganta\"}, {\"X\": 2.5820887088775635, \"Y\": 0.9497254490852356, \"LABEL\": \"parisienne\"}, {\"X\": 2.311403274536133, \"Y\": -6.085269927978516, \"LABEL\": \"1-for-20\"}, {\"X\": 1.3428709506988525, \"Y\": 3.247605323791504, \"LABEL\": \"shilansky\"}, {\"X\": -3.3485524654388428, \"Y\": 1.1234428882598877, \"LABEL\": \"all-region\"}, {\"X\": -4.402211666107178, \"Y\": -0.6135270595550537, \"LABEL\": \"shs\"}, {\"X\": 1.6019272804260254, \"Y\": 4.479060649871826, \"LABEL\": \"mulford\"}, {\"X\": -7.468056678771973, \"Y\": 2.6066818237304688, \"LABEL\": \"1825\"}, {\"X\": -2.267620086669922, \"Y\": -3.531733751296997, \"LABEL\": \"lockheed\"}, {\"X\": -5.463967323303223, \"Y\": -1.9828447103500366, \"LABEL\": \"136.1\"}, {\"X\": -0.8176723122596741, \"Y\": 4.757254123687744, \"LABEL\": \"sartori\"}, {\"X\": 3.209603786468506, \"Y\": 0.7422327399253845, \"LABEL\": \"puerta\"}, {\"X\": 0.3483690023422241, \"Y\": -2.0002541542053223, \"LABEL\": \"mudbath\"}, {\"X\": 1.7576370239257812, \"Y\": 3.531216859817505, \"LABEL\": \"thorsell\"}, {\"X\": -4.059535026550293, \"Y\": -2.0446176528930664, \"LABEL\": \"344,000\"}, {\"X\": -0.40445947647094727, \"Y\": 7.845881462097168, \"LABEL\": \"boscov\"}, {\"X\": 0.24694490432739258, \"Y\": 4.688421726226807, \"LABEL\": \"abshier\"}, {\"X\": 1.1746004819869995, \"Y\": 0.2723824083805084, \"LABEL\": \"lambis\"}, {\"X\": -2.2083311080932617, \"Y\": 2.6890780925750732, \"LABEL\": \"mdul\"}, {\"X\": 2.4515388011932373, \"Y\": 2.448873281478882, \"LABEL\": \"toce\"}, {\"X\": 3.920036792755127, \"Y\": -1.1604593992233276, \"LABEL\": \"dungarvan\"}, {\"X\": 3.0385608673095703, \"Y\": -1.0168583393096924, \"LABEL\": \"nantlle\"}, {\"X\": 0.18828895688056946, \"Y\": 3.692013740539551, \"LABEL\": \"reuter\"}, {\"X\": 6.088454246520996, \"Y\": 1.3121216297149658, \"LABEL\": \"kenworth\"}, {\"X\": -0.6043010950088501, \"Y\": -3.397913694381714, \"LABEL\": \"summitry\"}, {\"X\": 5.022923469543457, \"Y\": -2.680340051651001, \"LABEL\": \"gasconade\"}, {\"X\": -0.11323712021112442, \"Y\": -2.021397113800049, \"LABEL\": \"chapeau\"}, {\"X\": -3.3167386054992676, \"Y\": 1.6987463235855103, \"LABEL\": \"maurya\"}, {\"X\": 5.787333011627197, \"Y\": 0.8513563275337219, \"LABEL\": \"paleographic\"}, {\"X\": -2.7281038761138916, \"Y\": 4.131550312042236, \"LABEL\": \"maturino\"}, {\"X\": 3.286886692047119, \"Y\": 0.23861591517925262, \"LABEL\": \"hellinikon\"}, {\"X\": 1.540741205215454, \"Y\": 2.932509183883667, \"LABEL\": \"michot\"}, {\"X\": 1.0510132312774658, \"Y\": 5.701760292053223, \"LABEL\": \"jibril\"}, {\"X\": 1.4524427652359009, \"Y\": -1.833793044090271, \"LABEL\": \"raou\"}, {\"X\": 0.02541918121278286, \"Y\": 3.406329870223999, \"LABEL\": \"fowkes\"}, {\"X\": -3.8948240280151367, \"Y\": -2.5661814212799072, \"LABEL\": \"2,443\"}, {\"X\": 1.7977958917617798, \"Y\": -1.6031419038772583, \"LABEL\": \"nwbl\"}, {\"X\": -2.181586980819702, \"Y\": 4.368649005889893, \"LABEL\": \"hefter\"}, {\"X\": 5.092470645904541, \"Y\": -1.6017720699310303, \"LABEL\": \"gullane\"}, {\"X\": 1.712112545967102, \"Y\": -0.32797762751579285, \"LABEL\": \"gata\"}, {\"X\": -5.549430847167969, \"Y\": -2.5431816577911377, \"LABEL\": \"makeup\"}, {\"X\": -4.045392036437988, \"Y\": -1.2729451656341553, \"LABEL\": \"6.5625\"}, {\"X\": 0.4588247537612915, \"Y\": 2.693142890930176, \"LABEL\": \"desrochers\"}, {\"X\": 1.5448964834213257, \"Y\": -2.491229772567749, \"LABEL\": \"fndd\"}, {\"X\": 2.468534231185913, \"Y\": 4.964800834655762, \"LABEL\": \"perlstein\"}, {\"X\": 2.452259063720703, \"Y\": 1.1419130563735962, \"LABEL\": \"l'herbe\"}, {\"X\": 1.1242297887802124, \"Y\": 5.240325450897217, \"LABEL\": \"lowy\"}, {\"X\": -1.653107762336731, \"Y\": -1.9385015964508057, \"LABEL\": \"oceangoing\"}, {\"X\": 0.9978818893432617, \"Y\": 4.174007892608643, \"LABEL\": \"bouchenaki\"}, {\"X\": -5.3974175453186035, \"Y\": -5.022475719451904, \"LABEL\": \"aprile\"}, {\"X\": -2.760507822036743, \"Y\": 5.348100185394287, \"LABEL\": \"belsue\"}, {\"X\": 0.4035775661468506, \"Y\": 4.023531913757324, \"LABEL\": \"pi\\u00ebch\"}, {\"X\": 0.3789336085319519, \"Y\": 0.19295907020568848, \"LABEL\": \"auran\"}, {\"X\": -0.5942087173461914, \"Y\": -7.364913463592529, \"LABEL\": \"recreation\"}, {\"X\": 5.950986385345459, \"Y\": 3.0937492847442627, \"LABEL\": \"medicos\"}, {\"X\": -4.308138847351074, \"Y\": 6.675012588500977, \"LABEL\": \"kimi\"}, {\"X\": 2.8733294010162354, \"Y\": 6.5592942237854, \"LABEL\": \"mcnair\"}, {\"X\": -6.308288097381592, \"Y\": -0.1779879331588745, \"LABEL\": \"9-0\"}, {\"X\": 4.1020612716674805, \"Y\": -1.667693853378296, \"LABEL\": \"comox\"}, {\"X\": -0.5813482403755188, \"Y\": -3.3511905670166016, \"LABEL\": \"aggrandizement\"}, {\"X\": -0.04112810641527176, \"Y\": -3.4067485332489014, \"LABEL\": \"mwata\"}, {\"X\": 1.3679808378219604, \"Y\": 4.610035419464111, \"LABEL\": \"allmand\"}, {\"X\": 1.578622579574585, \"Y\": 0.48695802688598633, \"LABEL\": \"siwash\"}, {\"X\": -1.0464469194412231, \"Y\": 5.382779121398926, \"LABEL\": \"car\\u00edas\"}, {\"X\": 0.8830556869506836, \"Y\": 0.9543860554695129, \"LABEL\": \"baatar\"}, {\"X\": 0.6399301886558533, \"Y\": 2.654249906539917, \"LABEL\": \"petronijevic\"}, {\"X\": 0.16295748949050903, \"Y\": -1.6770484447479248, \"LABEL\": \"polarizer\"}, {\"X\": -1.4341176748275757, \"Y\": -3.008028030395508, \"LABEL\": \"swoops\"}, {\"X\": 0.856813907623291, \"Y\": -0.5738729238510132, \"LABEL\": \"raga\"}, {\"X\": 2.1498661041259766, \"Y\": 5.893796443939209, \"LABEL\": \"forsythe\"}, {\"X\": 3.5831868648529053, \"Y\": -2.2905595302581787, \"LABEL\": \"hitra\"}, {\"X\": -0.45258283615112305, \"Y\": 3.7049572467803955, \"LABEL\": \"anthon\"}, {\"X\": -4.533746242523193, \"Y\": -1.1870174407958984, \"LABEL\": \"17.00\"}, {\"X\": 0.8909214735031128, \"Y\": -1.1804064512252808, \"LABEL\": \"hypanthium\"}, {\"X\": 1.6669498682022095, \"Y\": 4.2456769943237305, \"LABEL\": \"eberling\"}, {\"X\": 0.5986993312835693, \"Y\": 5.864846706390381, \"LABEL\": \"guofang\"}, {\"X\": 0.45761433243751526, \"Y\": 3.4133458137512207, \"LABEL\": \"jaswinder\"}, {\"X\": -1.850172758102417, \"Y\": 2.694093942642212, \"LABEL\": \"rf5\"}, {\"X\": 4.08656644821167, \"Y\": 0.5741473436355591, \"LABEL\": \"virgenes\"}, {\"X\": -0.24231237173080444, \"Y\": 2.431746482849121, \"LABEL\": \"fardon\"}, {\"X\": -5.413594722747803, \"Y\": -2.029160976409912, \"LABEL\": \"200.1\"}, {\"X\": -4.755273818969727, \"Y\": -0.9858924150466919, \"LABEL\": \"24.71\"}, {\"X\": 4.156581878662109, \"Y\": 2.458941698074341, \"LABEL\": \"ppd.\"}, {\"X\": -1.8973329067230225, \"Y\": -4.401849746704102, \"LABEL\": \"extermination\"}, {\"X\": 0.6101857423782349, \"Y\": 2.3627235889434814, \"LABEL\": \"teixiera\"}, {\"X\": -6.774021625518799, \"Y\": -3.884448766708374, \"LABEL\": \"0707\"}, {\"X\": -1.2042406797409058, \"Y\": 0.4444080591201782, \"LABEL\": \"y7\"}, {\"X\": -3.479682445526123, \"Y\": 4.1222825050354, \"LABEL\": \"riz\"}, {\"X\": 2.401961088180542, \"Y\": 3.9515135288238525, \"LABEL\": \"pakzad\"}, {\"X\": -2.3872671127319336, \"Y\": 4.221264362335205, \"LABEL\": \"chull\"}, {\"X\": -2.9266774654388428, \"Y\": -0.10867045074701309, \"LABEL\": \"krl\"}, {\"X\": 2.8894286155700684, \"Y\": -0.7346170544624329, \"LABEL\": \"fremington\"}, {\"X\": -1.4699244499206543, \"Y\": 1.254220724105835, \"LABEL\": \"step-siblings\"}, {\"X\": 2.4886624813079834, \"Y\": -0.17448942363262177, \"LABEL\": \"saint-marcel\"}, {\"X\": 0.6181434392929077, \"Y\": -3.846590757369995, \"LABEL\": \"disorganized\"}, {\"X\": -3.1595468521118164, \"Y\": 3.217925548553467, \"LABEL\": \"courtin\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='LABEL'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a21c2f",
   "metadata": {},
   "source": [
    "### Other relationships\n",
    "\n",
    "Beyond cosine similarity, there are other word relationships to explore via vector space math. For example, one way of modeling something like a _concept_ is to think about what other concepts comprise it. In other words: what plus what creates a new concept? Could we identify concepts by adding together vectors to create a new vector? Which words would this new vector be closest to in the vector space? Using the `.similar_by_vector()` method, we can find out.\n",
    "\n",
    "```{margin} What this loop does\n",
    "For each concept in our `concepts` dictionary:\n",
    "\n",
    "1. Get its associated pair of words\n",
    "2. Query the model for those words' vectors and add them together to create a new vector\n",
    "3. Find the most similar words to this new vector\n",
    "4. Use a dataframe to display the results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432fc44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'sand' + 'ocean' (for 'beach')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sand</td>\n",
       "      <td>0.845458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocean</td>\n",
       "      <td>0.845268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.687682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaches</td>\n",
       "      <td>0.667521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waters</td>\n",
       "      <td>0.664894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coastal</td>\n",
       "      <td>0.632485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>water</td>\n",
       "      <td>0.618701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coast</td>\n",
       "      <td>0.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dunes</td>\n",
       "      <td>0.599333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>surface</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0     sand  0.845458\n",
       "1    ocean  0.845268\n",
       "2      sea  0.687682\n",
       "3  beaches  0.667521\n",
       "4   waters  0.664894\n",
       "5  coastal  0.632485\n",
       "6    water  0.618701\n",
       "7    coast  0.604373\n",
       "8    dunes  0.599333\n",
       "9  surface  0.597545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'vacation' + 'room' (for 'hotel')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vacation</td>\n",
       "      <td>0.823460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>room</td>\n",
       "      <td>0.810719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rooms</td>\n",
       "      <td>0.704233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>0.658199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel</td>\n",
       "      <td>0.647865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dining</td>\n",
       "      <td>0.634925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stay</td>\n",
       "      <td>0.617807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apartment</td>\n",
       "      <td>0.616495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>staying</td>\n",
       "      <td>0.615182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>home</td>\n",
       "      <td>0.606009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0   vacation  0.823460\n",
       "1       room  0.810719\n",
       "2      rooms  0.704233\n",
       "3    bedroom  0.658199\n",
       "4      hotel  0.647865\n",
       "5     dining  0.634925\n",
       "6       stay  0.617807\n",
       "7  apartment  0.616495\n",
       "8    staying  0.615182\n",
       "9       home  0.606009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tokens to 'air' + 'car' (for 'airplane')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air</td>\n",
       "      <td>0.827957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>0.810086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>0.719382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cars</td>\n",
       "      <td>0.671697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.645963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vehicles</td>\n",
       "      <td>0.637166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>passenger</td>\n",
       "      <td>0.625993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aircraft</td>\n",
       "      <td>0.624820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jet</td>\n",
       "      <td>0.618584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.610345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD     SCORE\n",
       "0        air  0.827957\n",
       "1        car  0.810086\n",
       "2    vehicle  0.719382\n",
       "3       cars  0.671697\n",
       "4      truck  0.645963\n",
       "5   vehicles  0.637166\n",
       "6  passenger  0.625993\n",
       "7   aircraft  0.624820\n",
       "8        jet  0.618584\n",
       "9   airplane  0.610345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concepts = {'beach': ('sand', 'ocean'), 'hotel': ('vacation', 'room'), 'airplane': ('air', 'car')}\n",
    "for concept in concepts:\n",
    "    pair = concepts[concept]\n",
    "    generated_concept = model[pair[0]] + model[pair[1]]\n",
    "    similarities = model.similar_by_vector(generated_concept)\n",
    "    print(f\"Most similar tokens to '{pair[0]}' + '{pair[1]}' (for '{concept}')\")\n",
    "    df = pd.DataFrame(similarities, columns=['WORD', 'SCORE'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b64296",
   "metadata": {},
   "source": [
    "Not bad! Our target concept isn't the most similar word for either of these examples, but it's in the top 10.\n",
    "\n",
    "Most famously, word embeddings enable quasi-logical reasoning. Though, as we mentioned earlier, relationships between antonyms and synoyms do not necessarily map to a vector space, certain analogies do – at least under the right circumstances, and with particular training data. The logic here is that we identify a relationship between two words and we subtract one of those words' vectors from the other. To that new vector we add in a vector for a target word, which forms the analogy. Querying for the word closest to this modified vector should produce a similar relation between the result and the target word as that between the original pair.\n",
    "\n",
    "Here, we ask: \"strong is to stronger what clear is to X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae0fa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easier</td>\n",
       "      <td>0.633451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>should</td>\n",
       "      <td>0.630116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clearer</td>\n",
       "      <td>0.621850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better</td>\n",
       "      <td>0.602637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>must</td>\n",
       "      <td>0.601793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>need</td>\n",
       "      <td>0.595918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meant</td>\n",
       "      <td>0.594797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>harder</td>\n",
       "      <td>0.591297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anything</td>\n",
       "      <td>0.589579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.589187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WORD     SCORE\n",
       "0    easier  0.633451\n",
       "1    should  0.630116\n",
       "2   clearer  0.621850\n",
       "3    better  0.602637\n",
       "4      must  0.601793\n",
       "5      need  0.595918\n",
       "6     meant  0.594797\n",
       "7    harder  0.591297\n",
       "8  anything  0.589579\n",
       "9   nothing  0.589187"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'clearer'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['stronger', 'clear'], negative=['strong'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'clearer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecf5b1",
   "metadata": {},
   "source": [
    "And here, we ask: \"Paris is to France what Berlin is to X\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f1b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germany</td>\n",
       "      <td>0.835242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german</td>\n",
       "      <td>0.684480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austria</td>\n",
       "      <td>0.612803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poland</td>\n",
       "      <td>0.581331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germans</td>\n",
       "      <td>0.574868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>munich</td>\n",
       "      <td>0.543591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>belgium</td>\n",
       "      <td>0.532413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>britain</td>\n",
       "      <td>0.529541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>europe</td>\n",
       "      <td>0.524402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>czech</td>\n",
       "      <td>0.515241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WORD     SCORE\n",
       "0  germany  0.835242\n",
       "1   german  0.684480\n",
       "2  austria  0.612803\n",
       "3   poland  0.581331\n",
       "4  germans  0.574868\n",
       "5   munich  0.543591\n",
       "6  belgium  0.532413\n",
       "7  britain  0.529541\n",
       "8   europe  0.524402\n",
       "9    czech  0.515241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'Germany'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['france', 'berlin'], negative=['paris'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'Germany'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702496a",
   "metadata": {},
   "source": [
    "Both of the above produce compelling results, though your mileage may vary. Consider the following: \"arm is to hand what leg is to X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b08a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>final</td>\n",
       "      <td>0.543408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>0.540411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legs</td>\n",
       "      <td>0.527352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>back</td>\n",
       "      <td>0.523477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saturday</td>\n",
       "      <td>0.522487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>round</td>\n",
       "      <td>0.516250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>draw</td>\n",
       "      <td>0.516066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>second</td>\n",
       "      <td>0.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>place</td>\n",
       "      <td>0.509784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>side</td>\n",
       "      <td>0.508683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WORD     SCORE\n",
       "0     final  0.543408\n",
       "1     table  0.540411\n",
       "2      legs  0.527352\n",
       "3      back  0.523477\n",
       "4  saturday  0.522487\n",
       "5     round  0.516250\n",
       "6      draw  0.516066\n",
       "7    second  0.510900\n",
       "8     place  0.509784\n",
       "9      side  0.508683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal target: 'foot'\n"
     ]
    }
   ],
   "source": [
    "analogies = model.most_similar(positive=['hand', 'leg'], negative=['arm'])\n",
    "display(pd.DataFrame(analogies, columns=['WORD', 'SCORE']))\n",
    "print(\"Ideal target: 'foot'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e207db9",
   "metadata": {},
   "source": [
    "Importantly, these results are always going to be specific to the data on which a model was trained. Claims made on the basis of word embeddings that aspire to general linguistic truths would be treading on shaky ground here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5ded1",
   "metadata": {},
   "source": [
    "Document similarity\n",
    "------------------------\n",
    "\n",
    "While the above word relationships are relatively abstract (and any such findings therefrom should be couched accordingly), we can ground them with a concrete task. In this final section, we use GloVe embeddings to encode our corpus documents. This involves associating a word vector for each token in an obituary. Of course, GloVe has not been trained on the obituaries, so there may be important differences in token behavior between that model and the corpus; but we can assume that the general nature of GloVe will give us a decent sense of the overall feature space of the corpus. The result will be an enriched representation of each document, the nuances of which may better help us identify things like similarities between obituaries in our corpus.\n",
    "\n",
    "The other consideration for using GloVe with our specific corpus concerns the out-of-vocabulary words we've already discussed. Before we can encode our documents, we need to filter out tokens for which GloVe has no representation. We can do so by referencing the `in_glove` set we produced above.\n",
    "\n",
    "```{margin} What this loop does\n",
    "For every obituary:\n",
    "\n",
    "1. Create a new list to hold the tokens we want to keep\n",
    "2. Go through each of the tokens and check whether they are in GloVe\n",
    "3. Append in-vocabulary tokens to the new list\n",
    "4. Once we've checked all tokens, append them to a new corpus list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c21df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = []\n",
    "for doc in corpus:\n",
    "    keep = []\n",
    "    for token in doc:\n",
    "        if token in in_glove:\n",
    "            keep.append(token)\n",
    "    pruned.append(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a3902",
   "metadata": {},
   "source": [
    "Time to encode. This is an easy operation. All we need to do is run the list of document's tokens directly into the model object and `gensim` will encode each accordingly. The result will be an `(n, 200)` array, where `n` is the number of tokens we passed to the model; each one will have 200 dimensions.\n",
    "\n",
    "But if we kept this array as is, we'd run into trouble. Matrix operations often require identically shaped representations, so documents with different lengths would be incomparable. To get around this, we take the mean of all the vectors in a document. The result is a 200-dimension vector that stands as a general representation of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58fee46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = [np.mean(model[doc], axis=0) for doc in pruned]\n",
    "doc_embeddings = np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81aa64e",
   "metadata": {},
   "source": [
    "Let's quickly check our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7212620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an encoded document: (485, 200) \n",
      "Shape of an encoded document after taking its mean embedding: (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of an encoded document: {model[pruned[0]].shape}\",\n",
    "    f\"\\nShape of an encoded document after taking its mean embedding: {doc_embeddings[0].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48ab3e",
   "metadata": {},
   "source": [
    "From here, we can treat these embeddings almost as if they represented words. Let's plot our obituaries accordingly. Take a look around at this and see what you can find. As a starting point, you might focus on that cluster of nodes just up from the center toward the left of the graph. All the obituaries there are for sports players – they're even broken out by sport (baseball players are on the far left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d708fb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f8b5c14adf5d430b93d49afe203a812b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f8b5c14adf5d430b93d49afe203a812b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f8b5c14adf5d430b93d49afe203a812b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-87ef32330d8bd366ea1006c8e1ea2795\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"tooltip\": {\"field\": \"LABEL\", \"type\": \"nominal\"}, \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-87ef32330d8bd366ea1006c8e1ea2795\": [{\"X\": 9.629589080810547, \"Y\": 0.9365087747573853, \"LABEL\": \"Ada Lovelace\"}, {\"X\": 4.867339134216309, \"Y\": -13.727782249450684, \"LABEL\": \"Robert E Lee\"}, {\"X\": 2.516657829284668, \"Y\": -18.354692459106445, \"LABEL\": \"Andrew Johnson\"}, {\"X\": 5.089731216430664, \"Y\": -12.619865417480469, \"LABEL\": \"Bedford Forrest\"}, {\"X\": -0.30361512303352356, \"Y\": -9.816756248474121, \"LABEL\": \"Lucretia Mott\"}, {\"X\": 11.412369728088379, \"Y\": 0.43414148688316345, \"LABEL\": \"Charles Darwin\"}, {\"X\": 3.7749245166778564, \"Y\": -13.713809967041016, \"LABEL\": \"Ulysses Grant\"}, {\"X\": -10.804316520690918, \"Y\": 3.915217161178589, \"LABEL\": \"Mary Ewing Outerbridge\"}, {\"X\": 5.313749313354492, \"Y\": 7.089348793029785, \"LABEL\": \"Emma Lazarus\"}, {\"X\": 0.1769566833972931, \"Y\": 4.888604164123535, \"LABEL\": \"Louisa M Alcott\"}, {\"X\": -2.5393667221069336, \"Y\": -1.0644351243972778, \"LABEL\": \"P T Barnum\"}, {\"X\": 2.2706544399261475, \"Y\": 4.895766735076904, \"LABEL\": \"R L Stevenson\"}, {\"X\": 1.4236607551574707, \"Y\": -11.55919361114502, \"LABEL\": \"Fred Douglass\"}, {\"X\": 1.1170852184295654, \"Y\": 5.052165508270264, \"LABEL\": \"Harriet Beecher Stowe\"}, {\"X\": 1.3462700843811035, \"Y\": 3.350062131881714, \"LABEL\": \"Stephen Crane\"}, {\"X\": 11.823625564575195, \"Y\": 5.069955348968506, \"LABEL\": \"Nietzsche\"}, {\"X\": 3.474562406539917, \"Y\": -14.088415145874023, \"LABEL\": \"William McKinley\"}, {\"X\": -5.221592903137207, \"Y\": -9.45670223236084, \"LABEL\": \"Queen Victoria\"}, {\"X\": -0.3932449519634247, \"Y\": -16.153573989868164, \"LABEL\": \"Benjamin Harrison\"}, {\"X\": 0.8819878101348877, \"Y\": -10.319847106933594, \"LABEL\": \"Elizabeth Cady Stanton\"}, {\"X\": 5.680935859680176, \"Y\": 11.466192245483398, \"LABEL\": \"James M N Whistler\"}, {\"X\": -6.502901077270508, \"Y\": -3.6163651943206787, \"LABEL\": \"Emily Warren Roebling\"}, {\"X\": 0.9912346601486206, \"Y\": -10.336971282958984, \"LABEL\": \"Susan B Anthony\"}, {\"X\": 13.706411361694336, \"Y\": -15.936660766601562, \"LABEL\": \"Qiu Jin\"}, {\"X\": 1.1903727054595947, \"Y\": -15.833939552307129, \"LABEL\": \"Cleveland\"}, {\"X\": 0.06014833599328995, \"Y\": 3.009098529815674, \"LABEL\": \"Sarah Orne Jewett\"}, {\"X\": 5.059043884277344, \"Y\": -12.145303726196289, \"LABEL\": \"Geronimo\"}, {\"X\": 8.49888801574707, \"Y\": -6.145576000213623, \"LABEL\": \"William James\"}, {\"X\": -5.059149742126465, \"Y\": -8.053226470947266, \"LABEL\": \"Florence Nightingale\"}, {\"X\": 9.45051097869873, \"Y\": 4.464698314666748, \"LABEL\": \"Tolstoy\"}, {\"X\": 2.86517333984375, \"Y\": -4.517377853393555, \"LABEL\": \"Joseph Pulitzer\"}, {\"X\": 7.037650108337402, \"Y\": -10.98705768585205, \"LABEL\": \"John P Holland\"}, {\"X\": 7.9997172355651855, \"Y\": -11.967913627624512, \"LABEL\": \"Alfred Thayer Mahan\"}, {\"X\": 7.371427536010742, \"Y\": -2.075955390930176, \"LABEL\": \"John Muir\"}, {\"X\": -0.6195724010467529, \"Y\": -4.894759654998779, \"LABEL\": \"F W Taylor\"}, {\"X\": -1.4776277542114258, \"Y\": -9.218297958374023, \"LABEL\": \"B T Washington\"}, {\"X\": -3.7878124713897705, \"Y\": -4.599079608917236, \"LABEL\": \"J J Hill\"}, {\"X\": -1.648998737335205, \"Y\": 2.9855120182037354, \"LABEL\": \"Jack London\"}, {\"X\": 9.92032241821289, \"Y\": -1.5003327131271362, \"LABEL\": \"Martian Theory\"}, {\"X\": 7.427640914916992, \"Y\": 11.48624324798584, \"LABEL\": \"Hilaire G E Degas\"}, {\"X\": -4.147123336791992, \"Y\": -3.4893581867218018, \"LABEL\": \"C J Walker\"}, {\"X\": -2.7459497451782227, \"Y\": -4.899856090545654, \"LABEL\": \"Carnegie Started\"}, {\"X\": 0.44971612095832825, \"Y\": -10.595927238464355, \"LABEL\": \"Anna H Shaw\"}, {\"X\": -1.8113512992858887, \"Y\": 13.811561584472656, \"LABEL\": \"Marlene Dietrich\"}, {\"X\": -1.9595662355422974, \"Y\": 1.944274663925171, \"LABEL\": \"Nellie Bly\"}, {\"X\": -3.6001923084259033, \"Y\": 0.19057707488536835, \"LABEL\": \"Alexander Graham Bell\"}, {\"X\": 1.1338599920272827, \"Y\": -17.525060653686523, \"LABEL\": \"Warren Harding\"}, {\"X\": -3.8007283210754395, \"Y\": 5.842870712280273, \"LABEL\": \"Harry Houdini\"}, {\"X\": 0.49707192182540894, \"Y\": -9.317200660705566, \"LABEL\": \"Victoria Martin\"}, {\"X\": -2.137650966644287, \"Y\": -11.47287368774414, \"LABEL\": \"Mabel Craty\"}, {\"X\": 13.6246976852417, \"Y\": -0.4075273275375366, \"LABEL\": \"Marie Curie\"}, {\"X\": 9.034079551696777, \"Y\": -13.869738578796387, \"LABEL\": \"Balfour\"}, {\"X\": 6.1557183265686035, \"Y\": -4.759181499481201, \"LABEL\": \"Elmer Sperry\"}, {\"X\": 2.7730519771575928, \"Y\": -17.90154457092285, \"LABEL\": \"William Howard Taft\"}, {\"X\": 4.122943878173828, \"Y\": 5.514073848724365, \"LABEL\": \"Conan Doyle\"}, {\"X\": 3.894010305404663, \"Y\": -7.373220443725586, \"LABEL\": \"Ida B Wells\"}, {\"X\": -0.6402124762535095, \"Y\": -2.9942216873168945, \"LABEL\": \"Melvil Dewey\"}, {\"X\": 0.8306495547294617, \"Y\": 10.522436141967773, \"LABEL\": \"Thomas Edison\"}, {\"X\": -13.630939483642578, \"Y\": 3.5920586585998535, \"LABEL\": \"Knute Rocke\"}, {\"X\": -11.455645561218262, \"Y\": 17.965166091918945, \"LABEL\": \"John Philip Sousa\"}, {\"X\": -6.687372207641602, \"Y\": 12.15854263305664, \"LABEL\": \"Florenz Ziegfeld\"}, {\"X\": 2.818314552307129, \"Y\": -17.133880615234375, \"LABEL\": \"Calvin Coolidge\"}, {\"X\": -1.5672276020050049, \"Y\": 4.57751989364624, \"LABEL\": \"Ring Lardner\"}, {\"X\": -7.8486647605896, \"Y\": -0.3336794078350067, \"LABEL\": \"Louis C Tiffany\"}, {\"X\": -3.5341227054595947, \"Y\": 0.47870972752571106, \"LABEL\": \"T A Watson\"}, {\"X\": 4.497917652130127, \"Y\": -9.539314270019531, \"LABEL\": \"Justice Holmes\"}, {\"X\": 2.8870668411254883, \"Y\": -10.985051155090332, \"LABEL\": \"Jane Addams\"}, {\"X\": -2.852731466293335, \"Y\": 5.445327281951904, \"LABEL\": \"Will Rogers\"}, {\"X\": 2.634998083114624, \"Y\": -4.535669803619385, \"LABEL\": \"Adolph S Ochs\"}, {\"X\": -0.8043901920318604, \"Y\": 6.835999965667725, \"LABEL\": \"Anne Macy\"}, {\"X\": -14.196510314941406, \"Y\": 2.408979892730713, \"LABEL\": \"John W Heisman\"}, {\"X\": 9.145484924316406, \"Y\": 4.776538848876953, \"LABEL\": \"Maxim Gorky\"}, {\"X\": -14.743207931518555, \"Y\": 16.750234603881836, \"LABEL\": \"Maurice Ravel\"}, {\"X\": 2.715054750442505, \"Y\": 4.199475288391113, \"LABEL\": \"Edith Wharton\"}, {\"X\": -2.939866542816162, \"Y\": -3.85809588432312, \"LABEL\": \"John Rockefeller\"}, {\"X\": 4.025189399719238, \"Y\": -9.70034122467041, \"LABEL\": \"Clarence Darrow\"}, {\"X\": 13.446882247924805, \"Y\": -3.987703800201416, \"LABEL\": \"George E Hale\"}, {\"X\": 2.4567582607269287, \"Y\": 15.349662780761719, \"LABEL\": \"Constantin Stanislavsky\"}, {\"X\": 3.4906468391418457, \"Y\": 7.675736427307129, \"LABEL\": \"W B Yeats\"}, {\"X\": -8.473063468933105, \"Y\": -9.35084056854248, \"LABEL\": \"Pope Pius XI\"}, {\"X\": -5.475143909454346, \"Y\": 3.418489933013916, \"LABEL\": \"Howard Carter\"}, {\"X\": 1.8188273906707764, \"Y\": 4.609699726104736, \"LABEL\": \"Scott Fitzgerald\"}, {\"X\": -1.1627827882766724, \"Y\": -7.4035563468933105, \"LABEL\": \"Marcus Garvey\"}, {\"X\": -4.838081359863281, \"Y\": 0.37718555331230164, \"LABEL\": \"Frank Conrad\"}, {\"X\": -16.145322799682617, \"Y\": 6.350594997406006, \"LABEL\": \"Lou Gehrig\"}, {\"X\": 3.6721866130828857, \"Y\": 4.287343502044678, \"LABEL\": \"James Joyce\"}, {\"X\": 2.9453887939453125, \"Y\": 2.5019946098327637, \"LABEL\": \"Virginia Woolf\"}, {\"X\": -7.108323097229004, \"Y\": 14.725702285766602, \"LABEL\": \"George M Cohan\"}, {\"X\": 8.089288711547852, \"Y\": -3.9795265197753906, \"LABEL\": \"J H Kellogg\"}, {\"X\": 7.745167255401611, \"Y\": -3.375455617904663, \"LABEL\": \"George Washington Carver\"}, {\"X\": 0.2762145698070526, \"Y\": -17.672365188598633, \"LABEL\": \"Alfred E Smith\"}, {\"X\": 4.564617156982422, \"Y\": -2.2918553352355957, \"LABEL\": \"Ida M Tarbell\"}, {\"X\": -3.5655741691589355, \"Y\": 4.17132568359375, \"LABEL\": \"Ernie Pyle\"}, {\"X\": 4.576817512512207, \"Y\": -17.442001342773438, \"LABEL\": \"Harry S Truman\"}, {\"X\": 5.769756317138672, \"Y\": -13.118968963623047, \"LABEL\": \"George Patton\"}, {\"X\": -0.487134724855423, \"Y\": -17.166996002197266, \"LABEL\": \"FDR\"}, {\"X\": -10.25243854522705, \"Y\": 16.744211196899414, \"LABEL\": \"Jerome Kern\"}, {\"X\": 10.450125694274902, \"Y\": -17.857093811035156, \"LABEL\": \"Adolf Hitler\"}, {\"X\": -15.567388534545898, \"Y\": 17.081615447998047, \"LABEL\": \"Bela Bartok\"}, {\"X\": 3.3424673080444336, \"Y\": 4.723523139953613, \"LABEL\": \"Gertrude Stein\"}, {\"X\": 9.325911521911621, \"Y\": -12.526713371276855, \"LABEL\": \"Lord Keynes\"}, {\"X\": 14.586517333984375, \"Y\": -4.484496593475342, \"LABEL\": \"C E M Clung\"}, {\"X\": 1.1403337717056274, \"Y\": 2.509639024734497, \"LABEL\": \"Willa Cather\"}, {\"X\": -4.850642204284668, \"Y\": 6.138102054595947, \"LABEL\": \"Al Capone\"}, {\"X\": 2.3354763984680176, \"Y\": -15.820819854736328, \"LABEL\": \"Fiorello La Guardia\"}, {\"X\": 14.609024047851562, \"Y\": -0.10717794299125671, \"LABEL\": \"Max Planck\"}, {\"X\": -1.1804206371307373, \"Y\": -5.477313041687012, \"LABEL\": \"Henry Ford\"}, {\"X\": 6.029560565948486, \"Y\": -13.791752815246582, \"LABEL\": \"John Pershing\"}, {\"X\": 2.0751850605010986, \"Y\": 15.046469688415527, \"LABEL\": \"Sergei Eisenstein\"}, {\"X\": 8.11977481842041, \"Y\": -20.070783615112305, \"LABEL\": \"Mohandas K Gandhi\"}, {\"X\": -14.677245140075684, \"Y\": 4.58611536026001, \"LABEL\": \"Babe Ruth\"}, {\"X\": -0.5605786442756653, \"Y\": 1.7567886114120483, \"LABEL\": \"Mitchell\"}, {\"X\": 14.690046310424805, \"Y\": -2.3300743103027344, \"LABEL\": \"A J Dempster\"}, {\"X\": 0.5355619788169861, \"Y\": 4.450295925140381, \"LABEL\": \"Edna St V Millay\"}, {\"X\": 5.150554656982422, \"Y\": -16.838817596435547, \"LABEL\": \"Henry L Stimson\"}, {\"X\": -7.985596656799316, \"Y\": 14.369399070739746, \"LABEL\": \"Fanny Brice\"}, {\"X\": 10.201523780822754, \"Y\": -3.951920509338379, \"LABEL\": \"Henrietta Lacks\"}, {\"X\": 7.256105422973633, \"Y\": -15.879838943481445, \"LABEL\": \"Eva Peron\"}, {\"X\": 7.958618640899658, \"Y\": -7.40358304977417, \"LABEL\": \"John Dewey\"}, {\"X\": 8.137778282165527, \"Y\": -22.409303665161133, \"LABEL\": \"Chaim Weizmann\"}, {\"X\": -3.047520399093628, \"Y\": -6.486069679260254, \"LABEL\": \"Charles Spaulding\"}, {\"X\": 3.39670467376709, \"Y\": -18.607646942138672, \"LABEL\": \"Fred Vinson\"}, {\"X\": 0.5231119394302368, \"Y\": 2.0844290256500244, \"LABEL\": \"Marjorie Rawlings\"}, {\"X\": 11.478231430053711, \"Y\": -18.480487823486328, \"LABEL\": \"Joseph Stalin\"}, {\"X\": -13.02806568145752, \"Y\": 4.952950477600098, \"LABEL\": \"Jim Thorpe\"}, {\"X\": -1.8037348985671997, \"Y\": 3.449126958847046, \"LABEL\": \"Eugene O Neill\"}, {\"X\": 3.311312437057495, \"Y\": -3.8299105167388916, \"LABEL\": \"Anne O Hare McCormick\"}, {\"X\": 5.014588832855225, \"Y\": 10.005289077758789, \"LABEL\": \"Frida Kahlo\"}, {\"X\": 8.413653373718262, \"Y\": -17.18549156188965, \"LABEL\": \"Getulio Vargas\"}, {\"X\": 14.445099830627441, \"Y\": -1.8850390911102295, \"LABEL\": \"Enrico Fermi\"}, {\"X\": 6.477755069732666, \"Y\": 11.336223602294922, \"LABEL\": \"Henri Matisse\"}, {\"X\": 7.114038944244385, \"Y\": -2.987826347351074, \"LABEL\": \"Liberty H Bailey\"}, {\"X\": -5.830735206604004, \"Y\": 15.424117088317871, \"LABEL\": \"Lionel Barrymore\"}, {\"X\": 9.499425888061523, \"Y\": 6.423043251037598, \"LABEL\": \"Thomas Mann\"}, {\"X\": 12.435925483703613, \"Y\": 0.6852051019668579, \"LABEL\": \"Albert Einstein\"}, {\"X\": -12.06922435760498, \"Y\": 6.419966220855713, \"LABEL\": \"Margaret Abbott\"}, {\"X\": -0.3511526584625244, \"Y\": -12.968914031982422, \"LABEL\": \"Walter White\"}, {\"X\": -15.838933944702148, \"Y\": 5.7552266120910645, \"LABEL\": \"Cy Young\"}, {\"X\": -0.6612284779548645, \"Y\": -0.9929090142250061, \"LABEL\": \"Dale Carnegie\"}, {\"X\": -11.752488136291504, \"Y\": 4.983523368835449, \"LABEL\": \"Babe Zaharias\"}, {\"X\": -2.0537309646606445, \"Y\": -3.7528295516967773, \"LABEL\": \"Charles Merrill\"}, {\"X\": -2.085387706756592, \"Y\": -4.954380035400391, \"LABEL\": \"Thomas J Watson Sr\"}, {\"X\": -2.122974395751953, \"Y\": -5.693045616149902, \"LABEL\": \"Gerard Swope\"}, {\"X\": 6.393711090087891, \"Y\": 13.563359260559082, \"LABEL\": \"Christian Dior\"}, {\"X\": -10.264820098876953, \"Y\": 19.21905517578125, \"LABEL\": \"W C Handy\"}, {\"X\": -9.749808311462402, \"Y\": 19.22344207763672, \"LABEL\": \"Billie Holiday\"}, {\"X\": -7.139647483825684, \"Y\": -2.679274320602417, \"LABEL\": \"Frank Lloyd Wright\"}, {\"X\": -6.252832412719727, \"Y\": 15.452739715576172, \"LABEL\": \"Ethel Barrymore\"}, {\"X\": -1.6900622844696045, \"Y\": 16.44804573059082, \"LABEL\": \"Cecil De Mille\"}, {\"X\": 14.377174377441406, \"Y\": -4.461243152618408, \"LABEL\": \"Ross G Harrison\"}, {\"X\": 5.730043411254883, \"Y\": -17.34250259399414, \"LABEL\": \"John Dulles\"}, {\"X\": 8.909778594970703, \"Y\": 5.260741233825684, \"LABEL\": \"Boris Pasternak\"}, {\"X\": 15.18835163116455, \"Y\": -3.321732997894287, \"LABEL\": \"Beno Gutenberg\"}, {\"X\": 6.0727105140686035, \"Y\": 0.7039583921432495, \"LABEL\": \"Emily Post\"}, {\"X\": 1.2883164882659912, \"Y\": 0.7812788486480713, \"LABEL\": \"Richard Wright\"}, {\"X\": 6.606455326080322, \"Y\": -17.90180778503418, \"LABEL\": \"Hammarskjold\"}, {\"X\": 2.51444411277771, \"Y\": 5.4511637687683105, \"LABEL\": \"Ernest Hemingway\"}, {\"X\": 3.3103435039520264, \"Y\": 9.779006958007812, \"LABEL\": \"Primitive Artist\"}, {\"X\": -1.2825796604156494, \"Y\": -11.307229995727539, \"LABEL\": \"Emily Balch\"}, {\"X\": 1.1649328470230103, \"Y\": -18.612628936767578, \"LABEL\": \"Sam Rayburn\"}, {\"X\": 12.17062759399414, \"Y\": 2.2361319065093994, \"LABEL\": \"Carl G Jung\"}, {\"X\": -4.2258782386779785, \"Y\": 10.714470863342285, \"LABEL\": \"Marilyn Monroe\"}, {\"X\": 1.90577232837677, \"Y\": -14.886177062988281, \"LABEL\": \"Eleanor Roosevelt\"}, {\"X\": 3.077925443649292, \"Y\": 3.4568052291870117, \"LABEL\": \"William Faulkner\"}, {\"X\": 3.701728343963623, \"Y\": 2.523127794265747, \"LABEL\": \"Sylvia Plath\"}, {\"X\": 2.2289395332336426, \"Y\": -19.08131217956543, \"LABEL\": \"John F Kennedy\"}, {\"X\": 4.192113876342773, \"Y\": 3.648496150970459, \"LABEL\": \"Robert Frost\"}, {\"X\": -0.7063899636268616, \"Y\": -11.29255485534668, \"LABEL\": \"W E B DuBois\"}, {\"X\": -1.7259103059768677, \"Y\": -14.54096508026123, \"LABEL\": \"Herbert Hoover\"}, {\"X\": 6.282223224639893, \"Y\": -13.74743938446045, \"LABEL\": \"Douglas MacArthur\"}, {\"X\": 1.7478026151657104, \"Y\": 7.9571709632873535, \"LABEL\": \"Sean O Casey\"}, {\"X\": 8.371752738952637, \"Y\": -1.8794926404953003, \"LABEL\": \"Rachel Carson\"}, {\"X\": -9.30225944519043, \"Y\": 16.191822052001953, \"LABEL\": \"Cole Porter\"}, {\"X\": 3.076063871383667, \"Y\": 1.314249038696289, \"LABEL\": \"Nella Larsen\"}, {\"X\": 1.6825166940689087, \"Y\": -17.522794723510742, \"LABEL\": \"Adlai Ewing Stevenson\"}, {\"X\": -3.164984941482544, \"Y\": 17.577924728393555, \"LABEL\": \"David O Selznick\"}, {\"X\": 8.69693660736084, \"Y\": -14.189995765686035, \"LABEL\": \"Churchill\"}, {\"X\": -15.152002334594727, \"Y\": 3.9001054763793945, \"LABEL\": \"Branch Rickey\"}, {\"X\": 11.748671531677246, \"Y\": 4.267360687255859, \"LABEL\": \"Martin Buber\"}, {\"X\": 1.4773584604263306, \"Y\": -3.502977132797241, \"LABEL\": \"Edward R Murrow\"}, {\"X\": 10.981849670410156, \"Y\": 3.3068172931671143, \"LABEL\": \"Albert Schweitzer\"}, {\"X\": 1.810014009475708, \"Y\": 6.656957626342773, \"LABEL\": \"Shirley Jackson\"}, {\"X\": 1.8925938606262207, \"Y\": -9.45506477355957, \"LABEL\": \"Margaret Sanger\"}, {\"X\": 6.807074069976807, \"Y\": -11.948288917541504, \"LABEL\": \"Chester Nimitz\"}, {\"X\": -2.056762218475342, \"Y\": 12.418746948242188, \"LABEL\": \"Buster Keaton\"}, {\"X\": -2.6335644721984863, \"Y\": 6.498964309692383, \"LABEL\": \"Lenny Bruce\"}, {\"X\": -0.9291820526123047, \"Y\": 17.55532455444336, \"LABEL\": \"Walt Disney\"}, {\"X\": -1.7889564037322998, \"Y\": -5.057743072509766, \"LABEL\": \"Alfred P Sloan Jr\"}, {\"X\": 10.934745788574219, \"Y\": -4.189286231994629, \"LABEL\": \"Gregory Pincus\"}, {\"X\": 3.474595785140991, \"Y\": -1.6320171356201172, \"LABEL\": \"Henry R Luce\"}, {\"X\": 5.184930801391602, \"Y\": 2.462585687637329, \"LABEL\": \"Langston Hughes\"}, {\"X\": 12.949685096740723, \"Y\": -1.5517901182174683, \"LABEL\": \"J Robert Oppenheimer\"}, {\"X\": 0.8506120443344116, \"Y\": -16.64697265625, \"LABEL\": \"Robert Francis Kennedy\"}, {\"X\": 0.06093268468976021, \"Y\": 6.959182262420654, \"LABEL\": \"Helen Keller\"}, {\"X\": 4.050485610961914, \"Y\": -1.4843933582305908, \"LABEL\": \"Upton Sinclair\"}, {\"X\": 4.620257377624512, \"Y\": -8.236494064331055, \"LABEL\": \"Martin Luther King Jr\"}, {\"X\": -6.331336498260498, \"Y\": -12.70248031616211, \"LABEL\": \"Yuri Gagarin\"}, {\"X\": -7.336552619934082, \"Y\": -2.553863286972046, \"LABEL\": \"Mies van der Rohe\"}, {\"X\": 4.473064422607422, \"Y\": -17.60696029663086, \"LABEL\": \"David Eisenhower\"}, {\"X\": -12.037976264953613, \"Y\": 20.4371280670166, \"LABEL\": \"Coleman Hawkins\"}, {\"X\": -1.0962679386138916, \"Y\": 14.572998046875, \"LABEL\": \"Madhubala\"}, {\"X\": -6.652101516723633, \"Y\": 13.022965431213379, \"LABEL\": \"Judy Garland\"}, {\"X\": -11.519660949707031, \"Y\": 5.149566173553467, \"LABEL\": \"Maureen Connolly\"}, {\"X\": 11.490662574768066, \"Y\": -16.636425018310547, \"LABEL\": \"Ho Chi Minh\"}, {\"X\": -12.171581268310547, \"Y\": 7.268080234527588, \"LABEL\": \"Sonja Henie\"}, {\"X\": 2.202540159225464, \"Y\": -16.904741287231445, \"LABEL\": \"Everett Dirksen\"}, {\"X\": -2.9562528133392334, \"Y\": -14.008023262023926, \"LABEL\": \"Walter Reuther\"}, {\"X\": 10.0339937210083, \"Y\": -17.23975372314453, \"LABEL\": \"Edouard Daladier\"}, {\"X\": 8.901248931884766, \"Y\": 6.926815032958984, \"LABEL\": \"Erich Maria Remarque\"}, {\"X\": 9.946044921875, \"Y\": -16.896339416503906, \"LABEL\": \"De Gaulle Rallied\"}, {\"X\": 6.375982284545898, \"Y\": 14.089868545532227, \"LABEL\": \"Coco Chanel\"}, {\"X\": 5.421616554260254, \"Y\": -14.56261157989502, \"LABEL\": \"Florence Blanchfield\"}, {\"X\": 11.947996139526367, \"Y\": -18.720796585083008, \"LABEL\": \"Khrushchev\"}, {\"X\": 3.6141252517700195, \"Y\": 11.977758407592773, \"LABEL\": \"Diane Arbus\"}, {\"X\": 3.14437198638916, \"Y\": -11.699508666992188, \"LABEL\": \"Ralph Bunche\"}, {\"X\": -11.873806953430176, \"Y\": 5.324117183685303, \"LABEL\": \"Bobby Jones\"}, {\"X\": -10.47250747680664, \"Y\": 19.456758499145508, \"LABEL\": \"Louis Armstrong\"}, {\"X\": 5.490179538726807, \"Y\": -17.52106285095215, \"LABEL\": \"Dean Acheson\"}, {\"X\": -14.25207233428955, \"Y\": 16.682188034057617, \"LABEL\": \"Igor Stravinsky\"}, {\"X\": 3.6063179969787598, \"Y\": -20.43814468383789, \"LABEL\": \"Hugo Black\"}, {\"X\": -8.741172790527344, \"Y\": 20.10617446899414, \"LABEL\": \"Mahalia Jackson\"}, {\"X\": -14.888214111328125, \"Y\": 4.1791863441467285, \"LABEL\": \"Jackie Robinson\"}, {\"X\": -5.368350982666016, \"Y\": -9.634806632995605, \"LABEL\": \"The Duke of Windsor\"}, {\"X\": -0.6426783204078674, \"Y\": -14.689474105834961, \"LABEL\": \"J Edgar Hoover\"}, {\"X\": -2.308452844619751, \"Y\": -14.421500205993652, \"LABEL\": \"Lyndon Johnson\"}, {\"X\": -13.81699275970459, \"Y\": 16.135818481445312, \"LABEL\": \"Otto Klemperer\"}, {\"X\": 0.9663636684417725, \"Y\": -6.817568778991699, \"LABEL\": \"Eddie Rickenbacker\"}, {\"X\": 1.193108320236206, \"Y\": -20.33041000366211, \"LABEL\": \"Jeanette Rankin\"}, {\"X\": 6.1103901863098145, \"Y\": 10.780670166015625, \"LABEL\": \"Pablo Picasso\"}, {\"X\": 0.5244559049606323, \"Y\": -6.968406677246094, \"LABEL\": \"Roberto Clemente\"}, {\"X\": 3.218234062194824, \"Y\": 6.269067764282227, \"LABEL\": \"Nancy Mitford\"}, {\"X\": 3.557950735092163, \"Y\": -20.199440002441406, \"LABEL\": \"Earl Warren\"}, {\"X\": 1.1160459518432617, \"Y\": 8.574992179870605, \"LABEL\": \"Sylvia Plath\"}, {\"X\": -6.244418621063232, \"Y\": 10.41963005065918, \"LABEL\": \"Ed Sullivan\"}, {\"X\": -6.011885166168213, \"Y\": 14.240015029907227, \"LABEL\": \"Katharine Cornell\"}, {\"X\": 2.142029285430908, \"Y\": -6.557278156280518, \"LABEL\": \"Charles Lindbergh\"}, {\"X\": 11.842513084411621, \"Y\": -14.831551551818848, \"LABEL\": \"Haile Selassie\"}, {\"X\": 5.107857704162598, \"Y\": -7.8079938888549805, \"LABEL\": \"Elijah Muhammad\"}, {\"X\": 9.465653419494629, \"Y\": -17.16649627685547, \"LABEL\": \"Franco\"}, {\"X\": 3.864711284637451, \"Y\": 11.90059757232666, \"LABEL\": \"Walker Evans\"}, {\"X\": 12.281484603881836, \"Y\": -16.960769653320312, \"LABEL\": \"Chiang Kai shek\"}, {\"X\": -3.19472599029541, \"Y\": -3.142597198486328, \"LABEL\": \"J Paul Getty\"}, {\"X\": 12.985475540161133, \"Y\": -16.64879608154297, \"LABEL\": \"Mao Tse Tung\"}, {\"X\": 6.411359786987305, \"Y\": 10.695928573608398, \"LABEL\": \"Max Ernst\"}, {\"X\": -1.0085030794143677, \"Y\": -18.12965202331543, \"LABEL\": \"Richard Daley\"}, {\"X\": 13.216479301452637, \"Y\": -0.3249548673629761, \"LABEL\": \"Jacques Monod\"}, {\"X\": -1.367120623588562, \"Y\": 17.1641902923584, \"LABEL\": \"Adolph Zukor\"}, {\"X\": -2.0315723419189453, \"Y\": 12.687402725219727, \"LABEL\": \"Charles Chaplin\"}, {\"X\": -5.378741264343262, \"Y\": 13.529718399047852, \"LABEL\": \"Joan Crawford\"}, {\"X\": 9.092671394348145, \"Y\": -14.452479362487793, \"LABEL\": \"Dash Ended\"}, {\"X\": -11.737982749938965, \"Y\": 15.359981536865234, \"LABEL\": \"Maria Callas\"}, {\"X\": 8.854317665100098, \"Y\": -22.21473503112793, \"LABEL\": \"Golda Meir\"}, {\"X\": 10.609031677246094, \"Y\": 1.3082987070083618, \"LABEL\": \"Margaret Mead\"}, {\"X\": 6.118866920471191, \"Y\": -9.024884223937988, \"LABEL\": \"Pope Paul VI\"}, {\"X\": -7.283303260803223, \"Y\": -6.654460906982422, \"LABEL\": \"Bruce Catton\"}, {\"X\": -12.953668594360352, \"Y\": 16.32244873046875, \"LABEL\": \"Arthur Fiedler\"}, {\"X\": -3.606452465057373, \"Y\": 12.465727806091309, \"LABEL\": \"John Wayne\"}, {\"X\": 1.2237907648086548, \"Y\": -13.392544746398926, \"LABEL\": \"A Philip Randolph\"}, {\"X\": -12.13571548461914, \"Y\": 19.790708541870117, \"LABEL\": \"Stan Kenton\"}, {\"X\": -9.776265144348145, \"Y\": 16.63808250427246, \"LABEL\": \"Richard Rodgers\"}, {\"X\": -12.791735649108887, \"Y\": 4.72695779800415, \"LABEL\": \"Jesse Owens\"}, {\"X\": 0.1744154691696167, \"Y\": 13.81369400024414, \"LABEL\": \"Alfred Hitchcock\"}, {\"X\": 11.404035568237305, \"Y\": 1.2002414464950562, \"LABEL\": \"Jean Piaget\"}, {\"X\": 10.629410743713379, \"Y\": 5.37824010848999, \"LABEL\": \"Jean Paul Sartre\"}, {\"X\": -9.51002311706543, \"Y\": 6.543694019317627, \"LABEL\": \"Joe Louis\"}, {\"X\": -4.598476886749268, \"Y\": -5.0298991203308105, \"LABEL\": \"Robert Moses\"}, {\"X\": 9.557087898254395, \"Y\": -22.23974609375, \"LABEL\": \"Anwar el Sadat\"}, {\"X\": -4.135482311248779, \"Y\": 14.641820907592773, \"LABEL\": \"Ingrid Bergman\"}, {\"X\": 11.428175926208496, \"Y\": 2.1299140453338623, \"LABEL\": \"Anna Freud\"}, {\"X\": 11.94921875, \"Y\": -19.105857849121094, \"LABEL\": \"Leonid Brezhnev\"}, {\"X\": -13.345328330993652, \"Y\": 17.284900665283203, \"LABEL\": \"Arthur Rubinstein\"}, {\"X\": -11.595566749572754, \"Y\": 19.937498092651367, \"LABEL\": \"Thelonious Monk\"}, {\"X\": -4.014244556427002, \"Y\": 16.423534393310547, \"LABEL\": \"Lee Strasberg\"}, {\"X\": -15.019454956054688, \"Y\": 4.6996564865112305, \"LABEL\": \"Satchel Paige\"}, {\"X\": -9.574095726013184, \"Y\": 6.669792175292969, \"LABEL\": \"Jack Dempsey\"}, {\"X\": -11.981386184692383, \"Y\": 21.897388458251953, \"LABEL\": \"Earl Hines\"}, {\"X\": -10.348932266235352, \"Y\": 21.249935150146484, \"LABEL\": \"Muddy Waters\"}, {\"X\": 2.417151689529419, \"Y\": 5.5440239906311035, \"LABEL\": \"Truman Capote\"}, {\"X\": 0.5363287925720215, \"Y\": 6.157252311706543, \"LABEL\": \"Lillian Hellman\"}, {\"X\": -10.660518646240234, \"Y\": 7.940510272979736, \"LABEL\": \"Johnny Weissmuller\"}, {\"X\": -7.373063564300537, \"Y\": 0.5054081082344055, \"LABEL\": \"Ansel Adams\"}, {\"X\": -8.515951156616211, \"Y\": 15.578156471252441, \"LABEL\": \"Ethel Merman\"}, {\"X\": 8.100703239440918, \"Y\": -20.053998947143555, \"LABEL\": \"Indira Gandhi\"}, {\"X\": -3.5809130668640137, \"Y\": -2.1500587463378906, \"LABEL\": \"Ray A Kroc\"}, {\"X\": -5.22397518157959, \"Y\": 15.093145370483398, \"LABEL\": \"Richard Burton\"}, {\"X\": -11.587529182434082, \"Y\": 21.17261505126953, \"LABEL\": \"Count Basie\"}, {\"X\": 1.3015097379684448, \"Y\": 7.229896545410156, \"LABEL\": \"E B White\"}, {\"X\": -2.68548321723938, \"Y\": 15.73446273803711, \"LABEL\": \"Orson Welles\"}, {\"X\": -15.570432662963867, \"Y\": 5.330615520477295, \"LABEL\": \"Roger Maris\"}, {\"X\": -3.728980779647827, \"Y\": 13.766958236694336, \"LABEL\": \"James Cagney\"}, {\"X\": 4.857714653015137, \"Y\": 11.632116317749023, \"LABEL\": \"Georgia O Keeffe\"}, {\"X\": -11.422660827636719, \"Y\": 20.63717269897461, \"LABEL\": \"Benny Goodman\"}, {\"X\": 6.449415683746338, \"Y\": 5.915811061859131, \"LABEL\": \"Jorge Luis Borges\"}, {\"X\": 5.973481178283691, \"Y\": 5.009847164154053, \"LABEL\": \"Bernard Malamud\"}, {\"X\": -7.632770538330078, \"Y\": 10.633408546447754, \"LABEL\": \"Kate Smith\"}, {\"X\": -2.7195780277252197, \"Y\": -8.981995582580566, \"LABEL\": \"The Challenger\"}, {\"X\": 8.389544486999512, \"Y\": 7.210494518280029, \"LABEL\": \"Primo Levi\"}, {\"X\": 2.7997899055480957, \"Y\": -2.075590133666992, \"LABEL\": \"Clare Boothe Luce\"}, {\"X\": -2.9558186531066895, \"Y\": 14.558831214904785, \"LABEL\": \"John Huston\"}, {\"X\": -5.270463466644287, \"Y\": 12.391576766967773, \"LABEL\": \"Rita Hayworth\"}, {\"X\": 4.1467084884643555, \"Y\": -6.818028926849365, \"LABEL\": \"James Baldwin\"}, {\"X\": -0.20809608697891235, \"Y\": -17.895172119140625, \"LABEL\": \"Alf Landon\"}, {\"X\": -13.740716934204102, \"Y\": 18.37775230407715, \"LABEL\": \"Andres Segovie\"}, {\"X\": -3.0713820457458496, \"Y\": 16.13964080810547, \"LABEL\": \"John Houseman\"}, {\"X\": 2.1412930488586426, \"Y\": 3.7850356101989746, \"LABEL\": \"Louis L Amour\"}, {\"X\": 12.002198219299316, \"Y\": -2.0154478549957275, \"LABEL\": \"William B Shockley\"}, {\"X\": -7.188117504119873, \"Y\": 13.554760932922363, \"LABEL\": \"Lucille Ball\"}, {\"X\": 3.9904696941375732, \"Y\": 3.6188926696777344, \"LABEL\": \"Robert Penn Warren\"}, {\"X\": -2.993260622024536, \"Y\": -15.6643705368042, \"LABEL\": \"Ferdinand Marcos\"}, {\"X\": 12.461625099182129, \"Y\": -19.119094848632812, \"LABEL\": \"Andrei Sakharov\"}, {\"X\": 11.80474853515625, \"Y\": -19.722991943359375, \"LABEL\": \"Andrei A Gromyko\"}, {\"X\": 3.803722858428955, \"Y\": -5.2729573249816895, \"LABEL\": \"I F Stone\"}, {\"X\": -13.136995315551758, \"Y\": 17.290422439575195, \"LABEL\": \"Vladimir Horowitz\"}, {\"X\": 12.181300163269043, \"Y\": -14.329141616821289, \"LABEL\": \"Hirohito\"}, {\"X\": -4.576432704925537, \"Y\": -1.7550851106643677, \"LABEL\": \"August A Busch Jr\"}, {\"X\": 0.6167632937431335, \"Y\": -19.481542587280273, \"LABEL\": \"Claude Pepper\"}, {\"X\": 3.112546443939209, \"Y\": 7.466588973999023, \"LABEL\": \"Samuel Beckett\"}, {\"X\": -2.007981061935425, \"Y\": 13.82043743133545, \"LABEL\": \"Greta Garbo\"}, {\"X\": -5.084105014801025, \"Y\": 13.166306495666504, \"LABEL\": \"Sammy Davis Jr\"}, {\"X\": -12.992807388305664, \"Y\": 16.50141143798828, \"LABEL\": \"Leonard Bernstein\"}, {\"X\": 5.654520034790039, \"Y\": 13.032342910766602, \"LABEL\": \"Erte\"}, {\"X\": 0.8844993710517883, \"Y\": -12.610634803771973, \"LABEL\": \"Ralph David Abernathy\"}, {\"X\": -6.092902660369873, \"Y\": 16.384614944458008, \"LABEL\": \"Rex Harrison\"}, {\"X\": -2.862534999847412, \"Y\": 14.518528938293457, \"LABEL\": \"Frank Capra\"}, {\"X\": -0.7489873766899109, \"Y\": 8.879402160644531, \"LABEL\": \"Dr Seuss\"}, {\"X\": -12.450113296508789, \"Y\": 20.800580978393555, \"LABEL\": \"Miles Davis\"}, {\"X\": -10.867227554321289, \"Y\": 13.873992919921875, \"LABEL\": \"Martha Graham\"}, {\"X\": -15.863840103149414, \"Y\": 4.244723320007324, \"LABEL\": \"Leo Durocher\"}, {\"X\": -5.181403636932373, \"Y\": 16.91637420654297, \"LABEL\": \"Peggy Ashcroft\"}, {\"X\": 1.953492522239685, \"Y\": 1.8476524353027344, \"LABEL\": \"Alex Haley\"}, {\"X\": -14.235946655273438, \"Y\": 17.534887313842773, \"LABEL\": \"John Cage\"}, {\"X\": 9.157862663269043, \"Y\": -22.620685577392578, \"LABEL\": \"Menachem Begin\"}, {\"X\": -6.655925750732422, \"Y\": 15.450852394104004, \"LABEL\": \"Shirley Booth\"}, {\"X\": 6.403168678283691, \"Y\": 4.0633955001831055, \"LABEL\": \"Isaac Asimov\"}, {\"X\": 2.9871246814727783, \"Y\": -0.25527191162109375, \"LABEL\": \"William Shawn\"}, {\"X\": 3.1391007900238037, \"Y\": -7.799642562866211, \"LABEL\": \"Marsha P Johnson\"}, {\"X\": 3.6216492652893066, \"Y\": -20.73087501525879, \"LABEL\": \"Thurgood Marshall\"}, {\"X\": -0.025618620216846466, \"Y\": 14.099115371704102, \"LABEL\": \"Federico Fellini\"}, {\"X\": 1.7541379928588867, \"Y\": -13.795405387878418, \"LABEL\": \"Cesar Chavez\"}, {\"X\": -14.002293586730957, \"Y\": 19.273035049438477, \"LABEL\": \"Carlos Montoya\"}, {\"X\": -12.635844230651855, \"Y\": 21.199996948242188, \"LABEL\": \"Dizzy Gillespie\"}, {\"X\": -11.325723648071289, \"Y\": 4.3122029304504395, \"LABEL\": \"Arthur Ashe\"}, {\"X\": 4.702512741088867, \"Y\": 5.378389358520508, \"LABEL\": \"William Golding\"}, {\"X\": 11.471202850341797, \"Y\": -5.67047643661499, \"LABEL\": \"Albert Sabin\"}, {\"X\": 0.7590458989143372, \"Y\": -18.519386291503906, \"LABEL\": \"Thomas P O Neill Jr\"}, {\"X\": 1.6892129182815552, \"Y\": -17.274255752563477, \"LABEL\": \"Richard Nixon\"}, {\"X\": 11.3215913772583, \"Y\": 1.8216707706451416, \"LABEL\": \"Erik Erikson\"}, {\"X\": 12.909110069274902, \"Y\": -1.4237960577011108, \"LABEL\": \"Linus C Pauling\"}, {\"X\": -5.725397109985352, \"Y\": 14.565417289733887, \"LABEL\": \"Jessica Tandy\"}, {\"X\": 9.70884895324707, \"Y\": -8.380881309509277, \"LABEL\": \"Jan Tinbergen\"}, {\"X\": 1.5300692319869995, \"Y\": -0.6255696415901184, \"LABEL\": \"Jacqueline Kennedy\"}, {\"X\": 11.458001136779785, \"Y\": -5.619353771209717, \"LABEL\": \"Jonas Salk\"}, {\"X\": 3.310856819152832, \"Y\": 12.234801292419434, \"LABEL\": \"Alfred Eisenstaedt\"}, {\"X\": -7.527838230133057, \"Y\": 15.655458450317383, \"LABEL\": \"Ginger Rogers\"}, {\"X\": -6.486595630645752, \"Y\": 14.33803939819336, \"LABEL\": \"George Abbott\"}, {\"X\": 9.317960739135742, \"Y\": -22.759197235107422, \"LABEL\": \"Yitzhak Rabin\"}, {\"X\": 10.1116361618042, \"Y\": -1.4016354084014893, \"LABEL\": \"Carl Sagan\"}, {\"X\": 4.682015895843506, \"Y\": -0.24707086384296417, \"LABEL\": \"Timothy Leary\"}, {\"X\": -7.98546838760376, \"Y\": 16.594234466552734, \"LABEL\": \"Gene Kelly\"}, {\"X\": 4.855006217956543, \"Y\": 3.3466439247131348, \"LABEL\": \"Allen Ginsberg\"}, {\"X\": 13.162900924682617, \"Y\": -17.003646850585938, \"LABEL\": \"Deng Xiaoping\"}, {\"X\": -3.966529369354248, \"Y\": 13.405994415283203, \"LABEL\": \"James Stewart\"}, {\"X\": -2.5082755088806152, \"Y\": 11.209596633911133, \"LABEL\": \"Bob Kane\"}, {\"X\": 6.970714569091797, \"Y\": 0.5519394278526306, \"LABEL\": \"Benjamin Spock\"}, {\"X\": -11.216814994812012, \"Y\": 5.593843460083008, \"LABEL\": \"Helen Moody\"}, {\"X\": -5.174684524536133, \"Y\": 15.951761245727539, \"LABEL\": \"Maureen O Sullivan\"}, {\"X\": 9.226228713989258, \"Y\": -7.758716106414795, \"LABEL\": \"Theodore Schultz\"}, {\"X\": -6.292104244232178, \"Y\": -12.628838539123535, \"LABEL\": \"Alan B Shepard Jr\"}, {\"X\": -11.844756126403809, \"Y\": 13.566322326660156, \"LABEL\": \"Galina Ulanova\"}, {\"X\": -0.42799046635627747, \"Y\": -19.266468048095703, \"LABEL\": \"Bella Abzug\"}, {\"X\": 1.4201784133911133, \"Y\": -3.4274697303771973, \"LABEL\": \"Fred W Friendly\"}, {\"X\": -9.123132705688477, \"Y\": 18.30845832824707, \"LABEL\": \"Frank Sinatra\"}, {\"X\": 10.336825370788574, \"Y\": -21.98391342163086, \"LABEL\": \"Hassan II\"}, {\"X\": 5.405998706817627, \"Y\": 6.564864635467529, \"LABEL\": \"Iris Murdoch\"}, {\"X\": 10.242327690124512, \"Y\": -22.4055118560791, \"LABEL\": \"King Hussein\"}, {\"X\": 9.397595405578613, \"Y\": -15.240979194641113, \"LABEL\": \"Pierre Trudeau\"}, {\"X\": 0.28496065735816956, \"Y\": -16.12995719909668, \"LABEL\": \"Elliot Richardson\"}, {\"X\": -1.7634185552597046, \"Y\": 9.884428977966309, \"LABEL\": \"Charles M Schulz\"}, {\"X\": 9.921907424926758, \"Y\": 0.5387915968894958, \"LABEL\": \"Karen Sparck Jones\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_data = prepare_vis_data(doc_embeddings, manifest['NAME'])\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip='LABEL'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee6c3b",
   "metadata": {},
   "source": [
    "The document embeddings seem to be partitioned into different clusters. We'll end by using a hierarchical clusterer to see if we can further specifiy these clusters. This involves loading the `AgglomerativeClustering` object from `scikit-learn` and fitting it to our document embeddings. Hierarchical clustering requires us to predefine the number of clusters we'd like to generate. In this case, we'll go with 18.\n",
    "\n",
    "```{margin} Why this number of clusters?\n",
    "We grid searched different numbers and measured the results with a [sihouette coeffiencient].\n",
    "\n",
    "[silhouette coefficient]: https://towardsdatascience.com/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8689e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 18\n",
    "agg = AgglomerativeClustering(n_clusters=n_clusters).fit(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1aa71f",
   "metadata": {},
   "source": [
    "Now we can assign the clusterer's predicted labels to the dataframe that contains our visualization data and re-plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "902d168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d0ab88aa422e4fb485da9ba889c39358\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d0ab88aa422e4fb485da9ba889c39358\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d0ab88aa422e4fb485da9ba889c39358\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0bb6dd7c2c952879b850154984e35506\"}, \"mark\": {\"type\": \"circle\", \"size\": 30}, \"encoding\": {\"color\": {\"field\": \"CLUSTER\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"LABEL\", \"type\": \"nominal\"}, {\"field\": \"CLUSTER\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"X\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Y\", \"type\": \"quantitative\"}}, \"height\": 650, \"selection\": {\"selector003\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 650, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0bb6dd7c2c952879b850154984e35506\": [{\"X\": 9.629589080810547, \"Y\": 0.9365087747573853, \"LABEL\": \"Ada Lovelace\", \"CLUSTER\": 2}, {\"X\": 4.867339134216309, \"Y\": -13.727782249450684, \"LABEL\": \"Robert E Lee\", \"CLUSTER\": 13}, {\"X\": 2.516657829284668, \"Y\": -18.354692459106445, \"LABEL\": \"Andrew Johnson\", \"CLUSTER\": 8}, {\"X\": 5.089731216430664, \"Y\": -12.619865417480469, \"LABEL\": \"Bedford Forrest\", \"CLUSTER\": 13}, {\"X\": -0.30361512303352356, \"Y\": -9.816756248474121, \"LABEL\": \"Lucretia Mott\", \"CLUSTER\": 5}, {\"X\": 11.412369728088379, \"Y\": 0.43414148688316345, \"LABEL\": \"Charles Darwin\", \"CLUSTER\": 2}, {\"X\": 3.7749245166778564, \"Y\": -13.713809967041016, \"LABEL\": \"Ulysses Grant\", \"CLUSTER\": 13}, {\"X\": -10.804316520690918, \"Y\": 3.915217161178589, \"LABEL\": \"Mary Ewing Outerbridge\", \"CLUSTER\": 15}, {\"X\": 5.313749313354492, \"Y\": 7.089348793029785, \"LABEL\": \"Emma Lazarus\", \"CLUSTER\": 9}, {\"X\": 0.1769566833972931, \"Y\": 4.888604164123535, \"LABEL\": \"Louisa M Alcott\", \"CLUSTER\": 9}, {\"X\": -2.5393667221069336, \"Y\": -1.0644351243972778, \"LABEL\": \"P T Barnum\", \"CLUSTER\": 9}, {\"X\": 2.2706544399261475, \"Y\": 4.895766735076904, \"LABEL\": \"R L Stevenson\", \"CLUSTER\": 9}, {\"X\": 1.4236607551574707, \"Y\": -11.55919361114502, \"LABEL\": \"Fred Douglass\", \"CLUSTER\": 5}, {\"X\": 1.1170852184295654, \"Y\": 5.052165508270264, \"LABEL\": \"Harriet Beecher Stowe\", \"CLUSTER\": 9}, {\"X\": 1.3462700843811035, \"Y\": 3.350062131881714, \"LABEL\": \"Stephen Crane\", \"CLUSTER\": 9}, {\"X\": 11.823625564575195, \"Y\": 5.069955348968506, \"LABEL\": \"Nietzsche\", \"CLUSTER\": 2}, {\"X\": 3.474562406539917, \"Y\": -14.088415145874023, \"LABEL\": \"William McKinley\", \"CLUSTER\": 13}, {\"X\": -5.221592903137207, \"Y\": -9.45670223236084, \"LABEL\": \"Queen Victoria\", \"CLUSTER\": 13}, {\"X\": -0.3932449519634247, \"Y\": -16.153573989868164, \"LABEL\": \"Benjamin Harrison\", \"CLUSTER\": 8}, {\"X\": 0.8819878101348877, \"Y\": -10.319847106933594, \"LABEL\": \"Elizabeth Cady Stanton\", \"CLUSTER\": 5}, {\"X\": 5.680935859680176, \"Y\": 11.466192245483398, \"LABEL\": \"James M N Whistler\", \"CLUSTER\": 10}, {\"X\": -6.502901077270508, \"Y\": -3.6163651943206787, \"LABEL\": \"Emily Warren Roebling\", \"CLUSTER\": 18}, {\"X\": 0.9912346601486206, \"Y\": -10.336971282958984, \"LABEL\": \"Susan B Anthony\", \"CLUSTER\": 5}, {\"X\": 13.706411361694336, \"Y\": -15.936660766601562, \"LABEL\": \"Qiu Jin\", \"CLUSTER\": 4}, {\"X\": 1.1903727054595947, \"Y\": -15.833939552307129, \"LABEL\": \"Cleveland\", \"CLUSTER\": 8}, {\"X\": 0.06014833599328995, \"Y\": 3.009098529815674, \"LABEL\": \"Sarah Orne Jewett\", \"CLUSTER\": 9}, {\"X\": 5.059043884277344, \"Y\": -12.145303726196289, \"LABEL\": \"Geronimo\", \"CLUSTER\": 13}, {\"X\": 8.49888801574707, \"Y\": -6.145576000213623, \"LABEL\": \"William James\", \"CLUSTER\": 5}, {\"X\": -5.059149742126465, \"Y\": -8.053226470947266, \"LABEL\": \"Florence Nightingale\", \"CLUSTER\": 9}, {\"X\": 9.45051097869873, \"Y\": 4.464698314666748, \"LABEL\": \"Tolstoy\", \"CLUSTER\": 2}, {\"X\": 2.86517333984375, \"Y\": -4.517377853393555, \"LABEL\": \"Joseph Pulitzer\", \"CLUSTER\": 5}, {\"X\": 7.037650108337402, \"Y\": -10.98705768585205, \"LABEL\": \"John P Holland\", \"CLUSTER\": 13}, {\"X\": 7.9997172355651855, \"Y\": -11.967913627624512, \"LABEL\": \"Alfred Thayer Mahan\", \"CLUSTER\": 13}, {\"X\": 7.371427536010742, \"Y\": -2.075955390930176, \"LABEL\": \"John Muir\", \"CLUSTER\": 5}, {\"X\": -0.6195724010467529, \"Y\": -4.894759654998779, \"LABEL\": \"F W Taylor\", \"CLUSTER\": 18}, {\"X\": -1.4776277542114258, \"Y\": -9.218297958374023, \"LABEL\": \"B T Washington\", \"CLUSTER\": 5}, {\"X\": -3.7878124713897705, \"Y\": -4.599079608917236, \"LABEL\": \"J J Hill\", \"CLUSTER\": 18}, {\"X\": -1.648998737335205, \"Y\": 2.9855120182037354, \"LABEL\": \"Jack London\", \"CLUSTER\": 9}, {\"X\": 9.92032241821289, \"Y\": -1.5003327131271362, \"LABEL\": \"Martian Theory\", \"CLUSTER\": 6}, {\"X\": 7.427640914916992, \"Y\": 11.48624324798584, \"LABEL\": \"Hilaire G E Degas\", \"CLUSTER\": 10}, {\"X\": -4.147123336791992, \"Y\": -3.4893581867218018, \"LABEL\": \"C J Walker\", \"CLUSTER\": 18}, {\"X\": -2.7459497451782227, \"Y\": -4.899856090545654, \"LABEL\": \"Carnegie Started\", \"CLUSTER\": 18}, {\"X\": 0.44971612095832825, \"Y\": -10.595927238464355, \"LABEL\": \"Anna H Shaw\", \"CLUSTER\": 5}, {\"X\": -1.8113512992858887, \"Y\": 13.811561584472656, \"LABEL\": \"Marlene Dietrich\", \"CLUSTER\": 14}, {\"X\": -1.9595662355422974, \"Y\": 1.944274663925171, \"LABEL\": \"Nellie Bly\", \"CLUSTER\": 9}, {\"X\": -3.6001923084259033, \"Y\": 0.19057707488536835, \"LABEL\": \"Alexander Graham Bell\", \"CLUSTER\": 3}, {\"X\": 1.1338599920272827, \"Y\": -17.525060653686523, \"LABEL\": \"Warren Harding\", \"CLUSTER\": 8}, {\"X\": -3.8007283210754395, \"Y\": 5.842870712280273, \"LABEL\": \"Harry Houdini\", \"CLUSTER\": 9}, {\"X\": 0.49707192182540894, \"Y\": -9.317200660705566, \"LABEL\": \"Victoria Martin\", \"CLUSTER\": 5}, {\"X\": -2.137650966644287, \"Y\": -11.47287368774414, \"LABEL\": \"Mabel Craty\", \"CLUSTER\": 5}, {\"X\": 13.6246976852417, \"Y\": -0.4075273275375366, \"LABEL\": \"Marie Curie\", \"CLUSTER\": 6}, {\"X\": 9.034079551696777, \"Y\": -13.869738578796387, \"LABEL\": \"Balfour\", \"CLUSTER\": 4}, {\"X\": 6.1557183265686035, \"Y\": -4.759181499481201, \"LABEL\": \"Elmer Sperry\", \"CLUSTER\": 6}, {\"X\": 2.7730519771575928, \"Y\": -17.90154457092285, \"LABEL\": \"William Howard Taft\", \"CLUSTER\": 8}, {\"X\": 4.122943878173828, \"Y\": 5.514073848724365, \"LABEL\": \"Conan Doyle\", \"CLUSTER\": 9}, {\"X\": 3.894010305404663, \"Y\": -7.373220443725586, \"LABEL\": \"Ida B Wells\", \"CLUSTER\": 5}, {\"X\": -0.6402124762535095, \"Y\": -2.9942216873168945, \"LABEL\": \"Melvil Dewey\", \"CLUSTER\": 5}, {\"X\": 0.8306495547294617, \"Y\": 10.522436141967773, \"LABEL\": \"Thomas Edison\", \"CLUSTER\": 3}, {\"X\": -13.630939483642578, \"Y\": 3.5920586585998535, \"LABEL\": \"Knute Rocke\", \"CLUSTER\": 1}, {\"X\": -11.455645561218262, \"Y\": 17.965166091918945, \"LABEL\": \"John Philip Sousa\", \"CLUSTER\": 9}, {\"X\": -6.687372207641602, \"Y\": 12.15854263305664, \"LABEL\": \"Florenz Ziegfeld\", \"CLUSTER\": 14}, {\"X\": 2.818314552307129, \"Y\": -17.133880615234375, \"LABEL\": \"Calvin Coolidge\", \"CLUSTER\": 8}, {\"X\": -1.5672276020050049, \"Y\": 4.57751989364624, \"LABEL\": \"Ring Lardner\", \"CLUSTER\": 9}, {\"X\": -7.8486647605896, \"Y\": -0.3336794078350067, \"LABEL\": \"Louis C Tiffany\", \"CLUSTER\": 10}, {\"X\": -3.5341227054595947, \"Y\": 0.47870972752571106, \"LABEL\": \"T A Watson\", \"CLUSTER\": 3}, {\"X\": 4.497917652130127, \"Y\": -9.539314270019531, \"LABEL\": \"Justice Holmes\", \"CLUSTER\": 8}, {\"X\": 2.8870668411254883, \"Y\": -10.985051155090332, \"LABEL\": \"Jane Addams\", \"CLUSTER\": 5}, {\"X\": -2.852731466293335, \"Y\": 5.445327281951904, \"LABEL\": \"Will Rogers\", \"CLUSTER\": 9}, {\"X\": 2.634998083114624, \"Y\": -4.535669803619385, \"LABEL\": \"Adolph S Ochs\", \"CLUSTER\": 5}, {\"X\": -0.8043901920318604, \"Y\": 6.835999965667725, \"LABEL\": \"Anne Macy\", \"CLUSTER\": 9}, {\"X\": -14.196510314941406, \"Y\": 2.408979892730713, \"LABEL\": \"John W Heisman\", \"CLUSTER\": 1}, {\"X\": 9.145484924316406, \"Y\": 4.776538848876953, \"LABEL\": \"Maxim Gorky\", \"CLUSTER\": 2}, {\"X\": -14.743207931518555, \"Y\": 16.750234603881836, \"LABEL\": \"Maurice Ravel\", \"CLUSTER\": 7}, {\"X\": 2.715054750442505, \"Y\": 4.199475288391113, \"LABEL\": \"Edith Wharton\", \"CLUSTER\": 9}, {\"X\": -2.939866542816162, \"Y\": -3.85809588432312, \"LABEL\": \"John Rockefeller\", \"CLUSTER\": 18}, {\"X\": 4.025189399719238, \"Y\": -9.70034122467041, \"LABEL\": \"Clarence Darrow\", \"CLUSTER\": 8}, {\"X\": 13.446882247924805, \"Y\": -3.987703800201416, \"LABEL\": \"George E Hale\", \"CLUSTER\": 6}, {\"X\": 2.4567582607269287, \"Y\": 15.349662780761719, \"LABEL\": \"Constantin Stanislavsky\", \"CLUSTER\": 7}, {\"X\": 3.4906468391418457, \"Y\": 7.675736427307129, \"LABEL\": \"W B Yeats\", \"CLUSTER\": 9}, {\"X\": -8.473063468933105, \"Y\": -9.35084056854248, \"LABEL\": \"Pope Pius XI\", \"CLUSTER\": 2}, {\"X\": -5.475143909454346, \"Y\": 3.418489933013916, \"LABEL\": \"Howard Carter\", \"CLUSTER\": 3}, {\"X\": 1.8188273906707764, \"Y\": 4.609699726104736, \"LABEL\": \"Scott Fitzgerald\", \"CLUSTER\": 9}, {\"X\": -1.1627827882766724, \"Y\": -7.4035563468933105, \"LABEL\": \"Marcus Garvey\", \"CLUSTER\": 18}, {\"X\": -4.838081359863281, \"Y\": 0.37718555331230164, \"LABEL\": \"Frank Conrad\", \"CLUSTER\": 3}, {\"X\": -16.145322799682617, \"Y\": 6.350594997406006, \"LABEL\": \"Lou Gehrig\", \"CLUSTER\": 12}, {\"X\": 3.6721866130828857, \"Y\": 4.287343502044678, \"LABEL\": \"James Joyce\", \"CLUSTER\": 9}, {\"X\": 2.9453887939453125, \"Y\": 2.5019946098327637, \"LABEL\": \"Virginia Woolf\", \"CLUSTER\": 9}, {\"X\": -7.108323097229004, \"Y\": 14.725702285766602, \"LABEL\": \"George M Cohan\", \"CLUSTER\": 14}, {\"X\": 8.089288711547852, \"Y\": -3.9795265197753906, \"LABEL\": \"J H Kellogg\", \"CLUSTER\": 17}, {\"X\": 7.745167255401611, \"Y\": -3.375455617904663, \"LABEL\": \"George Washington Carver\", \"CLUSTER\": 17}, {\"X\": 0.2762145698070526, \"Y\": -17.672365188598633, \"LABEL\": \"Alfred E Smith\", \"CLUSTER\": 8}, {\"X\": 4.564617156982422, \"Y\": -2.2918553352355957, \"LABEL\": \"Ida M Tarbell\", \"CLUSTER\": 9}, {\"X\": -3.5655741691589355, \"Y\": 4.17132568359375, \"LABEL\": \"Ernie Pyle\", \"CLUSTER\": 13}, {\"X\": 4.576817512512207, \"Y\": -17.442001342773438, \"LABEL\": \"Harry S Truman\", \"CLUSTER\": 4}, {\"X\": 5.769756317138672, \"Y\": -13.118968963623047, \"LABEL\": \"George Patton\", \"CLUSTER\": 13}, {\"X\": -0.487134724855423, \"Y\": -17.166996002197266, \"LABEL\": \"FDR\", \"CLUSTER\": 8}, {\"X\": -10.25243854522705, \"Y\": 16.744211196899414, \"LABEL\": \"Jerome Kern\", \"CLUSTER\": 11}, {\"X\": 10.450125694274902, \"Y\": -17.857093811035156, \"LABEL\": \"Adolf Hitler\", \"CLUSTER\": 4}, {\"X\": -15.567388534545898, \"Y\": 17.081615447998047, \"LABEL\": \"Bela Bartok\", \"CLUSTER\": 7}, {\"X\": 3.3424673080444336, \"Y\": 4.723523139953613, \"LABEL\": \"Gertrude Stein\", \"CLUSTER\": 9}, {\"X\": 9.325911521911621, \"Y\": -12.526713371276855, \"LABEL\": \"Lord Keynes\", \"CLUSTER\": 4}, {\"X\": 14.586517333984375, \"Y\": -4.484496593475342, \"LABEL\": \"C E M Clung\", \"CLUSTER\": 6}, {\"X\": 1.1403337717056274, \"Y\": 2.509639024734497, \"LABEL\": \"Willa Cather\", \"CLUSTER\": 9}, {\"X\": -4.850642204284668, \"Y\": 6.138102054595947, \"LABEL\": \"Al Capone\", \"CLUSTER\": 13}, {\"X\": 2.3354763984680176, \"Y\": -15.820819854736328, \"LABEL\": \"Fiorello La Guardia\", \"CLUSTER\": 8}, {\"X\": 14.609024047851562, \"Y\": -0.10717794299125671, \"LABEL\": \"Max Planck\", \"CLUSTER\": 6}, {\"X\": -1.1804206371307373, \"Y\": -5.477313041687012, \"LABEL\": \"Henry Ford\", \"CLUSTER\": 18}, {\"X\": 6.029560565948486, \"Y\": -13.791752815246582, \"LABEL\": \"John Pershing\", \"CLUSTER\": 13}, {\"X\": 2.0751850605010986, \"Y\": 15.046469688415527, \"LABEL\": \"Sergei Eisenstein\", \"CLUSTER\": 7}, {\"X\": 8.11977481842041, \"Y\": -20.070783615112305, \"LABEL\": \"Mohandas K Gandhi\", \"CLUSTER\": 4}, {\"X\": -14.677245140075684, \"Y\": 4.58611536026001, \"LABEL\": \"Babe Ruth\", \"CLUSTER\": 1}, {\"X\": -0.5605786442756653, \"Y\": 1.7567886114120483, \"LABEL\": \"Mitchell\", \"CLUSTER\": 9}, {\"X\": 14.690046310424805, \"Y\": -2.3300743103027344, \"LABEL\": \"A J Dempster\", \"CLUSTER\": 6}, {\"X\": 0.5355619788169861, \"Y\": 4.450295925140381, \"LABEL\": \"Edna St V Millay\", \"CLUSTER\": 9}, {\"X\": 5.150554656982422, \"Y\": -16.838817596435547, \"LABEL\": \"Henry L Stimson\", \"CLUSTER\": 4}, {\"X\": -7.985596656799316, \"Y\": 14.369399070739746, \"LABEL\": \"Fanny Brice\", \"CLUSTER\": 14}, {\"X\": 10.201523780822754, \"Y\": -3.951920509338379, \"LABEL\": \"Henrietta Lacks\", \"CLUSTER\": 17}, {\"X\": 7.256105422973633, \"Y\": -15.879838943481445, \"LABEL\": \"Eva Peron\", \"CLUSTER\": 4}, {\"X\": 7.958618640899658, \"Y\": -7.40358304977417, \"LABEL\": \"John Dewey\", \"CLUSTER\": 5}, {\"X\": 8.137778282165527, \"Y\": -22.409303665161133, \"LABEL\": \"Chaim Weizmann\", \"CLUSTER\": 16}, {\"X\": -3.047520399093628, \"Y\": -6.486069679260254, \"LABEL\": \"Charles Spaulding\", \"CLUSTER\": 5}, {\"X\": 3.39670467376709, \"Y\": -18.607646942138672, \"LABEL\": \"Fred Vinson\", \"CLUSTER\": 8}, {\"X\": 0.5231119394302368, \"Y\": 2.0844290256500244, \"LABEL\": \"Marjorie Rawlings\", \"CLUSTER\": 9}, {\"X\": 11.478231430053711, \"Y\": -18.480487823486328, \"LABEL\": \"Joseph Stalin\", \"CLUSTER\": 4}, {\"X\": -13.02806568145752, \"Y\": 4.952950477600098, \"LABEL\": \"Jim Thorpe\", \"CLUSTER\": 15}, {\"X\": -1.8037348985671997, \"Y\": 3.449126958847046, \"LABEL\": \"Eugene O Neill\", \"CLUSTER\": 9}, {\"X\": 3.311312437057495, \"Y\": -3.8299105167388916, \"LABEL\": \"Anne O Hare McCormick\", \"CLUSTER\": 5}, {\"X\": 5.014588832855225, \"Y\": 10.005289077758789, \"LABEL\": \"Frida Kahlo\", \"CLUSTER\": 10}, {\"X\": 8.413653373718262, \"Y\": -17.18549156188965, \"LABEL\": \"Getulio Vargas\", \"CLUSTER\": 4}, {\"X\": 14.445099830627441, \"Y\": -1.8850390911102295, \"LABEL\": \"Enrico Fermi\", \"CLUSTER\": 6}, {\"X\": 6.477755069732666, \"Y\": 11.336223602294922, \"LABEL\": \"Henri Matisse\", \"CLUSTER\": 10}, {\"X\": 7.114038944244385, \"Y\": -2.987826347351074, \"LABEL\": \"Liberty H Bailey\", \"CLUSTER\": 5}, {\"X\": -5.830735206604004, \"Y\": 15.424117088317871, \"LABEL\": \"Lionel Barrymore\", \"CLUSTER\": 14}, {\"X\": 9.499425888061523, \"Y\": 6.423043251037598, \"LABEL\": \"Thomas Mann\", \"CLUSTER\": 2}, {\"X\": 12.435925483703613, \"Y\": 0.6852051019668579, \"LABEL\": \"Albert Einstein\", \"CLUSTER\": 2}, {\"X\": -12.06922435760498, \"Y\": 6.419966220855713, \"LABEL\": \"Margaret Abbott\", \"CLUSTER\": 15}, {\"X\": -0.3511526584625244, \"Y\": -12.968914031982422, \"LABEL\": \"Walter White\", \"CLUSTER\": 5}, {\"X\": -15.838933944702148, \"Y\": 5.7552266120910645, \"LABEL\": \"Cy Young\", \"CLUSTER\": 1}, {\"X\": -0.6612284779548645, \"Y\": -0.9929090142250061, \"LABEL\": \"Dale Carnegie\", \"CLUSTER\": 5}, {\"X\": -11.752488136291504, \"Y\": 4.983523368835449, \"LABEL\": \"Babe Zaharias\", \"CLUSTER\": 15}, {\"X\": -2.0537309646606445, \"Y\": -3.7528295516967773, \"LABEL\": \"Charles Merrill\", \"CLUSTER\": 18}, {\"X\": -2.085387706756592, \"Y\": -4.954380035400391, \"LABEL\": \"Thomas J Watson Sr\", \"CLUSTER\": 18}, {\"X\": -2.122974395751953, \"Y\": -5.693045616149902, \"LABEL\": \"Gerard Swope\", \"CLUSTER\": 18}, {\"X\": 6.393711090087891, \"Y\": 13.563359260559082, \"LABEL\": \"Christian Dior\", \"CLUSTER\": 10}, {\"X\": -10.264820098876953, \"Y\": 19.21905517578125, \"LABEL\": \"W C Handy\", \"CLUSTER\": 11}, {\"X\": -9.749808311462402, \"Y\": 19.22344207763672, \"LABEL\": \"Billie Holiday\", \"CLUSTER\": 11}, {\"X\": -7.139647483825684, \"Y\": -2.679274320602417, \"LABEL\": \"Frank Lloyd Wright\", \"CLUSTER\": 10}, {\"X\": -6.252832412719727, \"Y\": 15.452739715576172, \"LABEL\": \"Ethel Barrymore\", \"CLUSTER\": 14}, {\"X\": -1.6900622844696045, \"Y\": 16.44804573059082, \"LABEL\": \"Cecil De Mille\", \"CLUSTER\": 14}, {\"X\": 14.377174377441406, \"Y\": -4.461243152618408, \"LABEL\": \"Ross G Harrison\", \"CLUSTER\": 6}, {\"X\": 5.730043411254883, \"Y\": -17.34250259399414, \"LABEL\": \"John Dulles\", \"CLUSTER\": 4}, {\"X\": 8.909778594970703, \"Y\": 5.260741233825684, \"LABEL\": \"Boris Pasternak\", \"CLUSTER\": 2}, {\"X\": 15.18835163116455, \"Y\": -3.321732997894287, \"LABEL\": \"Beno Gutenberg\", \"CLUSTER\": 6}, {\"X\": 6.0727105140686035, \"Y\": 0.7039583921432495, \"LABEL\": \"Emily Post\", \"CLUSTER\": 9}, {\"X\": 1.2883164882659912, \"Y\": 0.7812788486480713, \"LABEL\": \"Richard Wright\", \"CLUSTER\": 9}, {\"X\": 6.606455326080322, \"Y\": -17.90180778503418, \"LABEL\": \"Hammarskjold\", \"CLUSTER\": 4}, {\"X\": 2.51444411277771, \"Y\": 5.4511637687683105, \"LABEL\": \"Ernest Hemingway\", \"CLUSTER\": 9}, {\"X\": 3.3103435039520264, \"Y\": 9.779006958007812, \"LABEL\": \"Primitive Artist\", \"CLUSTER\": 10}, {\"X\": -1.2825796604156494, \"Y\": -11.307229995727539, \"LABEL\": \"Emily Balch\", \"CLUSTER\": 5}, {\"X\": 1.1649328470230103, \"Y\": -18.612628936767578, \"LABEL\": \"Sam Rayburn\", \"CLUSTER\": 8}, {\"X\": 12.17062759399414, \"Y\": 2.2361319065093994, \"LABEL\": \"Carl G Jung\", \"CLUSTER\": 2}, {\"X\": -4.2258782386779785, \"Y\": 10.714470863342285, \"LABEL\": \"Marilyn Monroe\", \"CLUSTER\": 14}, {\"X\": 1.90577232837677, \"Y\": -14.886177062988281, \"LABEL\": \"Eleanor Roosevelt\", \"CLUSTER\": 5}, {\"X\": 3.077925443649292, \"Y\": 3.4568052291870117, \"LABEL\": \"William Faulkner\", \"CLUSTER\": 9}, {\"X\": 3.701728343963623, \"Y\": 2.523127794265747, \"LABEL\": \"Sylvia Plath\", \"CLUSTER\": 9}, {\"X\": 2.2289395332336426, \"Y\": -19.08131217956543, \"LABEL\": \"John F Kennedy\", \"CLUSTER\": 8}, {\"X\": 4.192113876342773, \"Y\": 3.648496150970459, \"LABEL\": \"Robert Frost\", \"CLUSTER\": 9}, {\"X\": -0.7063899636268616, \"Y\": -11.29255485534668, \"LABEL\": \"W E B DuBois\", \"CLUSTER\": 5}, {\"X\": -1.7259103059768677, \"Y\": -14.54096508026123, \"LABEL\": \"Herbert Hoover\", \"CLUSTER\": 5}, {\"X\": 6.282223224639893, \"Y\": -13.74743938446045, \"LABEL\": \"Douglas MacArthur\", \"CLUSTER\": 13}, {\"X\": 1.7478026151657104, \"Y\": 7.9571709632873535, \"LABEL\": \"Sean O Casey\", \"CLUSTER\": 9}, {\"X\": 8.371752738952637, \"Y\": -1.8794926404953003, \"LABEL\": \"Rachel Carson\", \"CLUSTER\": 17}, {\"X\": -9.30225944519043, \"Y\": 16.191822052001953, \"LABEL\": \"Cole Porter\", \"CLUSTER\": 11}, {\"X\": 3.076063871383667, \"Y\": 1.314249038696289, \"LABEL\": \"Nella Larsen\", \"CLUSTER\": 9}, {\"X\": 1.6825166940689087, \"Y\": -17.522794723510742, \"LABEL\": \"Adlai Ewing Stevenson\", \"CLUSTER\": 8}, {\"X\": -3.164984941482544, \"Y\": 17.577924728393555, \"LABEL\": \"David O Selznick\", \"CLUSTER\": 14}, {\"X\": 8.69693660736084, \"Y\": -14.189995765686035, \"LABEL\": \"Churchill\", \"CLUSTER\": 4}, {\"X\": -15.152002334594727, \"Y\": 3.9001054763793945, \"LABEL\": \"Branch Rickey\", \"CLUSTER\": 1}, {\"X\": 11.748671531677246, \"Y\": 4.267360687255859, \"LABEL\": \"Martin Buber\", \"CLUSTER\": 2}, {\"X\": 1.4773584604263306, \"Y\": -3.502977132797241, \"LABEL\": \"Edward R Murrow\", \"CLUSTER\": 5}, {\"X\": 10.981849670410156, \"Y\": 3.3068172931671143, \"LABEL\": \"Albert Schweitzer\", \"CLUSTER\": 2}, {\"X\": 1.810014009475708, \"Y\": 6.656957626342773, \"LABEL\": \"Shirley Jackson\", \"CLUSTER\": 9}, {\"X\": 1.8925938606262207, \"Y\": -9.45506477355957, \"LABEL\": \"Margaret Sanger\", \"CLUSTER\": 5}, {\"X\": 6.807074069976807, \"Y\": -11.948288917541504, \"LABEL\": \"Chester Nimitz\", \"CLUSTER\": 13}, {\"X\": -2.056762218475342, \"Y\": 12.418746948242188, \"LABEL\": \"Buster Keaton\", \"CLUSTER\": 14}, {\"X\": -2.6335644721984863, \"Y\": 6.498964309692383, \"LABEL\": \"Lenny Bruce\", \"CLUSTER\": 9}, {\"X\": -0.9291820526123047, \"Y\": 17.55532455444336, \"LABEL\": \"Walt Disney\", \"CLUSTER\": 14}, {\"X\": -1.7889564037322998, \"Y\": -5.057743072509766, \"LABEL\": \"Alfred P Sloan Jr\", \"CLUSTER\": 18}, {\"X\": 10.934745788574219, \"Y\": -4.189286231994629, \"LABEL\": \"Gregory Pincus\", \"CLUSTER\": 17}, {\"X\": 3.474595785140991, \"Y\": -1.6320171356201172, \"LABEL\": \"Henry R Luce\", \"CLUSTER\": 9}, {\"X\": 5.184930801391602, \"Y\": 2.462585687637329, \"LABEL\": \"Langston Hughes\", \"CLUSTER\": 9}, {\"X\": 12.949685096740723, \"Y\": -1.5517901182174683, \"LABEL\": \"J Robert Oppenheimer\", \"CLUSTER\": 6}, {\"X\": 0.8506120443344116, \"Y\": -16.64697265625, \"LABEL\": \"Robert Francis Kennedy\", \"CLUSTER\": 8}, {\"X\": 0.06093268468976021, \"Y\": 6.959182262420654, \"LABEL\": \"Helen Keller\", \"CLUSTER\": 9}, {\"X\": 4.050485610961914, \"Y\": -1.4843933582305908, \"LABEL\": \"Upton Sinclair\", \"CLUSTER\": 9}, {\"X\": 4.620257377624512, \"Y\": -8.236494064331055, \"LABEL\": \"Martin Luther King Jr\", \"CLUSTER\": 5}, {\"X\": -6.331336498260498, \"Y\": -12.70248031616211, \"LABEL\": \"Yuri Gagarin\", \"CLUSTER\": 3}, {\"X\": -7.336552619934082, \"Y\": -2.553863286972046, \"LABEL\": \"Mies van der Rohe\", \"CLUSTER\": 10}, {\"X\": 4.473064422607422, \"Y\": -17.60696029663086, \"LABEL\": \"David Eisenhower\", \"CLUSTER\": 4}, {\"X\": -12.037976264953613, \"Y\": 20.4371280670166, \"LABEL\": \"Coleman Hawkins\", \"CLUSTER\": 11}, {\"X\": -1.0962679386138916, \"Y\": 14.572998046875, \"LABEL\": \"Madhubala\", \"CLUSTER\": 14}, {\"X\": -6.652101516723633, \"Y\": 13.022965431213379, \"LABEL\": \"Judy Garland\", \"CLUSTER\": 11}, {\"X\": -11.519660949707031, \"Y\": 5.149566173553467, \"LABEL\": \"Maureen Connolly\", \"CLUSTER\": 15}, {\"X\": 11.490662574768066, \"Y\": -16.636425018310547, \"LABEL\": \"Ho Chi Minh\", \"CLUSTER\": 4}, {\"X\": -12.171581268310547, \"Y\": 7.268080234527588, \"LABEL\": \"Sonja Henie\", \"CLUSTER\": 15}, {\"X\": 2.202540159225464, \"Y\": -16.904741287231445, \"LABEL\": \"Everett Dirksen\", \"CLUSTER\": 8}, {\"X\": -2.9562528133392334, \"Y\": -14.008023262023926, \"LABEL\": \"Walter Reuther\", \"CLUSTER\": 5}, {\"X\": 10.0339937210083, \"Y\": -17.23975372314453, \"LABEL\": \"Edouard Daladier\", \"CLUSTER\": 4}, {\"X\": 8.901248931884766, \"Y\": 6.926815032958984, \"LABEL\": \"Erich Maria Remarque\", \"CLUSTER\": 2}, {\"X\": 9.946044921875, \"Y\": -16.896339416503906, \"LABEL\": \"De Gaulle Rallied\", \"CLUSTER\": 4}, {\"X\": 6.375982284545898, \"Y\": 14.089868545532227, \"LABEL\": \"Coco Chanel\", \"CLUSTER\": 10}, {\"X\": 5.421616554260254, \"Y\": -14.56261157989502, \"LABEL\": \"Florence Blanchfield\", \"CLUSTER\": 13}, {\"X\": 11.947996139526367, \"Y\": -18.720796585083008, \"LABEL\": \"Khrushchev\", \"CLUSTER\": 4}, {\"X\": 3.6141252517700195, \"Y\": 11.977758407592773, \"LABEL\": \"Diane Arbus\", \"CLUSTER\": 10}, {\"X\": 3.14437198638916, \"Y\": -11.699508666992188, \"LABEL\": \"Ralph Bunche\", \"CLUSTER\": 5}, {\"X\": -11.873806953430176, \"Y\": 5.324117183685303, \"LABEL\": \"Bobby Jones\", \"CLUSTER\": 15}, {\"X\": -10.47250747680664, \"Y\": 19.456758499145508, \"LABEL\": \"Louis Armstrong\", \"CLUSTER\": 11}, {\"X\": 5.490179538726807, \"Y\": -17.52106285095215, \"LABEL\": \"Dean Acheson\", \"CLUSTER\": 4}, {\"X\": -14.25207233428955, \"Y\": 16.682188034057617, \"LABEL\": \"Igor Stravinsky\", \"CLUSTER\": 7}, {\"X\": 3.6063179969787598, \"Y\": -20.43814468383789, \"LABEL\": \"Hugo Black\", \"CLUSTER\": 8}, {\"X\": -8.741172790527344, \"Y\": 20.10617446899414, \"LABEL\": \"Mahalia Jackson\", \"CLUSTER\": 11}, {\"X\": -14.888214111328125, \"Y\": 4.1791863441467285, \"LABEL\": \"Jackie Robinson\", \"CLUSTER\": 1}, {\"X\": -5.368350982666016, \"Y\": -9.634806632995605, \"LABEL\": \"The Duke of Windsor\", \"CLUSTER\": 13}, {\"X\": -0.6426783204078674, \"Y\": -14.689474105834961, \"LABEL\": \"J Edgar Hoover\", \"CLUSTER\": 5}, {\"X\": -2.308452844619751, \"Y\": -14.421500205993652, \"LABEL\": \"Lyndon Johnson\", \"CLUSTER\": 5}, {\"X\": -13.81699275970459, \"Y\": 16.135818481445312, \"LABEL\": \"Otto Klemperer\", \"CLUSTER\": 7}, {\"X\": 0.9663636684417725, \"Y\": -6.817568778991699, \"LABEL\": \"Eddie Rickenbacker\", \"CLUSTER\": 18}, {\"X\": 1.193108320236206, \"Y\": -20.33041000366211, \"LABEL\": \"Jeanette Rankin\", \"CLUSTER\": 8}, {\"X\": 6.1103901863098145, \"Y\": 10.780670166015625, \"LABEL\": \"Pablo Picasso\", \"CLUSTER\": 10}, {\"X\": 0.5244559049606323, \"Y\": -6.968406677246094, \"LABEL\": \"Roberto Clemente\", \"CLUSTER\": 4}, {\"X\": 3.218234062194824, \"Y\": 6.269067764282227, \"LABEL\": \"Nancy Mitford\", \"CLUSTER\": 9}, {\"X\": 3.557950735092163, \"Y\": -20.199440002441406, \"LABEL\": \"Earl Warren\", \"CLUSTER\": 8}, {\"X\": 1.1160459518432617, \"Y\": 8.574992179870605, \"LABEL\": \"Sylvia Plath\", \"CLUSTER\": 9}, {\"X\": -6.244418621063232, \"Y\": 10.41963005065918, \"LABEL\": \"Ed Sullivan\", \"CLUSTER\": 9}, {\"X\": -6.011885166168213, \"Y\": 14.240015029907227, \"LABEL\": \"Katharine Cornell\", \"CLUSTER\": 14}, {\"X\": 2.142029285430908, \"Y\": -6.557278156280518, \"LABEL\": \"Charles Lindbergh\", \"CLUSTER\": 5}, {\"X\": 11.842513084411621, \"Y\": -14.831551551818848, \"LABEL\": \"Haile Selassie\", \"CLUSTER\": 4}, {\"X\": 5.107857704162598, \"Y\": -7.8079938888549805, \"LABEL\": \"Elijah Muhammad\", \"CLUSTER\": 5}, {\"X\": 9.465653419494629, \"Y\": -17.16649627685547, \"LABEL\": \"Franco\", \"CLUSTER\": 4}, {\"X\": 3.864711284637451, \"Y\": 11.90059757232666, \"LABEL\": \"Walker Evans\", \"CLUSTER\": 10}, {\"X\": 12.281484603881836, \"Y\": -16.960769653320312, \"LABEL\": \"Chiang Kai shek\", \"CLUSTER\": 4}, {\"X\": -3.19472599029541, \"Y\": -3.142597198486328, \"LABEL\": \"J Paul Getty\", \"CLUSTER\": 18}, {\"X\": 12.985475540161133, \"Y\": -16.64879608154297, \"LABEL\": \"Mao Tse Tung\", \"CLUSTER\": 4}, {\"X\": 6.411359786987305, \"Y\": 10.695928573608398, \"LABEL\": \"Max Ernst\", \"CLUSTER\": 10}, {\"X\": -1.0085030794143677, \"Y\": -18.12965202331543, \"LABEL\": \"Richard Daley\", \"CLUSTER\": 8}, {\"X\": 13.216479301452637, \"Y\": -0.3249548673629761, \"LABEL\": \"Jacques Monod\", \"CLUSTER\": 6}, {\"X\": -1.367120623588562, \"Y\": 17.1641902923584, \"LABEL\": \"Adolph Zukor\", \"CLUSTER\": 14}, {\"X\": -2.0315723419189453, \"Y\": 12.687402725219727, \"LABEL\": \"Charles Chaplin\", \"CLUSTER\": 14}, {\"X\": -5.378741264343262, \"Y\": 13.529718399047852, \"LABEL\": \"Joan Crawford\", \"CLUSTER\": 14}, {\"X\": 9.092671394348145, \"Y\": -14.452479362487793, \"LABEL\": \"Dash Ended\", \"CLUSTER\": 4}, {\"X\": -11.737982749938965, \"Y\": 15.359981536865234, \"LABEL\": \"Maria Callas\", \"CLUSTER\": 7}, {\"X\": 8.854317665100098, \"Y\": -22.21473503112793, \"LABEL\": \"Golda Meir\", \"CLUSTER\": 16}, {\"X\": 10.609031677246094, \"Y\": 1.3082987070083618, \"LABEL\": \"Margaret Mead\", \"CLUSTER\": 2}, {\"X\": 6.118866920471191, \"Y\": -9.024884223937988, \"LABEL\": \"Pope Paul VI\", \"CLUSTER\": 2}, {\"X\": -7.283303260803223, \"Y\": -6.654460906982422, \"LABEL\": \"Bruce Catton\", \"CLUSTER\": 5}, {\"X\": -12.953668594360352, \"Y\": 16.32244873046875, \"LABEL\": \"Arthur Fiedler\", \"CLUSTER\": 7}, {\"X\": -3.606452465057373, \"Y\": 12.465727806091309, \"LABEL\": \"John Wayne\", \"CLUSTER\": 14}, {\"X\": 1.2237907648086548, \"Y\": -13.392544746398926, \"LABEL\": \"A Philip Randolph\", \"CLUSTER\": 5}, {\"X\": -12.13571548461914, \"Y\": 19.790708541870117, \"LABEL\": \"Stan Kenton\", \"CLUSTER\": 11}, {\"X\": -9.776265144348145, \"Y\": 16.63808250427246, \"LABEL\": \"Richard Rodgers\", \"CLUSTER\": 11}, {\"X\": -12.791735649108887, \"Y\": 4.72695779800415, \"LABEL\": \"Jesse Owens\", \"CLUSTER\": 15}, {\"X\": 0.1744154691696167, \"Y\": 13.81369400024414, \"LABEL\": \"Alfred Hitchcock\", \"CLUSTER\": 14}, {\"X\": 11.404035568237305, \"Y\": 1.2002414464950562, \"LABEL\": \"Jean Piaget\", \"CLUSTER\": 2}, {\"X\": 10.629410743713379, \"Y\": 5.37824010848999, \"LABEL\": \"Jean Paul Sartre\", \"CLUSTER\": 2}, {\"X\": -9.51002311706543, \"Y\": 6.543694019317627, \"LABEL\": \"Joe Louis\", \"CLUSTER\": 15}, {\"X\": -4.598476886749268, \"Y\": -5.0298991203308105, \"LABEL\": \"Robert Moses\", \"CLUSTER\": 18}, {\"X\": 9.557087898254395, \"Y\": -22.23974609375, \"LABEL\": \"Anwar el Sadat\", \"CLUSTER\": 16}, {\"X\": -4.135482311248779, \"Y\": 14.641820907592773, \"LABEL\": \"Ingrid Bergman\", \"CLUSTER\": 14}, {\"X\": 11.428175926208496, \"Y\": 2.1299140453338623, \"LABEL\": \"Anna Freud\", \"CLUSTER\": 2}, {\"X\": 11.94921875, \"Y\": -19.105857849121094, \"LABEL\": \"Leonid Brezhnev\", \"CLUSTER\": 4}, {\"X\": -13.345328330993652, \"Y\": 17.284900665283203, \"LABEL\": \"Arthur Rubinstein\", \"CLUSTER\": 7}, {\"X\": -11.595566749572754, \"Y\": 19.937498092651367, \"LABEL\": \"Thelonious Monk\", \"CLUSTER\": 11}, {\"X\": -4.014244556427002, \"Y\": 16.423534393310547, \"LABEL\": \"Lee Strasberg\", \"CLUSTER\": 14}, {\"X\": -15.019454956054688, \"Y\": 4.6996564865112305, \"LABEL\": \"Satchel Paige\", \"CLUSTER\": 1}, {\"X\": -9.574095726013184, \"Y\": 6.669792175292969, \"LABEL\": \"Jack Dempsey\", \"CLUSTER\": 15}, {\"X\": -11.981386184692383, \"Y\": 21.897388458251953, \"LABEL\": \"Earl Hines\", \"CLUSTER\": 11}, {\"X\": -10.348932266235352, \"Y\": 21.249935150146484, \"LABEL\": \"Muddy Waters\", \"CLUSTER\": 11}, {\"X\": 2.417151689529419, \"Y\": 5.5440239906311035, \"LABEL\": \"Truman Capote\", \"CLUSTER\": 9}, {\"X\": 0.5363287925720215, \"Y\": 6.157252311706543, \"LABEL\": \"Lillian Hellman\", \"CLUSTER\": 9}, {\"X\": -10.660518646240234, \"Y\": 7.940510272979736, \"LABEL\": \"Johnny Weissmuller\", \"CLUSTER\": 15}, {\"X\": -7.373063564300537, \"Y\": 0.5054081082344055, \"LABEL\": \"Ansel Adams\", \"CLUSTER\": 10}, {\"X\": -8.515951156616211, \"Y\": 15.578156471252441, \"LABEL\": \"Ethel Merman\", \"CLUSTER\": 11}, {\"X\": 8.100703239440918, \"Y\": -20.053998947143555, \"LABEL\": \"Indira Gandhi\", \"CLUSTER\": 4}, {\"X\": -3.5809130668640137, \"Y\": -2.1500587463378906, \"LABEL\": \"Ray A Kroc\", \"CLUSTER\": 18}, {\"X\": -5.22397518157959, \"Y\": 15.093145370483398, \"LABEL\": \"Richard Burton\", \"CLUSTER\": 14}, {\"X\": -11.587529182434082, \"Y\": 21.17261505126953, \"LABEL\": \"Count Basie\", \"CLUSTER\": 11}, {\"X\": 1.3015097379684448, \"Y\": 7.229896545410156, \"LABEL\": \"E B White\", \"CLUSTER\": 9}, {\"X\": -2.68548321723938, \"Y\": 15.73446273803711, \"LABEL\": \"Orson Welles\", \"CLUSTER\": 14}, {\"X\": -15.570432662963867, \"Y\": 5.330615520477295, \"LABEL\": \"Roger Maris\", \"CLUSTER\": 1}, {\"X\": -3.728980779647827, \"Y\": 13.766958236694336, \"LABEL\": \"James Cagney\", \"CLUSTER\": 14}, {\"X\": 4.857714653015137, \"Y\": 11.632116317749023, \"LABEL\": \"Georgia O Keeffe\", \"CLUSTER\": 10}, {\"X\": -11.422660827636719, \"Y\": 20.63717269897461, \"LABEL\": \"Benny Goodman\", \"CLUSTER\": 11}, {\"X\": 6.449415683746338, \"Y\": 5.915811061859131, \"LABEL\": \"Jorge Luis Borges\", \"CLUSTER\": 9}, {\"X\": 5.973481178283691, \"Y\": 5.009847164154053, \"LABEL\": \"Bernard Malamud\", \"CLUSTER\": 9}, {\"X\": -7.632770538330078, \"Y\": 10.633408546447754, \"LABEL\": \"Kate Smith\", \"CLUSTER\": 9}, {\"X\": -2.7195780277252197, \"Y\": -8.981995582580566, \"LABEL\": \"The Challenger\", \"CLUSTER\": 5}, {\"X\": 8.389544486999512, \"Y\": 7.210494518280029, \"LABEL\": \"Primo Levi\", \"CLUSTER\": 2}, {\"X\": 2.7997899055480957, \"Y\": -2.075590133666992, \"LABEL\": \"Clare Boothe Luce\", \"CLUSTER\": 9}, {\"X\": -2.9558186531066895, \"Y\": 14.558831214904785, \"LABEL\": \"John Huston\", \"CLUSTER\": 14}, {\"X\": -5.270463466644287, \"Y\": 12.391576766967773, \"LABEL\": \"Rita Hayworth\", \"CLUSTER\": 14}, {\"X\": 4.1467084884643555, \"Y\": -6.818028926849365, \"LABEL\": \"James Baldwin\", \"CLUSTER\": 5}, {\"X\": -0.20809608697891235, \"Y\": -17.895172119140625, \"LABEL\": \"Alf Landon\", \"CLUSTER\": 8}, {\"X\": -13.740716934204102, \"Y\": 18.37775230407715, \"LABEL\": \"Andres Segovie\", \"CLUSTER\": 7}, {\"X\": -3.0713820457458496, \"Y\": 16.13964080810547, \"LABEL\": \"John Houseman\", \"CLUSTER\": 14}, {\"X\": 2.1412930488586426, \"Y\": 3.7850356101989746, \"LABEL\": \"Louis L Amour\", \"CLUSTER\": 9}, {\"X\": 12.002198219299316, \"Y\": -2.0154478549957275, \"LABEL\": \"William B Shockley\", \"CLUSTER\": 6}, {\"X\": -7.188117504119873, \"Y\": 13.554760932922363, \"LABEL\": \"Lucille Ball\", \"CLUSTER\": 14}, {\"X\": 3.9904696941375732, \"Y\": 3.6188926696777344, \"LABEL\": \"Robert Penn Warren\", \"CLUSTER\": 9}, {\"X\": -2.993260622024536, \"Y\": -15.6643705368042, \"LABEL\": \"Ferdinand Marcos\", \"CLUSTER\": 4}, {\"X\": 12.461625099182129, \"Y\": -19.119094848632812, \"LABEL\": \"Andrei Sakharov\", \"CLUSTER\": 4}, {\"X\": 11.80474853515625, \"Y\": -19.722991943359375, \"LABEL\": \"Andrei A Gromyko\", \"CLUSTER\": 4}, {\"X\": 3.803722858428955, \"Y\": -5.2729573249816895, \"LABEL\": \"I F Stone\", \"CLUSTER\": 5}, {\"X\": -13.136995315551758, \"Y\": 17.290422439575195, \"LABEL\": \"Vladimir Horowitz\", \"CLUSTER\": 7}, {\"X\": 12.181300163269043, \"Y\": -14.329141616821289, \"LABEL\": \"Hirohito\", \"CLUSTER\": 4}, {\"X\": -4.576432704925537, \"Y\": -1.7550851106643677, \"LABEL\": \"August A Busch Jr\", \"CLUSTER\": 18}, {\"X\": 0.6167632937431335, \"Y\": -19.481542587280273, \"LABEL\": \"Claude Pepper\", \"CLUSTER\": 8}, {\"X\": 3.112546443939209, \"Y\": 7.466588973999023, \"LABEL\": \"Samuel Beckett\", \"CLUSTER\": 9}, {\"X\": -2.007981061935425, \"Y\": 13.82043743133545, \"LABEL\": \"Greta Garbo\", \"CLUSTER\": 14}, {\"X\": -5.084105014801025, \"Y\": 13.166306495666504, \"LABEL\": \"Sammy Davis Jr\", \"CLUSTER\": 14}, {\"X\": -12.992807388305664, \"Y\": 16.50141143798828, \"LABEL\": \"Leonard Bernstein\", \"CLUSTER\": 7}, {\"X\": 5.654520034790039, \"Y\": 13.032342910766602, \"LABEL\": \"Erte\", \"CLUSTER\": 10}, {\"X\": 0.8844993710517883, \"Y\": -12.610634803771973, \"LABEL\": \"Ralph David Abernathy\", \"CLUSTER\": 5}, {\"X\": -6.092902660369873, \"Y\": 16.384614944458008, \"LABEL\": \"Rex Harrison\", \"CLUSTER\": 14}, {\"X\": -2.862534999847412, \"Y\": 14.518528938293457, \"LABEL\": \"Frank Capra\", \"CLUSTER\": 14}, {\"X\": -0.7489873766899109, \"Y\": 8.879402160644531, \"LABEL\": \"Dr Seuss\", \"CLUSTER\": 9}, {\"X\": -12.450113296508789, \"Y\": 20.800580978393555, \"LABEL\": \"Miles Davis\", \"CLUSTER\": 11}, {\"X\": -10.867227554321289, \"Y\": 13.873992919921875, \"LABEL\": \"Martha Graham\", \"CLUSTER\": 7}, {\"X\": -15.863840103149414, \"Y\": 4.244723320007324, \"LABEL\": \"Leo Durocher\", \"CLUSTER\": 1}, {\"X\": -5.181403636932373, \"Y\": 16.91637420654297, \"LABEL\": \"Peggy Ashcroft\", \"CLUSTER\": 14}, {\"X\": 1.953492522239685, \"Y\": 1.8476524353027344, \"LABEL\": \"Alex Haley\", \"CLUSTER\": 9}, {\"X\": -14.235946655273438, \"Y\": 17.534887313842773, \"LABEL\": \"John Cage\", \"CLUSTER\": 7}, {\"X\": 9.157862663269043, \"Y\": -22.620685577392578, \"LABEL\": \"Menachem Begin\", \"CLUSTER\": 16}, {\"X\": -6.655925750732422, \"Y\": 15.450852394104004, \"LABEL\": \"Shirley Booth\", \"CLUSTER\": 14}, {\"X\": 6.403168678283691, \"Y\": 4.0633955001831055, \"LABEL\": \"Isaac Asimov\", \"CLUSTER\": 9}, {\"X\": 2.9871246814727783, \"Y\": -0.25527191162109375, \"LABEL\": \"William Shawn\", \"CLUSTER\": 9}, {\"X\": 3.1391007900238037, \"Y\": -7.799642562866211, \"LABEL\": \"Marsha P Johnson\", \"CLUSTER\": 5}, {\"X\": 3.6216492652893066, \"Y\": -20.73087501525879, \"LABEL\": \"Thurgood Marshall\", \"CLUSTER\": 8}, {\"X\": -0.025618620216846466, \"Y\": 14.099115371704102, \"LABEL\": \"Federico Fellini\", \"CLUSTER\": 14}, {\"X\": 1.7541379928588867, \"Y\": -13.795405387878418, \"LABEL\": \"Cesar Chavez\", \"CLUSTER\": 4}, {\"X\": -14.002293586730957, \"Y\": 19.273035049438477, \"LABEL\": \"Carlos Montoya\", \"CLUSTER\": 7}, {\"X\": -12.635844230651855, \"Y\": 21.199996948242188, \"LABEL\": \"Dizzy Gillespie\", \"CLUSTER\": 11}, {\"X\": -11.325723648071289, \"Y\": 4.3122029304504395, \"LABEL\": \"Arthur Ashe\", \"CLUSTER\": 15}, {\"X\": 4.702512741088867, \"Y\": 5.378389358520508, \"LABEL\": \"William Golding\", \"CLUSTER\": 9}, {\"X\": 11.471202850341797, \"Y\": -5.67047643661499, \"LABEL\": \"Albert Sabin\", \"CLUSTER\": 17}, {\"X\": 0.7590458989143372, \"Y\": -18.519386291503906, \"LABEL\": \"Thomas P O Neill Jr\", \"CLUSTER\": 8}, {\"X\": 1.6892129182815552, \"Y\": -17.274255752563477, \"LABEL\": \"Richard Nixon\", \"CLUSTER\": 8}, {\"X\": 11.3215913772583, \"Y\": 1.8216707706451416, \"LABEL\": \"Erik Erikson\", \"CLUSTER\": 2}, {\"X\": 12.909110069274902, \"Y\": -1.4237960577011108, \"LABEL\": \"Linus C Pauling\", \"CLUSTER\": 6}, {\"X\": -5.725397109985352, \"Y\": 14.565417289733887, \"LABEL\": \"Jessica Tandy\", \"CLUSTER\": 14}, {\"X\": 9.70884895324707, \"Y\": -8.380881309509277, \"LABEL\": \"Jan Tinbergen\", \"CLUSTER\": 5}, {\"X\": 1.5300692319869995, \"Y\": -0.6255696415901184, \"LABEL\": \"Jacqueline Kennedy\", \"CLUSTER\": 9}, {\"X\": 11.458001136779785, \"Y\": -5.619353771209717, \"LABEL\": \"Jonas Salk\", \"CLUSTER\": 17}, {\"X\": 3.310856819152832, \"Y\": 12.234801292419434, \"LABEL\": \"Alfred Eisenstaedt\", \"CLUSTER\": 10}, {\"X\": -7.527838230133057, \"Y\": 15.655458450317383, \"LABEL\": \"Ginger Rogers\", \"CLUSTER\": 14}, {\"X\": -6.486595630645752, \"Y\": 14.33803939819336, \"LABEL\": \"George Abbott\", \"CLUSTER\": 14}, {\"X\": 9.317960739135742, \"Y\": -22.759197235107422, \"LABEL\": \"Yitzhak Rabin\", \"CLUSTER\": 16}, {\"X\": 10.1116361618042, \"Y\": -1.4016354084014893, \"LABEL\": \"Carl Sagan\", \"CLUSTER\": 6}, {\"X\": 4.682015895843506, \"Y\": -0.24707086384296417, \"LABEL\": \"Timothy Leary\", \"CLUSTER\": 9}, {\"X\": -7.98546838760376, \"Y\": 16.594234466552734, \"LABEL\": \"Gene Kelly\", \"CLUSTER\": 11}, {\"X\": 4.855006217956543, \"Y\": 3.3466439247131348, \"LABEL\": \"Allen Ginsberg\", \"CLUSTER\": 9}, {\"X\": 13.162900924682617, \"Y\": -17.003646850585938, \"LABEL\": \"Deng Xiaoping\", \"CLUSTER\": 4}, {\"X\": -3.966529369354248, \"Y\": 13.405994415283203, \"LABEL\": \"James Stewart\", \"CLUSTER\": 14}, {\"X\": -2.5082755088806152, \"Y\": 11.209596633911133, \"LABEL\": \"Bob Kane\", \"CLUSTER\": 9}, {\"X\": 6.970714569091797, \"Y\": 0.5519394278526306, \"LABEL\": \"Benjamin Spock\", \"CLUSTER\": 9}, {\"X\": -11.216814994812012, \"Y\": 5.593843460083008, \"LABEL\": \"Helen Moody\", \"CLUSTER\": 15}, {\"X\": -5.174684524536133, \"Y\": 15.951761245727539, \"LABEL\": \"Maureen O Sullivan\", \"CLUSTER\": 14}, {\"X\": 9.226228713989258, \"Y\": -7.758716106414795, \"LABEL\": \"Theodore Schultz\", \"CLUSTER\": 5}, {\"X\": -6.292104244232178, \"Y\": -12.628838539123535, \"LABEL\": \"Alan B Shepard Jr\", \"CLUSTER\": 3}, {\"X\": -11.844756126403809, \"Y\": 13.566322326660156, \"LABEL\": \"Galina Ulanova\", \"CLUSTER\": 7}, {\"X\": -0.42799046635627747, \"Y\": -19.266468048095703, \"LABEL\": \"Bella Abzug\", \"CLUSTER\": 8}, {\"X\": 1.4201784133911133, \"Y\": -3.4274697303771973, \"LABEL\": \"Fred W Friendly\", \"CLUSTER\": 5}, {\"X\": -9.123132705688477, \"Y\": 18.30845832824707, \"LABEL\": \"Frank Sinatra\", \"CLUSTER\": 11}, {\"X\": 10.336825370788574, \"Y\": -21.98391342163086, \"LABEL\": \"Hassan II\", \"CLUSTER\": 16}, {\"X\": 5.405998706817627, \"Y\": 6.564864635467529, \"LABEL\": \"Iris Murdoch\", \"CLUSTER\": 9}, {\"X\": 10.242327690124512, \"Y\": -22.4055118560791, \"LABEL\": \"King Hussein\", \"CLUSTER\": 16}, {\"X\": 9.397595405578613, \"Y\": -15.240979194641113, \"LABEL\": \"Pierre Trudeau\", \"CLUSTER\": 4}, {\"X\": 0.28496065735816956, \"Y\": -16.12995719909668, \"LABEL\": \"Elliot Richardson\", \"CLUSTER\": 8}, {\"X\": -1.7634185552597046, \"Y\": 9.884428977966309, \"LABEL\": \"Charles M Schulz\", \"CLUSTER\": 9}, {\"X\": 9.921907424926758, \"Y\": 0.5387915968894958, \"LABEL\": \"Karen Sparck Jones\", \"CLUSTER\": 2}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_data['CLUSTER'] = agg.labels_ + 1\n",
    "\n",
    "alt.Chart(vis_data).mark_circle(size=30).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    tooltip=['LABEL', 'CLUSTER'],\n",
    "    color='CLUSTER:N'\n",
    ").properties(\n",
    "    height=650,\n",
    "    width=650\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742d46b",
   "metadata": {},
   "source": [
    "Once again, take a look around and see what you can find. These clusters seem to be both detailed and nicely partitioned, bracketing off, for example, classical musicians and composers (cluster 7) from jazz and popular musicians (cluster 11).\n",
    "\n",
    "```{margin} What this loop does\n",
    "For each cluster:\n",
    "\n",
    "1. Subset the visualization data with that cluster and get all the people\n",
    "2. Convert the column into string and then split that string into chunks with `textwrap`\n",
    "3. Print to screen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2e8c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:  7\n",
      "-----------\n",
      "Maurice Ravel, Constantin Stanislavsky, Bela Bartok, Sergei Eisenstein, Igor\n",
      "Stravinsky, Otto Klemperer, Maria Callas, Arthur Fiedler, Arthur Rubinstein,\n",
      "Andres Segovie, Vladimir Horowitz, Leonard Bernstein, Martha Graham, John Cage,\n",
      "Carlos Montoya, Galina Ulanova\n",
      "\n",
      "\n",
      "Cluster: 11\n",
      "-----------\n",
      "Jerome Kern, W C Handy, Billie Holiday, Cole Porter, Coleman Hawkins, Judy\n",
      "Garland, Louis Armstrong, Mahalia Jackson, Stan Kenton, Richard Rodgers,\n",
      "Thelonious Monk, Earl Hines, Muddy Waters, Ethel Merman, Count Basie, Benny\n",
      "Goodman, Miles Davis, Dizzy Gillespie, Gene Kelly, Frank Sinatra\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for k in [7, 11]:\n",
    "    people = vis_data[vis_data['CLUSTER']==k]['LABEL']\n",
    "    people = ', '.join(person for person in people)\n",
    "    people = textwrap.wrap(people, 80)\n",
    "    print(f\"Cluster: {k:>2}\\n-----------\")\n",
    "    for entry in people:\n",
    "        print(entry)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c23c1",
   "metadata": {},
   "source": [
    "That kind of detail may not be desirable depending on your task, but for us, these clusters give us a wonderfully nuanced view of the kinds of people in the obituaries."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
