{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186f56bf",
   "metadata": {},
   "source": [
    "Text Annotation with spaCy\n",
    "======================\n",
    "\n",
    "This chapter introduces workshop participants to the general field of natural language processing, or NLP. While NLP is often used interchangeably with text mining/analytics in introductory settings, the former differs in important ways from many of the core methods in the latter. We will highlight a few such differences over the course of this session, and then more generally throughout the workshop series as a whole.\n",
    "\n",
    "```{admonition} Learning objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "+ Explain how document annotation differs from other representations of text data\n",
    "+ Have a general sense of how `spaCy` models and their respective pipelines work\n",
    "+ Extract linguistic information about text using `spaCy`\n",
    "+ Describe key terms in NLP, like part-of-speech tagging, dependency parsing, etc.\n",
    "+ Know how/where to look for more information about the linguistic data `spaCy` makes available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2030ad6",
   "metadata": {},
   "source": [
    "NLP vs. Text Mining: In Brief\n",
    "---------------------------------\n",
    "\n",
    "The short space of this reader, as well as that of our series, necessarily limits a conversation about everything that characterizes NLP from text mining/analytics. That there are differences at all is in itself worth noting and merits further exploration. But for the purposes of this series's focus, there are two such instances of these differences that are especially worth calling out.\n",
    "\n",
    "### Data structures\n",
    "\n",
    "At the outset, one way of distinguishing NLP from text mining has to do with NLP's underlying **data structure**. Generally speaking, NLP methods are **maximally preservative** when it comes to representing textual information in a way computers can read. Unlike text mining's atomizing focus on **bags of words**, in NLP we often use literal transcriptions of the input text and run our analyses directly on that. This is because much of the information NLP methods provide is context-sensitive: we need to know, for example, the subject of a sentence in order to do dependency parsing; part-of-speech taggers are most effective when they have surrounding tokens to consider. Accordingly, our workflow needs to retain as much information about our documents as possible, for as long as possible. In fact, many NLP methods _build on each other_, so data about our documents will grow over the course of processing them (rather than getting pared down, as with text mining). The dominant paradigm, then, for thinking about how text data is represented in NLP is **annotation**: NLP tends to add, associate, or tag documents with extra information.\n",
    "\n",
    "### Model-driven methods\n",
    "\n",
    "The other key difference between text mining and NLP – which goes hand-in-hand with the idea of annotation – lies in the fact that the latter tends to be more **model-driven**. NLP methods often rely on statistical models to create the above information, and ultimately these models have a lot of assumptions baked into them. Such assumptions range from philosophy of language (how do we know we're analyzing meaning?) to the kind of training data on which they're trained (what does the model represent, and what biases might thereby be involved?). Of course, it's possible to build your own models, and indeed a later chapter will show you how to do so, but you'll often find yourself using other researchers' models when doing NLP work. It's thus very important to know how researchers have built their models so you can do your own work responsibly.\n",
    "\n",
    "```{admonition} Keep in mind\n",
    "Throughout this series, we will be using NLP methods in the context of text-based data, but NLP applies more widely to speech data as well.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdaed4",
   "metadata": {},
   "source": [
    "spaCy Language Models\n",
    "-----------------------------\n",
    "\n",
    "```{margin} Want to know more?\n",
    "Explosion, the company behind `spaCy`, has a series of useful videos introducing the framework. [This one] is a good place to start.\n",
    "\n",
    "[This one]: https://www.youtube.com/watch?v=9k_EfV7Cns0&t=1365s\n",
    "```\n",
    "\n",
    "Much of this workshop series will use language models from `spaCy`, one of the most popular NLP libraries in Python. `spaCy` is both a framework and a model resource. It offers access to models through a unique set of coding workflows, which we'll discuss below (you can also train your own models with the library). Learning about these workflows will help us add annotate documents with extra information that will, in turn, enable us to perform a number of different NLP tasks.\n",
    "\n",
    "### `spaCy` pipelines\n",
    "\n",
    "In essence, a `spaCy` model is a collection of sub-models arranged into a **pipeline**. The idea here is that you send a document through this pipeline, and the model does the work of annotating your document. Once it has finished, you can access these annotations to perform whatever analysis you'd like to do.\n",
    "\n",
    "![](img/spacy_pipeline.png)\n",
    "\n",
    "Every component, or **pipe**, in a `spaCy` pipeline performs a different task, from tokenization to part-of-speeching tagging and named-entity recognition. Each model comes with a specific ordering of these tasks, but you can mix and match them after the fact, adding or removing pipes as you see fit. The result is a wide set of options; the present workshop series only samples a few core aspects of the library's overall capabilities.\n",
    "\n",
    "### Downloading a model\n",
    "\n",
    "The specific model we'll be using is `spaCy`'s medium-sized English model: [en_core_web_md]. It's been trained on the [OntoNotes] corpus and it features several useful pipes, which we'll discuss below.\n",
    "\n",
    "If you haven't used `spaCy` before, you'll need to download this model. You can do so by running the following in a command line interface:\n",
    "\n",
    "```sh\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "Just be sure you run this while working in the Python environment you'd like to use!\n",
    "\n",
    "Once this downloads, you can load the model with the code below. Note that it's conventional to assign the model to a variable called `nlp`.\n",
    "\n",
    "[en_core_web_md]: https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.3.0\n",
    "[OntoNotes]: https://catalog.ldc.upenn.edu/LDC2013T19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c88450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28d91f",
   "metadata": {},
   "source": [
    "Annotations\n",
    "--------------\n",
    "\n",
    "With the model loaded, we can send a document through the pipeline, which will in turn produce our text annotations. To annotate a document with the `spaCy` model, simply run it through the core function, `nlp()`. We'll do so with a short poem by Gertrude Stein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7089ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/stein_carafe.txt', 'r') as f:\n",
    "    stein_poem = f.read()\n",
    "    \n",
    "carafe = nlp(stein_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffcfe0",
   "metadata": {},
   "source": [
    "With this done, we can inspect the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175c11aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing. All this and not ordinary, not unordered in not resembling. The difference is spreading."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131ea58",
   "metadata": {},
   "source": [
    "...which seems to be no different from a string representation! This output is a bit misleading, however. Our `carafe` object actually has a ton of extra information associated with it, even though, on the surface, it appears to be a plain old string.\n",
    "\n",
    "If you'd like, you can inspect all these attributes and methods with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938a8d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = [i for i in dir(carafe) if i.startswith(\"_\") is False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33821a4",
   "metadata": {},
   "source": [
    "We won't show them all here, but suffice it to say, there are a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a866c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes in a SpaCy doc: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes in a SpaCy doc:\", len(attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49406fb2",
   "metadata": {},
   "source": [
    "This high number of attributes indicates an important point to keep in mind when working with `spaCy` and NLP generally: as we mentioned before, the primary data model for NLP aims to **maximally preserve information** about your document. It keeps documents intact and in fact adds much more information about them than Python's base string methods have. In this sense, we might say that `spaCy` is additive in nature, whereas text mining methods are subtractive, or reductive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c6083",
   "metadata": {},
   "source": [
    "### Document Annotations\n",
    "\n",
    "So, while the base representation of `carafe` looks like a string, under the surface there are all sorts of annotations about it. To access them, we use the attributes counted above. For example, `spaCy` adds extra segmentation information about a document, like which parts of it belong to different sentences. We can check to see whether this information has been attached to our text with the `.has_annotation()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa74c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe.has_annotation('SENT_START')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85abd99",
   "metadata": {},
   "source": [
    "We can use the same method to check for a few other annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae7610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies: True\n",
      "    Entities: True\n",
      "        Tags: True\n"
     ]
    }
   ],
   "source": [
    "annotation_types = {'Dependencies': 'DEP', 'Entities': 'ENT_IOB', 'Tags': 'TAG'}\n",
    "for a, t in annotation_types.items():\n",
    "    print(\n",
    "        f\"{a:>12}: {carafe.has_annotation(t)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3dcd9",
   "metadata": {},
   "source": [
    "Let's look at sentences. We can access them with `.sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d547a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x1255c5728>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee2958",
   "metadata": {},
   "source": [
    "...but you can see that there's a small complication here: `.sents` returns a generator, not a list. The reason has to do with memory efficiency. Because `spaCy` adds so much extra information about your document, this information could slow down your code or overwhelm your computer if the library didn't store it in an efficient manner. Of course this isn't a problem with our small poem, but you can imagine how it could become one with a big corpus.\n",
    "\n",
    "To access the actual sentences in `carafe`, we'll need to convert the generator to a list.\n",
    "\n",
    "```{margin} Want to learn more about generators?\n",
    "The DataLab has a workshop about them. See [this link].\n",
    "\n",
    "[this link]: https://datalab.ucdavis.edu/eventscalendar/intermediate-python-iterator-generator-crash-course/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d8a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an [...]\n",
      "All this and not ordinary, not unordered in not resembling.\n",
      "The difference is spreading.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "sentences = list(carafe.sents)\n",
    "for s in sentences:\n",
    "    s = textwrap.shorten(s.text, width=100)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba091428",
   "metadata": {},
   "source": [
    "One very useful attribute is `.noun_chunks`. It returns nouns and compound nouns in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce397c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind\n",
      "glass\n",
      "a cousin\n",
      "a spectacle\n",
      "nothing\n",
      "a single hurt color\n",
      "an arrangement\n",
      "a system\n",
      "The difference\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = list(carafe.noun_chunks)\n",
    "\n",
    "for noun in noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b6c36",
   "metadata": {},
   "source": [
    "See how this picks up not only nouns, but articles and compound information? Articles could be helpful if you wanted to track singular/plural relationships, while compound nouns might tell you something about the way a document refers to the entities therein. The latter could have repeating patterns, and you might imagine how you could use noun chunks to create and count n-gram tokens and feed that into a classifier.\n",
    "\n",
    "Consider this example from _The Odyssey_. Homer used many epithets and repeating phrases throughout his epic. According to some theories, these act as mnemonic devices, helping a performer keep everything in their head during an oral performance (the poem wasn't written down in Homer's day). Using `.noun_chunks` in conjunction with a Python `Counter`, we may be able to identify these in Homer's text. Below, we'll do so with _The Odyssey_ Book XI.\n",
    "\n",
    "First, let's load and model the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff86244",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/odyssey_book_11.txt', 'r') as f:\n",
    "    book_eleven = f.read()\n",
    "    \n",
    "odyssey = nlp(book_eleven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fb8a0",
   "metadata": {},
   "source": [
    "Now we'll import a `Counter` and initialize it. Then we'll get the noun chunks from the document and populate them in the count dictionary with a list comprehension line. Be sure to only grab the text from each token. We'll explain why in a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e036a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counts = Counter([chunk.text for chunk in odyssey.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0cb42c",
   "metadata": {},
   "source": [
    "With that done, let's look for repeating noun chunks with three or more words.\n",
    "\n",
    "```{margin} What we're doing here...\n",
    "For every noun chunk in the counter:\n",
    "\n",
    "1. Split the chunk\n",
    "2. Check if the length of the chunk is more than two and the count is more than one\n",
    "3. If so, join the chunk back together and print it along with the chunk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f2faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHRASE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the sea shore</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a fair wind</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the poor feckless ghosts</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the same time</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the other side</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his golden sceptre</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your own house</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her own son</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the Achaean land</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her own husband</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my wicked wife</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all the Danaans</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the poor creature</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          COUNT\n",
       "PHRASE                         \n",
       "the sea shore                 2\n",
       "a fair wind                   2\n",
       "the poor feckless ghosts      2\n",
       "the same time                 2\n",
       "the other side                2\n",
       "his golden sceptre            2\n",
       "your own house                2\n",
       "her own son                   2\n",
       "the Achaean land              2\n",
       "her own husband               2\n",
       "my wicked wife                2\n",
       "all the Danaans               2\n",
       "the poor creature             2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunks = []\n",
    "for chunk, count in noun_counts.items():\n",
    "    chunk = chunk.split()\n",
    "    if (len(chunk) > 2) and (count > 1):\n",
    "        joined = ' '.join(chunk)\n",
    "        chunks.append({\n",
    "            'PHRASE': joined,\n",
    "            'COUNT': count\n",
    "        })\n",
    "        \n",
    "chunks = pd.DataFrame(chunks).set_index('PHRASE')\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b5b8f",
   "metadata": {},
   "source": [
    "Excellent! Looks like we turned up a few: \"the poor feckless ghosts,\" \"my wicked wife,\" and \"all the Danaans\" are likely the kind of repeating phrases scholars think of in Homer's text.\n",
    "\n",
    "Another way to look at entities in a text is with `.ents`. `spaCy` uses **named-entity recognition** to extract significant objects, or entities, in a document. In general, anything that has a proper name associated with it is considered an entity, but things like expressions of time and geographic location are also often tagged. Here are the first five from Book XI above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82661f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circe\n",
      "Oceanus\n",
      "Cimmerians\n",
      "Circe\n",
      "Perimedes\n"
     ]
    }
   ],
   "source": [
    "entities = list(odyssey.ents)\n",
    "\n",
    "count = 0\n",
    "while count < 5:\n",
    "    print(entities[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae04b8",
   "metadata": {},
   "source": [
    "You can select particular entities using the `.label_` attribute. Here are all the temporal entities in Book XI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c27f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all night', 'to-morrow morning', 'the light of day']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text for e in odyssey.ents if e.label_ == 'TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07082a34",
   "metadata": {},
   "source": [
    "And here is a unique listing of all the people.\n",
    "\n",
    "```{margin} How many labels are there?\n",
    "This will depend on the model. Here's the [label scheme] for the one we're using.\n",
    "\n",
    "[label scheme]: https://spacy.io/models/en#en_core_web_md-labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08bf7198",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achilles',\n",
       " 'Aeson',\n",
       " 'Alcinous',\n",
       " 'Arete',\n",
       " 'Ariadne',\n",
       " 'Cassandra',\n",
       " 'Chloris',\n",
       " 'Circe',\n",
       " 'Clytemnestra',\n",
       " 'Diana',\n",
       " 'Echeneus',\n",
       " 'Epicaste',\n",
       " 'Eriphyle',\n",
       " 'Eurylochus',\n",
       " 'Helen',\n",
       " 'Iphicles',\n",
       " 'Iphimedeia',\n",
       " 'Jove',\n",
       " 'Leda',\n",
       " 'Leto',\n",
       " 'Maera',\n",
       " 'Megara',\n",
       " 'Memnon',\n",
       " 'Minerva',\n",
       " 'Neleus',\n",
       " 'Neoptolemus',\n",
       " 'Nestor',\n",
       " 'OEdipodes',\n",
       " 'Orestes',\n",
       " 'Ossa',\n",
       " 'Periclymenus',\n",
       " 'Perimedes',\n",
       " 'Pero',\n",
       " 'Pollux',\n",
       " 'Priam',\n",
       " 'Proserpine',\n",
       " 'Pylos',\n",
       " 'Pytho',\n",
       " 'Queen',\n",
       " 'Scyros',\n",
       " 'Sisyphus',\n",
       " 'Teiresias',\n",
       " 'Telemachus',\n",
       " 'Theban Teiresias',\n",
       " 'Ulysses'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(e.text for e in odyssey.ents if e.label_ == 'PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca5df5",
   "metadata": {},
   "source": [
    "Don't see an entity that you know to be in your document? You can add more to the `spaCy` model. Doing so is beyond the scope of our workshop session, but the library's `EntityRuler()` [documentation] will show you how.\n",
    "\n",
    "[documentation]: https://spacy.io/api/entityruler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be80aaf",
   "metadata": {},
   "source": [
    "### Token Annotations\n",
    "\n",
    "In addition to storing all of this information about texts, `spaCy` creates a substantial amount of annotations for each of the tokens in that document. The same logic as above applies to accessing this information.\n",
    "\n",
    "Let's return to the Stein poem. Indexing `carafe` will return individual tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0feacad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glass"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122d323",
   "metadata": {},
   "source": [
    "Like `carafe`, each one has several attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "357d0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token attributes: 94\n"
     ]
    }
   ],
   "source": [
    "token_attributes = [i for i in dir(carafe[3]) if i.startswith(\"_\") is False]\n",
    "\n",
    "print(\"Number of token attributes:\", len(token_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3a618",
   "metadata": {},
   "source": [
    "That's a lot!\n",
    "\n",
    "These attributes range from simple booleans, like whether a token is an alphabetic character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60cc2a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].is_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45209904",
   "metadata": {},
   "source": [
    "...or whether it is a stop word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d34d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf005d21",
   "metadata": {},
   "source": [
    "...to more complex pieces of information, like tracking back to the sentence this token is part of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14fe0227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca85db",
   "metadata": {},
   "source": [
    "...sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d01ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ead8",
   "metadata": {},
   "source": [
    "...and even vector space representations (more about these on day three!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b004fa79",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4859e-01, -1.7940e-01,  4.3666e-02,  1.5748e-01,  1.3568e-01,\n",
       "       -9.3666e-01, -6.8430e-01,  4.7692e-01, -4.1391e-01,  9.3575e-01,\n",
       "       -1.6360e-01,  6.7553e-02, -2.7843e-01, -5.6125e-01,  1.3088e-01,\n",
       "       -1.0006e-01,  7.0374e-03,  2.6217e+00,  5.4600e-02, -5.8931e-01,\n",
       "        2.5739e-04, -2.6791e-01,  4.6093e-01, -5.9145e-02, -1.0330e-01,\n",
       "       -3.7589e-01, -2.5343e-01,  1.4790e-02, -4.8031e-01, -4.4314e-01,\n",
       "        2.4685e-01, -8.6519e-04, -1.2361e-01,  9.1683e-02, -1.5880e-01,\n",
       "       -4.5974e-01,  3.3017e-01, -4.4124e-01,  3.3604e-01, -3.0438e-01,\n",
       "        4.4664e-01,  2.2697e-01,  2.9327e-02, -2.7025e-01,  3.1813e-01,\n",
       "       -1.5890e-01, -4.1371e-01, -9.0721e-01, -2.0866e-01,  3.6400e-01,\n",
       "        5.6862e-02, -2.6824e-01, -2.9722e-01,  6.2107e-02, -4.7908e-01,\n",
       "       -5.8164e-01, -1.4302e-01,  7.0109e-02, -1.2735e-01,  3.6194e-02,\n",
       "       -1.6634e-01, -2.2135e-01, -5.0446e-02,  4.3839e-01, -5.5363e-01,\n",
       "       -4.4219e-01, -1.3657e-01, -2.8472e-01, -5.0637e-01,  7.9913e-01,\n",
       "        3.8253e-02, -2.9499e-01,  4.3688e-01,  8.3770e-02, -1.4432e-01,\n",
       "        6.6395e-01, -2.1807e-01,  5.4256e-02, -3.6963e-01,  3.3715e-02,\n",
       "       -2.0889e-01,  3.8404e-01, -3.3217e-02,  6.1296e-05, -8.7465e-01,\n",
       "       -4.6473e-01,  9.2310e-01,  1.8017e+00, -6.3421e-01, -7.6744e-02,\n",
       "        1.8829e-01, -7.6714e-02,  5.7521e-01,  4.8993e-01,  4.5188e-01,\n",
       "       -3.1280e-01,  3.0696e-01, -1.8402e-01, -8.0179e-02,  1.0587e-01,\n",
       "        6.2712e-01,  2.5180e-02,  1.3250e-02,  2.0200e-01, -4.9301e-01,\n",
       "       -7.0543e-01,  1.7630e-01, -3.8573e-01,  1.4884e-01,  1.1469e-01,\n",
       "       -2.4512e-01,  4.0795e-01,  4.0503e-01, -7.7637e-01,  2.9981e-01,\n",
       "        8.6308e-02, -3.4265e-01, -1.0283e-01,  6.5428e-01,  1.0040e+00,\n",
       "        7.9215e-02,  1.4525e-01,  2.6923e-01,  3.0525e-01,  2.4649e-01,\n",
       "       -4.8161e-01,  3.2354e-01, -4.3063e-01,  1.3162e-01,  3.0885e-01,\n",
       "        2.8071e-02,  2.0262e-01,  5.5863e-02,  1.2160e-01,  9.4541e-02,\n",
       "        3.6149e-01, -2.4719e-01,  4.8192e-01,  1.7732e-02, -2.5866e-01,\n",
       "       -2.0020e+00,  3.9600e-01,  2.7223e-01,  2.7166e-01, -2.8302e-01,\n",
       "       -3.3678e-01, -5.5586e-01,  1.2634e-01,  6.2432e-01, -3.5482e-01,\n",
       "        1.2412e-01,  2.3334e-01,  1.4205e-01,  1.8260e-01, -2.7955e-01,\n",
       "       -2.7223e-01,  2.6309e-01,  1.9212e-01,  2.0547e-02,  4.1270e-01,\n",
       "       -9.5296e-02, -2.0779e-01, -4.3821e-01,  6.5274e-01,  5.6938e-01,\n",
       "       -3.7614e-01,  2.0610e-02, -2.3933e-01, -4.5018e-02,  7.8979e-01,\n",
       "       -5.6471e-02, -6.9630e-01, -3.7204e-01,  4.7623e-01, -4.0311e-01,\n",
       "       -2.2279e-01,  2.9097e-01, -3.1518e-02,  1.8166e-01, -1.2901e+00,\n",
       "        1.7859e-02,  1.4502e-01,  2.5328e-01,  1.4368e-01, -2.8549e-01,\n",
       "       -2.8093e-01,  3.4198e-01,  4.3326e-01, -1.8720e-01, -2.0026e-02,\n",
       "       -5.1639e-01, -1.8429e-01,  2.6677e-01, -5.4715e-01,  1.3708e-01,\n",
       "        8.5359e-01, -3.3253e-02,  6.5259e-02,  6.3762e-03, -2.0237e-01,\n",
       "       -2.0636e-01,  2.5313e-01,  2.4637e-01, -1.5723e-01,  1.2737e-01,\n",
       "       -1.3642e-01, -5.0911e-02,  8.9525e-02, -1.7082e-02, -1.1004e-02,\n",
       "       -3.0895e-01,  1.2306e-02,  2.2061e-01, -7.4971e-01,  5.6255e-01,\n",
       "        1.8285e-01,  1.0815e-01, -3.0618e-01, -2.2243e-01, -2.0343e-01,\n",
       "       -3.2494e-01, -5.1475e-02,  2.5458e-01,  4.1998e-01,  2.2291e-01,\n",
       "        2.6603e-01, -5.2339e-01, -2.4402e-01, -3.4416e-01,  1.7581e-01,\n",
       "       -1.6235e-01,  1.0313e-01, -7.9232e-02,  4.2340e-02, -2.1850e-01,\n",
       "        5.7816e-02,  5.7152e-01,  3.3476e-01,  4.1512e-01, -2.9936e-01,\n",
       "        6.5527e-01,  8.0372e-01,  7.6235e-02, -2.4887e-02,  3.0213e-01,\n",
       "        3.9550e-01,  6.1762e-02,  2.9353e-02, -3.6386e-01, -3.3995e-02,\n",
       "        4.1919e-01, -1.9895e-01, -1.2398e-02, -2.9315e-01,  2.4226e-01,\n",
       "        3.4726e-01,  3.7107e-03, -2.3358e-01,  5.1316e-02,  1.9872e-01,\n",
       "        4.9645e-01,  6.8326e-01,  1.9632e-01,  1.3301e-01,  1.4365e-01,\n",
       "        2.1953e-01, -3.8584e-01,  7.6862e-02, -3.8999e-01, -3.7050e-01,\n",
       "       -4.5012e-01,  2.6216e-02,  3.0185e-01, -4.0049e-01,  3.3771e-01,\n",
       "        7.1870e-02, -1.3996e-01,  1.5457e-01,  2.2269e-01, -1.5383e-02,\n",
       "        5.0255e-02,  1.2920e-01,  8.6803e-02,  1.0577e-01, -1.1371e-01,\n",
       "       -2.5309e-01,  5.2190e-01,  2.1234e-01, -3.2938e-02,  4.0497e-01,\n",
       "       -5.4544e-01,  1.2056e-01,  2.9463e-01,  1.4404e-01, -2.9775e-01,\n",
       "        4.5954e-01, -2.8968e-01,  2.5130e-01,  2.2969e-01,  4.5593e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362911a6",
   "metadata": {},
   "source": [
    "Here's a listing of some attributes you might want to know about when text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63904114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LOWERCASE</th>\n",
       "      <th>ALPHABETIC</th>\n",
       "      <th>DIGIT</th>\n",
       "      <th>PUNCTUATION</th>\n",
       "      <th>STARTS SENTENCE</th>\n",
       "      <th>LIKE URL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kind</td>\n",
       "      <td>kind</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glass</td>\n",
       "      <td>glass</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cousin</td>\n",
       "      <td>cousin</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spectacle</td>\n",
       "      <td>spectacle</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TEXT  LOWERCASE  ALPHABETIC  DIGIT  PUNCTUATION  STARTS SENTENCE  \\\n",
       "INDEX                                                                          \n",
       "0              A          a        True  False        False             True   \n",
       "1           kind       kind        True  False        False            False   \n",
       "2             in         in        True  False        False            False   \n",
       "3          glass      glass        True  False        False            False   \n",
       "4            and        and        True  False        False            False   \n",
       "5              a          a        True  False        False            False   \n",
       "6         cousin     cousin        True  False        False            False   \n",
       "7              ,          ,       False  False         True            False   \n",
       "8              a          a        True  False        False            False   \n",
       "9      spectacle  spectacle        True  False        False            False   \n",
       "\n",
       "       LIKE URL  \n",
       "INDEX            \n",
       "0         False  \n",
       "1         False  \n",
       "2         False  \n",
       "3         False  \n",
       "4         False  \n",
       "5         False  \n",
       "6         False  \n",
       "7         False  \n",
       "8         False  \n",
       "9         False  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_attributes = []\n",
    "for token in carafe:\n",
    "    sample_attributes.append({\n",
    "        'INDEX': token.i,\n",
    "        'TEXT': token.text,\n",
    "        'LOWERCASE': token.lower_,\n",
    "        'ALPHABETIC': token.is_alpha,\n",
    "        'DIGIT': token.is_digit,\n",
    "        'PUNCTUATION': token.is_punct,\n",
    "        'STARTS SENTENCE': token.is_sent_start,\n",
    "        'LIKE URL': token.like_url\n",
    "    })\n",
    "\n",
    "sample_attributes = pd.DataFrame(sample_attributes).set_index('INDEX')\n",
    "sample_attributes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbfa4",
   "metadata": {},
   "source": [
    "We'll discuss some of the more complex annotations later on, both in this session and others. For now, let's collect some simple information about each of the tokens in our document. We'll use list comprehension to do so. We'll also use the `.text` attribute for each token, since we only want the text representation. Otherwise, we'd be creating a list of generators, where each generator has all those attribute for every token! (This is why we made sure to only use `.text` in our work with _The Odyssey_ above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bde860e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "-----\n",
      "A kind in glass and a cousin a spectacle and nothing strange a single hurt color and an [...] \n",
      "\n",
      "Punctuation\n",
      "-----------\n",
      ", . , . .\n"
     ]
    }
   ],
   "source": [
    "words = ' '.join([token.text for token in carafe if token.is_alpha])\n",
    "punctuation = ' '.join([token.text for token in carafe if token.is_punct])\n",
    "\n",
    "print(\n",
    "    f\"Words\\n-----\\n{textwrap.shorten(words, width=100)}\",\n",
    "    f\"\\n\\nPunctuation\\n-----------\\n{punctuation}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a9135",
   "metadata": {},
   "source": [
    "Want some linguistic information? We can get that too. For example, here are prefixes and suffixes:\n",
    "\n",
    "```{margin} You might be wondering about those underscores...\n",
    "\n",
    "The syntax conventions of `spaCy` use a trailing underscore to access the actual attribute information for a token. Using an attribute without the underscore will return an id, which the library uses internally to piece together output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21ba417c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREFIX</th>\n",
       "      <th>SUFFIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>k</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>i</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>g</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>a</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cousin</th>\n",
       "      <td>c</td>\n",
       "      <td>sin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectacle</th>\n",
       "      <td>s</td>\n",
       "      <td>cle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>a</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PREFIX SUFFIX\n",
       "TOKEN                  \n",
       "A              A      A\n",
       "kind           k    ind\n",
       "in             i     in\n",
       "glass          g    ass\n",
       "and            a    and\n",
       "a              a      a\n",
       "cousin         c    sin\n",
       "a              a      a\n",
       "spectacle      s    cle\n",
       "and            a    and"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_suffix = []\n",
    "for token in carafe:\n",
    "    if token.is_alpha:\n",
    "        prefix_suffix.append({\n",
    "            'TOKEN': token.text,\n",
    "            'PREFIX': token.prefix_,\n",
    "            'SUFFIX': token.suffix_\n",
    "        })\n",
    "\n",
    "prefix_suffix = pd.DataFrame(prefix_suffix).set_index('TOKEN')\n",
    "prefix_suffix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51d698",
   "metadata": {},
   "source": [
    "And here are lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5633040e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEMMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ordinary</th>\n",
       "      <td>ordinary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unordered</th>\n",
       "      <td>unordere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resembling</th>\n",
       "      <td>resemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>difference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spreading</th>\n",
       "      <td>spread</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LEMMA\n",
       "TOKEN                 \n",
       "All                all\n",
       "this              this\n",
       "and                and\n",
       "not                not\n",
       "ordinary      ordinary\n",
       "not                not\n",
       "unordered     unordere\n",
       "in                  in\n",
       "not                not\n",
       "resembling    resemble\n",
       "The                the\n",
       "difference  difference\n",
       "is                  be\n",
       "spreading       spread"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = []\n",
    "for token in carafe:\n",
    "    if token.is_alpha:\n",
    "        lemmas.append({\n",
    "            'TOKEN': token.text,\n",
    "            'LEMMA': token.lemma_\n",
    "        })\n",
    "\n",
    "lemmas = pd.DataFrame(lemmas).set_index('TOKEN')\n",
    "lemmas[24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9807b2",
   "metadata": {},
   "source": [
    "With such attributes at your disposal, you might imagine how you could work `spaCy` into a text mining pipeline. Instead of using separate functions to clean your corpus, those steps could all be accomplished by accessing attributes.\n",
    "\n",
    "Before you do this, however, you should consider two things: 1) whether the increased computational/memory overhead is worthwhile for your project; and 2) whether `spaCy`'s base models will work for the kind of text you're using. This second point is especially important. While `spaCy`'s base models are incredibly powerful, they are built for general purpose applications and may struggle with domain-specific language. Medical text and early modern print are two such examples of where the base models interpret your documents in unexpected ways, thereby complicating, maybe even ruining, parts of a text mining pipeline that relies on them. Sometimes, in other words, it's just best to stick with a text mining pipeline that you know to be effective.\n",
    "\n",
    "That all said, there are ways to train your own `spaCy` model on a specific domain. This can be an extensive process, one which exceeds the limits of our short workshop, but if you want to learn more about doing so, you can visit [this page]. There are also [third party models] available, which you might find useful, though your milage may vary.\n",
    "\n",
    "[this page]: https://spacy.io/usage/training\n",
    "[third party models]: https://spacy.io/universe/category/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577ce50",
   "metadata": {},
   "source": [
    "Part-of-Speech Tagging\n",
    "----------------------------\n",
    "\n",
    "One of the most common tasks in NLP involves assigning **part-of-speech, or POS, tags** to each token in a document. As we saw in the text mining series, these tags are a necessary step for certain text cleaning process, like lemmatization; you might also use them to identify subsets of your data, which you could separate out and model. Beyond text cleaning, POS tags can be useful for tasks like **word sense disambiguation**, where you try to determine which particular facet of meaning a given token represents.\n",
    "\n",
    "Regardless of the task, the process of getting POS tags from `spaCy` will be the same. Each token in a document has an associated tag, which is accessible as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6880cb70",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_TAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cousin</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectacle</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strange</th>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurt</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrangement</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pointing</th>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ordinary</th>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unordered</th>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resembling</th>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spreading</th>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            POS_TAG\n",
       "TOKEN              \n",
       "A               DET\n",
       "kind           NOUN\n",
       "in              ADP\n",
       "glass          NOUN\n",
       "and           CCONJ\n",
       "a               DET\n",
       "cousin         NOUN\n",
       ",             PUNCT\n",
       "a               DET\n",
       "spectacle      NOUN\n",
       "and           CCONJ\n",
       "nothing        PRON\n",
       "strange         ADJ\n",
       "a               DET\n",
       "single          ADJ\n",
       "hurt           NOUN\n",
       "color          NOUN\n",
       "and           CCONJ\n",
       "an              DET\n",
       "arrangement    NOUN\n",
       "in              ADP\n",
       "a               DET\n",
       "system         NOUN\n",
       "to              ADP\n",
       "pointing       VERB\n",
       ".             PUNCT\n",
       "All             DET\n",
       "this            DET\n",
       "and           CCONJ\n",
       "not            PART\n",
       "ordinary        ADJ\n",
       ",             PUNCT\n",
       "not            PART\n",
       "unordered      VERB\n",
       "in              ADP\n",
       "not            PART\n",
       "resembling     VERB\n",
       ".             PUNCT\n",
       "The             DET\n",
       "difference     NOUN\n",
       "is              AUX\n",
       "spreading      VERB\n",
       ".             PUNCT"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "for token in carafe:\n",
    "    pos.append({\n",
    "        'TOKEN': token.text,\n",
    "        'POS_TAG': token.pos_\n",
    "    })\n",
    "\n",
    "pos = pd.DataFrame(pos).set_index('TOKEN')\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ddd5d",
   "metadata": {},
   "source": [
    "If you don't know what a tag means, you can use `spacy.explain()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5d381a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coordinating conjunction'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('CCONJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429eccc9",
   "metadata": {},
   "source": [
    "`spaCy` actually has two types of POS tags. The ones accessible with the `.pos_` attribute are the basic tags, whereas those under `.tag_` are more detailed (these come from the [Penn Treebank project]). We'll print them out below, along with information about what they mean.\n",
    "\n",
    "[Penn Treebank project]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f11d6a1b",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_TAG</th>\n",
       "      <th>EXPLANATION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cousin</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>,</td>\n",
       "      <td>punctuation mark, comma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectacle</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strange</th>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurt</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrangement</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pointing</th>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>PDT</td>\n",
       "      <td>predeterminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ordinary</th>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>,</td>\n",
       "      <td>punctuation mark, comma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unordered</th>\n",
       "      <td>VBN</td>\n",
       "      <td>verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resembling</th>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spreading</th>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            POS_TAG                                        EXPLANATION\n",
       "TOKEN                                                                 \n",
       "A                DT                                         determiner\n",
       "kind             NN                             noun, singular or mass\n",
       "in               IN          conjunction, subordinating or preposition\n",
       "glass            NN                             noun, singular or mass\n",
       "and              CC                          conjunction, coordinating\n",
       "a                DT                                         determiner\n",
       "cousin           NN                             noun, singular or mass\n",
       ",                 ,                            punctuation mark, comma\n",
       "a                DT                                         determiner\n",
       "spectacle        NN                             noun, singular or mass\n",
       "and              CC                          conjunction, coordinating\n",
       "nothing          NN                             noun, singular or mass\n",
       "strange          JJ  adjective (English), other noun-modifier (Chin...\n",
       "a                DT                                         determiner\n",
       "single           JJ  adjective (English), other noun-modifier (Chin...\n",
       "hurt             NN                             noun, singular or mass\n",
       "color            NN                             noun, singular or mass\n",
       "and              CC                          conjunction, coordinating\n",
       "an               DT                                         determiner\n",
       "arrangement      NN                             noun, singular or mass\n",
       "in               IN          conjunction, subordinating or preposition\n",
       "a                DT                                         determiner\n",
       "system           NN                             noun, singular or mass\n",
       "to               IN          conjunction, subordinating or preposition\n",
       "pointing        VBG                 verb, gerund or present participle\n",
       ".                 .                  punctuation mark, sentence closer\n",
       "All             PDT                                      predeterminer\n",
       "this             DT                                         determiner\n",
       "and              CC                          conjunction, coordinating\n",
       "not              RB                                             adverb\n",
       "ordinary         JJ  adjective (English), other noun-modifier (Chin...\n",
       ",                 ,                            punctuation mark, comma\n",
       "not              RB                                             adverb\n",
       "unordered       VBN                              verb, past participle\n",
       "in               IN          conjunction, subordinating or preposition\n",
       "not              RB                                             adverb\n",
       "resembling      VBG                 verb, gerund or present participle\n",
       ".                 .                  punctuation mark, sentence closer\n",
       "The              DT                                         determiner\n",
       "difference       NN                             noun, singular or mass\n",
       "is              VBZ                  verb, 3rd person singular present\n",
       "spreading       VBG                 verb, gerund or present participle\n",
       ".                 .                  punctuation mark, sentence closer"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_tags = []\n",
    "for token in carafe:\n",
    "    detailed_tags.append({\n",
    "        'TOKEN': token.text,\n",
    "        'POS_TAG': token.tag_,\n",
    "        'EXPLANATION': spacy.explain(token.tag_)\n",
    "    })\n",
    "\n",
    "detailed_tags = pd.DataFrame(detailed_tags).set_index('TOKEN')\n",
    "detailed_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7307704",
   "metadata": {},
   "source": [
    "### Use case: word sense disambiguation\n",
    "\n",
    "This is all well and good in the abstract, but the power of POS tags lies in how they support other kinds of analysis. We'll do a quick word sense disambiguation task here but will return to do something more complex in a little while.\n",
    "\n",
    "Between the two strings:\n",
    "\n",
    "1. \"I am not going to bank on that happening.\"\n",
    "2. \"I went down to the river bank.\"\n",
    "\n",
    "How can we tell which sense of the word \"bank\" is being used? Well, we can model each with `spaCy` and see whether the POS tags for these two tokens match. If they don't match, this will indicate that the tokens represent two different senses of the word \"bank.\"\n",
    "\n",
    "All this can be accomplished with a `for` loop and `nlp.pipe()`. The latter function enables you to process different documents with the `spaCy` model all at once. This can be great for working with a large corpus, though note that, because `.pipe()` is meant to work on text at scale, it will return a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c31aa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x12da33318>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banks = [\"I am not going to bank on that happening.\", \"I went down to the river bank.\"]\n",
    "\n",
    "nlp.pipe(banks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480afb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not going to bank on that happening.\n",
      "+ bank: VB (verb, base form)\n",
      "\n",
      "I went down to the river bank.\n",
      "+ bank: NN (noun, singular or mass)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(banks):\n",
    "    for token in doc:\n",
    "        if token.text == 'bank':\n",
    "            print(\n",
    "                f\"{doc.text}\\n+ {token.text}: \"\n",
    "                f\"{token.tag_} ({spacy.explain(token.tag_)})\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ebedc",
   "metadata": {},
   "source": [
    "See how the tags differ between the two instances of \"bank\"? This indicates a difference in usage and, by proxy, a difference in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34942e8e",
   "metadata": {},
   "source": [
    "Dependency Parsing\n",
    "------------------------\n",
    "\n",
    "Another tool that can help with tasks like disambiguating word sense is dependency parsing. We've actually used it already: it allowed us to extract those noun chunks above. Dependency parsing involves analyzing the grammatical structure of text (usually sentences) to identify relationships between the words therein. The basic idea is that every word in a linguistic unit (eg. a sentence) is linked to at least one other word via a tree structure, and these linkages are hierarchical in nature, with various modifications occuring across the levels of sentences, clauses, phrases, and even compound nouns. Dependency parsing can tell you information about:\n",
    "\n",
    "1. The primary **subject** of a linguistic unit (and whether it is an **active** or **passive** subject)\n",
    "2. Various **heads**, which determine the syntatic categories of a phrase; these are often nouns and verbs, and you can think of them as the local subjects of subunits\n",
    "3. Various **dependents**, which modify, either directly or indirectly, their heads (think adjectives, adverbs, etc.)\n",
    "4. The **root** of the unit, which is often ([but not always!]) the primary verb\n",
    "\n",
    "Linguists have developed a number of different methods to parse dependencies, which we won't discuss here. Take note though that most popular one in NLP is the [Universal Dependencies] framework; `spaCy`, like most NLP models, uses this. The library also has some functionality for visualizing dependencies, which will help clarify what it is they are in the first place. Below, we visualize a sentence from the Stein poem.\n",
    "\n",
    "[but not always!]: https://universaldependencies.org/u/dep/root.html\n",
    "[Universal Dependencies]: https://universaldependencies.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c994f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"77aa0441c99f4606a7f7e9a9638d03d8-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">difference</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">spreading.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77aa0441c99f4606a7f7e9a9638d03d8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "to_render = list(carafe.sents)[2]\n",
    "displacy.render(to_render, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78735bd",
   "metadata": {},
   "source": [
    "See how the arcs have arrows? Arrows point to the dependents within a linguistic unit, that is, they point to modifying relationships between words. Arrows arc out from a segment's head, and the relationships they indicate are all specified with labels. As with the POS tags, you can use `spacy.explain()` on the dependency labels, which we'll do below. The whole list of them is also available in this [table of typologies]. Finally, somewhere in the tree you'll find a word with no arrows pointing to it (here, \"spreading\"). This is the root. One of its dependents is the subject of the sentence (here, \"difference\").\n",
    "\n",
    "[table of typologies]: https://universaldependencies.org/u/dep/all.html\n",
    "\n",
    "Seeing these relationships are quite useful in and of themselves, but the real power of dependency parsing comes in all the extra data it can provide about a token. Using this technique, you can link tokens back to their heads, or find local groupings of tokens that all refer to the same head.\n",
    "\n",
    "Here's how you could formalize that with a dataframe. Given this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bed9a8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Then I tried to find some way of embracing my mother's ghost.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = odyssey[2246:2260]\n",
    "sentence.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee5276",
   "metadata": {},
   "source": [
    "We can construct a `for` loop, which rolls through each token and retrieves its dependency info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7453a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>DEPENDENCY_SHORTCODE</th>\n",
       "      <th>DEPENDENCY</th>\n",
       "      <th>HEAD_INDEX</th>\n",
       "      <th>HEAD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDEX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>Then</td>\n",
       "      <td>advmod</td>\n",
       "      <td>adverbial modifier</td>\n",
       "      <td>2248</td>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>I</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>nominal subject</td>\n",
       "      <td>2248</td>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>tried</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>None</td>\n",
       "      <td>2248</td>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>to</td>\n",
       "      <td>aux</td>\n",
       "      <td>auxiliary</td>\n",
       "      <td>2250</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>find</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>open clausal complement</td>\n",
       "      <td>2248</td>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>some</td>\n",
       "      <td>det</td>\n",
       "      <td>determiner</td>\n",
       "      <td>2252</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>way</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "      <td>2250</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>of</td>\n",
       "      <td>prep</td>\n",
       "      <td>prepositional modifier</td>\n",
       "      <td>2252</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>embracing</td>\n",
       "      <td>pcomp</td>\n",
       "      <td>complement of preposition</td>\n",
       "      <td>2253</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>my</td>\n",
       "      <td>poss</td>\n",
       "      <td>possession modifier</td>\n",
       "      <td>2256</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>mother</td>\n",
       "      <td>poss</td>\n",
       "      <td>possession modifier</td>\n",
       "      <td>2258</td>\n",
       "      <td>ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>'s</td>\n",
       "      <td>case</td>\n",
       "      <td>case marking</td>\n",
       "      <td>2256</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>ghost</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "      <td>2254</td>\n",
       "      <td>embracing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>2248</td>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TOKEN DEPENDENCY_SHORTCODE                 DEPENDENCY  HEAD_INDEX  \\\n",
       "INDEX                                                                          \n",
       "2246        Then               advmod         adverbial modifier        2248   \n",
       "2247           I                nsubj            nominal subject        2248   \n",
       "2248       tried                 ROOT                       None        2248   \n",
       "2249          to                  aux                  auxiliary        2250   \n",
       "2250        find                xcomp    open clausal complement        2248   \n",
       "2251        some                  det                 determiner        2252   \n",
       "2252         way                 dobj              direct object        2250   \n",
       "2253          of                 prep     prepositional modifier        2252   \n",
       "2254   embracing                pcomp  complement of preposition        2253   \n",
       "2255          my                 poss        possession modifier        2256   \n",
       "2256      mother                 poss        possession modifier        2258   \n",
       "2257          's                 case               case marking        2256   \n",
       "2258       ghost                 dobj              direct object        2254   \n",
       "2259           .                punct                punctuation        2248   \n",
       "\n",
       "            HEAD  \n",
       "INDEX             \n",
       "2246       tried  \n",
       "2247       tried  \n",
       "2248       tried  \n",
       "2249        find  \n",
       "2250       tried  \n",
       "2251         way  \n",
       "2252        find  \n",
       "2253         way  \n",
       "2254          of  \n",
       "2255      mother  \n",
       "2256       ghost  \n",
       "2257      mother  \n",
       "2258   embracing  \n",
       "2259       tried  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies = []\n",
    "for token in sentence:\n",
    "    dependencies.append({\n",
    "        'INDEX': token.i,\n",
    "        'TOKEN': token.text,\n",
    "        'DEPENDENCY_SHORTCODE': token.dep_,\n",
    "        'DEPENDENCY': spacy.explain(token.dep_),\n",
    "        'HEAD_INDEX': token.head.i,\n",
    "        'HEAD': token.head\n",
    "    })\n",
    "    \n",
    "dependencies = pd.DataFrame(dependencies).set_index('INDEX')\n",
    "dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3c7b8",
   "metadata": {},
   "source": [
    "How many tokens are associated with each head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5b7503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HEAD\n",
       "tried        5\n",
       "find         2\n",
       "way          2\n",
       "of           1\n",
       "embracing    1\n",
       "mother       2\n",
       "ghost        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies.groupby('HEAD').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fa432",
   "metadata": {},
   "source": [
    "Which tokens are in each of these groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88d32fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEAD</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tried</th>\n",
       "      <td>[Then, I, tried, find, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>[to, way]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>[some, of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>[embracing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embracing</th>\n",
       "      <td>[ghost]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>[my, 's]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghost</th>\n",
       "      <td>[mother]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               GROUP\n",
       "HEAD                                \n",
       "tried      [Then, I, tried, find, .]\n",
       "find                       [to, way]\n",
       "way                       [some, of]\n",
       "of                       [embracing]\n",
       "embracing                    [ghost]\n",
       "mother                      [my, 's]\n",
       "ghost                       [mother]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = []\n",
    "for group in dependencies.groupby('HEAD'):\n",
    "    head, tokens = group[0].text, group[1]['TOKEN'].tolist()\n",
    "    groups.append({\n",
    "        'HEAD': head,\n",
    "        'GROUP': tokens\n",
    "    })\n",
    "    \n",
    "groups = pd.DataFrame(groups).set_index('HEAD')\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301f967",
   "metadata": {},
   "source": [
    "`spaCy` also has a special `.subtree` attribute for each token, which will also produce a similar set of local groupings. Note however that `.subtree` captures all tokens that hold a dependent relationship with the one in question, meaning that when you find the subtree of the root, you're going to print out the entire sentence.\n",
    "\n",
    "As you might expect by now, `.subtree` returns a generator, so convert it to a list or use list comprehension to extract the tokens. We'll do this in a separate function. Within this function, we're going to use the `.text_with_ws` attribute of each token in the subtree to return an exact, string-like representation of the tree (this will include any whitespace characters that are attached to a token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0439f50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPENDENCY</th>\n",
       "      <th>SUBTREE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Then</th>\n",
       "      <td>advmod</td>\n",
       "      <td>Then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tried</th>\n",
       "      <td>ROOT</td>\n",
       "      <td>\"Then I tried to find some way of embracing my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>aux</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>xcomp</td>\n",
       "      <td>to find some way of embracing my mother's ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some</th>\n",
       "      <td>det</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>dobj</td>\n",
       "      <td>some way of embracing my mother's ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>prep</td>\n",
       "      <td>of embracing my mother's ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embracing</th>\n",
       "      <td>pcomp</td>\n",
       "      <td>embracing my mother's ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>poss</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>poss</td>\n",
       "      <td>my mother's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>case</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghost</th>\n",
       "      <td>dobj</td>\n",
       "      <td>my mother's ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DEPENDENCY                                            SUBTREE\n",
       "TOKEN                                                                  \n",
       "Then          advmod                                               Then\n",
       "I              nsubj                                                  I\n",
       "tried           ROOT  \"Then I tried to find some way of embracing my...\n",
       "to               aux                                                 to\n",
       "find           xcomp    to find some way of embracing my mother's ghost\n",
       "some             det                                               some\n",
       "way             dobj            some way of embracing my mother's ghost\n",
       "of              prep                     of embracing my mother's ghost\n",
       "embracing      pcomp                        embracing my mother's ghost\n",
       "my              poss                                                 my\n",
       "mother          poss                                        my mother's\n",
       "'s              case                                                 's\n",
       "ghost           dobj                                  my mother's ghost\n",
       ".              punct                                                  ."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtree_to_text(subtree):\n",
    "    subtree = ''.join([token.text_with_ws for token in token.subtree])\n",
    "    subtree = subtree.strip()\n",
    "    return subtree\n",
    "\n",
    "sentence_trees = []\n",
    "for token in sentence:\n",
    "    subtree = subtree_to_text(token.subtree)\n",
    "    sentence_trees.append({\n",
    "        'TOKEN': token.text,\n",
    "        'DEPENDENCY': token.dep_,\n",
    "        'SUBTREE': subtree\n",
    "    })\n",
    "\n",
    "sentence_trees = pd.DataFrame(sentence_trees).set_index('TOKEN')\n",
    "sentence_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fb35d",
   "metadata": {},
   "source": [
    "Putting Everything Together\n",
    "---------------------------------\n",
    "\n",
    "Now that we've walked through all these options (which are really only a small sliver of what you can do with `spaCy`!), let's put them into action. Below, we'll construct two short examples of how you might combine different aspects of token attributes to analyze a text. Both of them are essentially **information retrieval** tasks, and you might imagine doing something similar to extract and analyze particular words in your corpus, or to find different grammatical patterns that could be of significance (as we'll discuss in the next session)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e946e62",
   "metadata": {},
   "source": [
    "### Finding lemmas\n",
    "\n",
    "In the first, we'll use the `.lemma_` attribute to search through Book XI of _The Odyssey_ and match its tokens to a few key words. If you've read _The Odyssey_, you'll know that Book XI is where Odysseus and his fellow sailors have to travel down to the underworld Hades, where they speak with the dead. We already saw one example of this: Odysseus attempts to embrace his dead mother after communing with her. The whole trip to Hades is an emotionally tumultuous experience for the travelers, and peppered throughout Book XI are expressions of grief.\n",
    "\n",
    "With `.lemma_`, we can search for these expressions. We'll roll through the text and determine whether a token lemma matches one of a selected set. When we find a match, we'll get the subtree of this token's _head_. That is, we'll find the head upon which this token depends, and then we'll use that to reconstruct the local context for the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22ebc67d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBTREE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOKEN</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weeping</th>\n",
       "      <td>weeping and in great distress of mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cried</th>\n",
       "      <td>cried when I saw him: 'Elpenor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tears</th>\n",
       "      <td>tears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorrow</th>\n",
       "      <td>all my sorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tears</th>\n",
       "      <td>tears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grieves</th>\n",
       "      <td>He grieves continually about your never having...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorrows</th>\n",
       "      <td>our sorrows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>great grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadder</th>\n",
       "      <td>still sadder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weeping</th>\n",
       "      <td>weeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wept</th>\n",
       "      <td>I too wept and pitied him as I beheld him. '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weeping</th>\n",
       "      <td>weeping and talking thus sadly with one another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tear</th>\n",
       "      <td>a tear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cries</th>\n",
       "      <td>such appalling cries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SUBTREE\n",
       "TOKEN                                                     \n",
       "weeping              weeping and in great distress of mind\n",
       "cried                       cried when I saw him: 'Elpenor\n",
       "sad                                                    sad\n",
       "tears                                                tears\n",
       "sorrow                                       all my sorrow\n",
       "sad                                                    sad\n",
       "tears                                                tears\n",
       "grieves  He grieves continually about your never having...\n",
       "sad                                                    sad\n",
       "sorrows                                        our sorrows\n",
       "grief                                                grief\n",
       "grief                                          great grief\n",
       "grief                                                grief\n",
       "sadder                                        still sadder\n",
       "weeping                                            weeping\n",
       "wept          I too wept and pitied him as I beheld him. '\n",
       "weeping    weeping and talking thus sadly with one another\n",
       "tear                                                a tear\n",
       "cries                                 such appalling cries"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorrowful_lemmas = []\n",
    "for token in odyssey:\n",
    "    if token.lemma_ in ('cry', 'grief', 'grieve', 'sad', 'sorrow', 'tear', 'weep'):\n",
    "        subtree = subtree_to_text(token.head.subtree)\n",
    "        sorrowful_lemmas.append({\n",
    "            'TOKEN': token.text,\n",
    "            'SUBTREE': subtree\n",
    "        })\n",
    "\n",
    "sorrowful_lemmas = pd.DataFrame(sorrowful_lemmas).set_index('TOKEN')\n",
    "sorrowful_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134e32f",
   "metadata": {},
   "source": [
    "### Verb-subject relations\n",
    "\n",
    "For this next example, we'll use dependency tags to find the subject sentences in Book XI. As before, we'll go through each token in the document, this time checking to see whether it has the `nsubj` or `nsubjpass` tag for its `.dep_` attribute, which denote the subjects of the sentence's root. We'll also check to see whether a token is a noun (otherwise we'd get a lot of articles like \"who,\" \"them,\" etc.). If a token matches these two conditions, we'll find its head verb as well as the token's subtree. Note that this time, the subtree will refer directly to the token in question, not to the head. This will let us capture some descriptive information about each sentence subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3315a72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEAD</th>\n",
       "      <th>HEAD_LEMMA</th>\n",
       "      <th>SUBTREE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBJECT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Circe</th>\n",
       "      <td>sent</td>\n",
       "      <td>send</td>\n",
       "      <td>Circe, that great and cunning goddess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sails</th>\n",
       "      <td>were</td>\n",
       "      <td>be</td>\n",
       "      <td>her sails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>went</td>\n",
       "      <td>go</td>\n",
       "      <td>the sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>darkness</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rays</th>\n",
       "      <td>pierce</td>\n",
       "      <td>pierce</td>\n",
       "      <td>the rays of the sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theseus</th>\n",
       "      <td>came</td>\n",
       "      <td>come</td>\n",
       "      <td>Theseus and Pirithous glorious children of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thousands</th>\n",
       "      <td>came</td>\n",
       "      <td>come</td>\n",
       "      <td>so many thousands of ghosts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proserpine</th>\n",
       "      <td>send</td>\n",
       "      <td>send</td>\n",
       "      <td>Proserpine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ship</th>\n",
       "      <td>went</td>\n",
       "      <td>go</td>\n",
       "      <td>the ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>sprang</td>\n",
       "      <td>spring</td>\n",
       "      <td>a fair wind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              HEAD HEAD_LEMMA  \\\n",
       "SUBJECT                         \n",
       "Circe         sent       send   \n",
       "sails         were         be   \n",
       "sun           went         go   \n",
       "darkness       was         be   \n",
       "rays        pierce     pierce   \n",
       "...            ...        ...   \n",
       "Theseus       came       come   \n",
       "thousands     came       come   \n",
       "Proserpine    send       send   \n",
       "ship          went         go   \n",
       "wind        sprang     spring   \n",
       "\n",
       "                                                      SUBTREE  \n",
       "SUBJECT                                                        \n",
       "Circe                   Circe, that great and cunning goddess  \n",
       "sails                                               her sails  \n",
       "sun                                                   the sun  \n",
       "darkness                                             darkness  \n",
       "rays                                      the rays of the sun  \n",
       "...                                                       ...  \n",
       "Theseus     Theseus and Pirithous glorious children of the...  \n",
       "thousands                         so many thousands of ghosts  \n",
       "Proserpine                                         Proserpine  \n",
       "ship                                                 the ship  \n",
       "wind                                              a fair wind  \n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj = []\n",
    "for token in odyssey:\n",
    "    if token.dep_ in ('nsubj', 'nsubjpass') and token.pos_ in ('NOUN', 'PROPN'):\n",
    "        nsubj.append({\n",
    "            'SUBJECT': token.text,\n",
    "            'HEAD': token.head.text,\n",
    "            'HEAD_LEMMA': token.head.lemma_,\n",
    "            'SUBTREE': subtree_to_text(token.subtree)\n",
    "        })\n",
    "\n",
    "nsubj_df = pd.DataFrame(nsubj).set_index('SUBJECT')\n",
    "nsubj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fa9c8",
   "metadata": {},
   "source": [
    "Let's look at a few subtrees. Note how sometimes they are simple noun chunks, while in other cases they expand to whole phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22167ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ the gods\n",
      "+ the ghosts\n",
      "+ your lands\n",
      "+ Theseus\n",
      "+ The rest of her children\n",
      "+ \"'Ulysses\n",
      "+ much hardship\n",
      "+ the wind\n",
      "+ The ghosts of other dead men\n",
      "+ My mother\n"
     ]
    }
   ],
   "source": [
    "for chunk in nsubj_df['SUBTREE'].sample(10):\n",
    "    print(f\"+ {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fe7f1",
   "metadata": {},
   "source": [
    "Time to zoom out. How many time do each of our selected subjects appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7f4e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT\n",
       "ghost           8\n",
       "Ulysses         5\n",
       "ghosts          4\n",
       "heaven          4\n",
       "wife            4\n",
       "Proserpine      4\n",
       "man             4\n",
       "one             4\n",
       "Alcinous        3\n",
       "people          3\n",
       "Neleus          2\n",
       "ship            2\n",
       "judgement       2\n",
       "Theseus         2\n",
       "Teiresias       2\n",
       "gods            2\n",
       "mother          2\n",
       "life            2\n",
       "wind            2\n",
       "Circe           2\n",
       "Clytemnestra    2\n",
       "Hercules        2\n",
       "Jove            2\n",
       "creature        2\n",
       "prisoners       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj_df.groupby('SUBJECT').size().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd680bc",
   "metadata": {},
   "source": [
    "What heads are associated with each subject? (Note that we're using the lemmatized form of the verbs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ada4b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT     HEAD_LEMMA\n",
       "ghost       come          3\n",
       "Ulysses     answer        2\n",
       "Proserpine  send          2\n",
       "one         invite        1\n",
       "man         do            1\n",
       "            kill          1\n",
       "mother      answer        1\n",
       "            come          1\n",
       "one         be            1\n",
       "            get           1\n",
       "Aegisthus   be            1\n",
       "man         be            1\n",
       "one         tell          1\n",
       "others      fall          1\n",
       "people      be            1\n",
       "            bless         1\n",
       "            hear          1\n",
       "man         cross         1\n",
       "limbs       fail          1\n",
       "property    be            1\n",
       "heaven      take          1\n",
       "ground      reek          1\n",
       "guest       be            1\n",
       "guests      sit           1\n",
       "hardship    reach         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj_df.groupby(['SUBJECT', 'HEAD_LEMMA']).size().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4b2d6",
   "metadata": {},
   "source": [
    "Such information provides another way of looking at something like topicality. Rather than using, say, a bag of words approach to build a topic model, you could instead segment your text into chunks like the above and start tallying up token distributions. Such distributions might help you identify the primary subject in a passage of text, whether that be a character or something like a concept. Or, you could leverage them to investigate how different subjects are talked about, say by throwing POS tags into the mix to further nuance relationships across entities.\n",
    "\n",
    "Our next session will demonstrate what such investigations look like in action. For now however, the main takeway is that the above annotation structures provide you with a host of different ways to segment and facet your text data. You are by no means limited to single token counts when working computationally analyzing text. Indeed, sometimes the most compelling ways to expore a corpus lie in the broader, fuzzier relationships that NLP annotations help us identify."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
