{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de12843",
   "metadata": {},
   "source": [
    "Text Annotation\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35941fb1",
   "metadata": {},
   "source": [
    "spaCy Language Models\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50030b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a3970",
   "metadata": {},
   "source": [
    "Text Annotations\n",
    "--------------------\n",
    "\n",
    "To annotate text with the `spaCy` model, simply run it through the core function, `nlp()`. We'll do so with a short poem by Gertrude Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4466ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/stein_carafe.txt', 'r') as f:\n",
    "    carafe = f.read()\n",
    "    \n",
    "doc = nlp(carafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18638765",
   "metadata": {},
   "source": [
    "With this done, we can inspect the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7dffe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing. All this and not ordinary, not unordered in not resembling. The difference is spreading."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7ec62",
   "metadata": {},
   "source": [
    "...which seems to be no different from a string representation! This output is a bit misleading, however. Our `doc` object actually has a ton of extra information associated with it, even though, on the surface, it appears to be a plain old string.\n",
    "\n",
    "If you'd like, you can inspect all these attributes and methods with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3e6fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = [i for i in dir(doc) if i.startswith(\"_\") is False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3864d7",
   "metadata": {},
   "source": [
    "We won't show them all here, but suffice it to say, there are a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e32017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes in a SpaCy doc: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes in a SpaCy doc:\", len(attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfffcec",
   "metadata": {},
   "source": [
    "This high number of attributes indicates an important point to keep in mind when working with `spaCy`: as opposed to other forms of text preprocessing, like changing the case of tokens, removing punctuation, or stemming a corpus, `spaCy` aims to **maximally preserve information** about your text. It keeps texts intact and in fact adds much more information about them than Python's base string methods have. In this sense, we might say that `spaCy` is additive in nature, whereas text mining methods are subtractive, or reductive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab3662",
   "metadata": {},
   "source": [
    "### Document\n",
    "\n",
    "So, while the base representation of `doc` looks like a string, under the surface there are all sorts of annotations about it. To access them, we use the attributes counted above. For example, `spaCy` adds extra segmentation information about a text, like which parts of it belong to different sentences. We can check to see whether this information has been attached to our text with the `has_annotation()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62918cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.has_annotation('SENT_START')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a94641",
   "metadata": {},
   "source": [
    "We can use the same method to check for a few other annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400e50eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies: True\n",
      "    Entities: True\n",
      "        Tags: True\n"
     ]
    }
   ],
   "source": [
    "annotation_types = {'Dependencies': 'DEP', 'Entities': 'ENT_IOB', 'Tags': 'TAG'}\n",
    "for a, t in annotation_types.items():\n",
    "    print(\n",
    "        f\"{a:>12}: {doc.has_annotation(t)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920d0f9",
   "metadata": {},
   "source": [
    "Let's look at sentences. We can access them with `sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e045add5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x119bedf48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5919a0",
   "metadata": {},
   "source": [
    "...but you can see that there's a small complication here: `sents` returns a generator, not a list. The reason has to do with memory efficiency. Because `SpaCy` adds so much extra information about your text, this information could slow down your code or overwhelm your computer if the library didn't store it in an efficient manner. Of course this isn't a problem with our small poem, but you can imagine how it could become one with a big corpus.\n",
    "\n",
    "To access the actual sentences in `doc`, we'll need to convert the generator to a list.\n",
    "\n",
    "```{margin} Want to learn more about generators?\n",
    "The DataLab has a workshop on them.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ad58bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing.\n",
      "All this and not ordinary, not unordered in not resembling.\n",
      "The difference is spreading.\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8d56e",
   "metadata": {},
   "source": [
    "One very useful attribute is `noun_chunks`. It returns nouns and compound nouns in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bccafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind\n",
      "glass\n",
      "a cousin\n",
      "a spectacle\n",
      "nothing\n",
      "a single hurt color\n",
      "an arrangement\n",
      "a system\n",
      "The difference\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "for noun in noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be47a9",
   "metadata": {},
   "source": [
    "See how this picks up not only nouns, but articles and compound information? Articles could be helpful if you wanted to track singular/plural relationships, or whereas compound nouns might tell you something about the way a text refers to the entities therein. The latter might have repeating patterns, for example, and you might imagine how you could use noun chunks to create and count n-gram tokens and feed that into a classifier.\n",
    "\n",
    "Consider this example from _The Odyssey_. Homer used many epithets and repeating phrases throughout his epic. According to some theories, these act as mnemonic devices, helping a performer keep everything in their head during an oral performance. Using `noun_chunks` in conjunction with a Python `Counter`, we might be able to identify these in Homer's text. Below, we'll do so with Book XI.\n",
    "\n",
    "First, let's load and model the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b201d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/odyssey_book_11.txt', 'r') as f:\n",
    "    book_eleven = f.read()\n",
    "    \n",
    "odyssey = nlp(book_eleven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035d09b",
   "metadata": {},
   "source": [
    "Now we'll import a `Counter` and initialize it. Then we'll get the noun chunks from the text and add them to the counter dictionary. Be sure to only grab the text! We'll explain why in a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5b2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "odyssey_noun_chunks = list(odyssey.noun_chunks)\n",
    "\n",
    "for chunk in odyssey_noun_chunks:\n",
    "    c[chunk.text] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518873b",
   "metadata": {},
   "source": [
    "With that done, let's look for repeating noun chunks with three or more words.\n",
    "\n",
    "```{margin} What we're doing here...\n",
    "For every noun chunk in the counter:\n",
    "\n",
    "1. Split the chunk\n",
    "2. Check if the length of the chunk is more than two and the count is more than one\n",
    "3. If so, join the chunk back together and print it along with the chunk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adaf2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase                  | Count\n",
      "-------------------------------\n",
      "the sea shore               2\n",
      "a fair wind                 2\n",
      "the poor feckless ghosts    2\n",
      "the same time               2\n",
      "the other side              2\n",
      "his golden sceptre          2\n",
      "your own house              2\n",
      "her own son                 2\n",
      "the Achaean land            2\n",
      "her own husband             2\n",
      "my wicked wife              2\n",
      "all the Danaans             2\n",
      "the poor creature           2\n"
     ]
    }
   ],
   "source": [
    "print(\"Phrase\".ljust(23), \"| Count\\n-------------------------------\")\n",
    "for chunk, count in c.items():\n",
    "    chunk = chunk.split()\n",
    "    if (len(chunk) > 2) and (count > 1):\n",
    "        joined = ' '.join(chunk)\n",
    "        print(f\"{joined:<24} {count:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50fe2b",
   "metadata": {},
   "source": [
    "Excellent! Looks like we turned up a few: \"the poor feckless ghosts,\" \"my wicked wife,\" and \"all the Danaans\" are likely the kind of repeating phrases scholars think of in Homer's text.\n",
    "\n",
    "Another way to look at entities in a text is with `ents`. `spaCy` uses **named-entity recognition** to extract significant objects, or entities, in a text. In general, anything that has a proper name associated with it is considered an entity. Here are the first five from Book XI above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5091a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circe\n",
      "Oceanus\n",
      "Cimmerians\n",
      "Circe\n",
      "Perimedes\n"
     ]
    }
   ],
   "source": [
    "entities = list(odyssey.ents)\n",
    "\n",
    "count = 0\n",
    "while count < 5:\n",
    "    print(entities[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e4626",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "In addition to storing all of this information about texts, `spaCy` creates a substantial amount of annotations for each of the tokens in that text. The same logic as above applies to accessing this information.\n",
    "\n",
    "Let's return to the Stein poem. Indexing `doc` will return individual tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae5d21db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glass"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07790feb",
   "metadata": {},
   "source": [
    "Like `doc`, each one has several attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3672029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token attributes: 94\n"
     ]
    }
   ],
   "source": [
    "token_attributes = [i for i in dir(doc[3]) if i.startswith(\"_\") is False]\n",
    "\n",
    "print(\"Number of token attributes:\", len(token_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61f663",
   "metadata": {},
   "source": [
    "Wow! That's a lot.\n",
    "\n",
    "These attributes range from simple booleans, like whether a token is an alphabetic character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284f1928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e5505",
   "metadata": {},
   "source": [
    "...or whether it is a stop word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c8bdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50535d",
   "metadata": {},
   "source": [
    "...to more complex pieces of information, like part of speech tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2214d2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017e8ca",
   "metadata": {},
   "source": [
    "...sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e126a9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb34bd",
   "metadata": {},
   "source": [
    "...and even vector space representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db96441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4859e-01, -1.7940e-01,  4.3666e-02,  1.5748e-01,  1.3568e-01,\n",
       "       -9.3666e-01, -6.8430e-01,  4.7692e-01, -4.1391e-01,  9.3575e-01,\n",
       "       -1.6360e-01,  6.7553e-02, -2.7843e-01, -5.6125e-01,  1.3088e-01,\n",
       "       -1.0006e-01,  7.0374e-03,  2.6217e+00,  5.4600e-02, -5.8931e-01,\n",
       "        2.5739e-04, -2.6791e-01,  4.6093e-01, -5.9145e-02, -1.0330e-01,\n",
       "       -3.7589e-01, -2.5343e-01,  1.4790e-02, -4.8031e-01, -4.4314e-01,\n",
       "        2.4685e-01, -8.6519e-04, -1.2361e-01,  9.1683e-02, -1.5880e-01,\n",
       "       -4.5974e-01,  3.3017e-01, -4.4124e-01,  3.3604e-01, -3.0438e-01,\n",
       "        4.4664e-01,  2.2697e-01,  2.9327e-02, -2.7025e-01,  3.1813e-01,\n",
       "       -1.5890e-01, -4.1371e-01, -9.0721e-01, -2.0866e-01,  3.6400e-01,\n",
       "        5.6862e-02, -2.6824e-01, -2.9722e-01,  6.2107e-02, -4.7908e-01,\n",
       "       -5.8164e-01, -1.4302e-01,  7.0109e-02, -1.2735e-01,  3.6194e-02,\n",
       "       -1.6634e-01, -2.2135e-01, -5.0446e-02,  4.3839e-01, -5.5363e-01,\n",
       "       -4.4219e-01, -1.3657e-01, -2.8472e-01, -5.0637e-01,  7.9913e-01,\n",
       "        3.8253e-02, -2.9499e-01,  4.3688e-01,  8.3770e-02, -1.4432e-01,\n",
       "        6.6395e-01, -2.1807e-01,  5.4256e-02, -3.6963e-01,  3.3715e-02,\n",
       "       -2.0889e-01,  3.8404e-01, -3.3217e-02,  6.1296e-05, -8.7465e-01,\n",
       "       -4.6473e-01,  9.2310e-01,  1.8017e+00, -6.3421e-01, -7.6744e-02,\n",
       "        1.8829e-01, -7.6714e-02,  5.7521e-01,  4.8993e-01,  4.5188e-01,\n",
       "       -3.1280e-01,  3.0696e-01, -1.8402e-01, -8.0179e-02,  1.0587e-01,\n",
       "        6.2712e-01,  2.5180e-02,  1.3250e-02,  2.0200e-01, -4.9301e-01,\n",
       "       -7.0543e-01,  1.7630e-01, -3.8573e-01,  1.4884e-01,  1.1469e-01,\n",
       "       -2.4512e-01,  4.0795e-01,  4.0503e-01, -7.7637e-01,  2.9981e-01,\n",
       "        8.6308e-02, -3.4265e-01, -1.0283e-01,  6.5428e-01,  1.0040e+00,\n",
       "        7.9215e-02,  1.4525e-01,  2.6923e-01,  3.0525e-01,  2.4649e-01,\n",
       "       -4.8161e-01,  3.2354e-01, -4.3063e-01,  1.3162e-01,  3.0885e-01,\n",
       "        2.8071e-02,  2.0262e-01,  5.5863e-02,  1.2160e-01,  9.4541e-02,\n",
       "        3.6149e-01, -2.4719e-01,  4.8192e-01,  1.7732e-02, -2.5866e-01,\n",
       "       -2.0020e+00,  3.9600e-01,  2.7223e-01,  2.7166e-01, -2.8302e-01,\n",
       "       -3.3678e-01, -5.5586e-01,  1.2634e-01,  6.2432e-01, -3.5482e-01,\n",
       "        1.2412e-01,  2.3334e-01,  1.4205e-01,  1.8260e-01, -2.7955e-01,\n",
       "       -2.7223e-01,  2.6309e-01,  1.9212e-01,  2.0547e-02,  4.1270e-01,\n",
       "       -9.5296e-02, -2.0779e-01, -4.3821e-01,  6.5274e-01,  5.6938e-01,\n",
       "       -3.7614e-01,  2.0610e-02, -2.3933e-01, -4.5018e-02,  7.8979e-01,\n",
       "       -5.6471e-02, -6.9630e-01, -3.7204e-01,  4.7623e-01, -4.0311e-01,\n",
       "       -2.2279e-01,  2.9097e-01, -3.1518e-02,  1.8166e-01, -1.2901e+00,\n",
       "        1.7859e-02,  1.4502e-01,  2.5328e-01,  1.4368e-01, -2.8549e-01,\n",
       "       -2.8093e-01,  3.4198e-01,  4.3326e-01, -1.8720e-01, -2.0026e-02,\n",
       "       -5.1639e-01, -1.8429e-01,  2.6677e-01, -5.4715e-01,  1.3708e-01,\n",
       "        8.5359e-01, -3.3253e-02,  6.5259e-02,  6.3762e-03, -2.0237e-01,\n",
       "       -2.0636e-01,  2.5313e-01,  2.4637e-01, -1.5723e-01,  1.2737e-01,\n",
       "       -1.3642e-01, -5.0911e-02,  8.9525e-02, -1.7082e-02, -1.1004e-02,\n",
       "       -3.0895e-01,  1.2306e-02,  2.2061e-01, -7.4971e-01,  5.6255e-01,\n",
       "        1.8285e-01,  1.0815e-01, -3.0618e-01, -2.2243e-01, -2.0343e-01,\n",
       "       -3.2494e-01, -5.1475e-02,  2.5458e-01,  4.1998e-01,  2.2291e-01,\n",
       "        2.6603e-01, -5.2339e-01, -2.4402e-01, -3.4416e-01,  1.7581e-01,\n",
       "       -1.6235e-01,  1.0313e-01, -7.9232e-02,  4.2340e-02, -2.1850e-01,\n",
       "        5.7816e-02,  5.7152e-01,  3.3476e-01,  4.1512e-01, -2.9936e-01,\n",
       "        6.5527e-01,  8.0372e-01,  7.6235e-02, -2.4887e-02,  3.0213e-01,\n",
       "        3.9550e-01,  6.1762e-02,  2.9353e-02, -3.6386e-01, -3.3995e-02,\n",
       "        4.1919e-01, -1.9895e-01, -1.2398e-02, -2.9315e-01,  2.4226e-01,\n",
       "        3.4726e-01,  3.7107e-03, -2.3358e-01,  5.1316e-02,  1.9872e-01,\n",
       "        4.9645e-01,  6.8326e-01,  1.9632e-01,  1.3301e-01,  1.4365e-01,\n",
       "        2.1953e-01, -3.8584e-01,  7.6862e-02, -3.8999e-01, -3.7050e-01,\n",
       "       -4.5012e-01,  2.6216e-02,  3.0185e-01, -4.0049e-01,  3.3771e-01,\n",
       "        7.1870e-02, -1.3996e-01,  1.5457e-01,  2.2269e-01, -1.5383e-02,\n",
       "        5.0255e-02,  1.2920e-01,  8.6803e-02,  1.0577e-01, -1.1371e-01,\n",
       "       -2.5309e-01,  5.2190e-01,  2.1234e-01, -3.2938e-02,  4.0497e-01,\n",
       "       -5.4544e-01,  1.2056e-01,  2.9463e-01,  1.4404e-01, -2.9775e-01,\n",
       "        4.5954e-01, -2.8968e-01,  2.5130e-01,  2.2969e-01,  4.5593e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748c16a",
   "metadata": {},
   "source": [
    "We'll discuss some of these more complex annotations later on, both in this session and others. For now, let's collect some simple information about each of the tokens in our text. We'll use list comprehension to do so. We'll also use the `text` attribute for each token, since we only want the text representation (otherwise, we'd be creating a list of generators, where each generator has all those attribute for every token!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b70b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "-----\n",
      "A kind in glass and a cousin a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing All this and not ordinary not unordered in not resembling The difference is spreading \n",
      "\n",
      "Puncuation\n",
      "----------\n",
      ", . , . .\n"
     ]
    }
   ],
   "source": [
    "words = [t.text for t in doc if t.is_alpha]\n",
    "punctuation = [t.text for t in doc if t.is_punct]\n",
    "\n",
    "print(\n",
    "    f\"Words\\n-----\\n{' '.join(words)}\",\n",
    "    f\"\\n\\nPuncuation\\n----------\\n{' '.join(punctuation)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f625768",
   "metadata": {},
   "source": [
    "Want some linguistic information? We can get that too. For example, here are prefixes and suffixes:\n",
    "\n",
    "```{margin} You might be wondering about those underscores...\n",
    "\n",
    "The syntax conventions of `spaCy` use an underscore to access the actual attribute information for a token. Using an attribute without the underscore will return an id, which the library uses internally to piece together output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8502a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A\n",
      "Prefix: A\n",
      "Suffix: A\n",
      "\n",
      "Text: kind\n",
      "Prefix: k\n",
      "Suffix: ind\n",
      "\n",
      "Text: in\n",
      "Prefix: i\n",
      "Suffix: in\n",
      "\n",
      "Text: glass\n",
      "Prefix: g\n",
      "Suffix: ass\n",
      "\n",
      "Text: and\n",
      "Prefix: a\n",
      "Suffix: and\n",
      "\n",
      "Text: a\n",
      "Prefix: a\n",
      "Suffix: a\n",
      "\n",
      "Text: cousin\n",
      "Prefix: c\n",
      "Suffix: sin\n",
      "\n",
      "Text: a\n",
      "Prefix: a\n",
      "Suffix: a\n",
      "\n",
      "Text: spectacle\n",
      "Prefix: s\n",
      "Suffix: cle\n",
      "\n",
      "Text: and\n",
      "Prefix: a\n",
      "Suffix: and\n",
      "\n",
      "Text: nothing\n",
      "Prefix: n\n",
      "Suffix: ing\n",
      "\n",
      "Text: strange\n",
      "Prefix: s\n",
      "Suffix: nge\n",
      "\n",
      "Text: a\n",
      "Prefix: a\n",
      "Suffix: a\n",
      "\n",
      "Text: single\n",
      "Prefix: s\n",
      "Suffix: gle\n",
      "\n",
      "Text: hurt\n",
      "Prefix: h\n",
      "Suffix: urt\n",
      "\n",
      "Text: color\n",
      "Prefix: c\n",
      "Suffix: lor\n",
      "\n",
      "Text: and\n",
      "Prefix: a\n",
      "Suffix: and\n",
      "\n",
      "Text: an\n",
      "Prefix: a\n",
      "Suffix: an\n",
      "\n",
      "Text: arrangement\n",
      "Prefix: a\n",
      "Suffix: ent\n",
      "\n",
      "Text: in\n",
      "Prefix: i\n",
      "Suffix: in\n",
      "\n",
      "Text: a\n",
      "Prefix: a\n",
      "Suffix: a\n",
      "\n",
      "Text: system\n",
      "Prefix: s\n",
      "Suffix: tem\n",
      "\n",
      "Text: to\n",
      "Prefix: t\n",
      "Suffix: to\n",
      "\n",
      "Text: pointing\n",
      "Prefix: p\n",
      "Suffix: ing\n",
      "\n",
      "Text: All\n",
      "Prefix: A\n",
      "Suffix: All\n",
      "\n",
      "Text: this\n",
      "Prefix: t\n",
      "Suffix: his\n",
      "\n",
      "Text: and\n",
      "Prefix: a\n",
      "Suffix: and\n",
      "\n",
      "Text: not\n",
      "Prefix: n\n",
      "Suffix: not\n",
      "\n",
      "Text: ordinary\n",
      "Prefix: o\n",
      "Suffix: ary\n",
      "\n",
      "Text: not\n",
      "Prefix: n\n",
      "Suffix: not\n",
      "\n",
      "Text: unordered\n",
      "Prefix: u\n",
      "Suffix: red\n",
      "\n",
      "Text: in\n",
      "Prefix: i\n",
      "Suffix: in\n",
      "\n",
      "Text: not\n",
      "Prefix: n\n",
      "Suffix: not\n",
      "\n",
      "Text: resembling\n",
      "Prefix: r\n",
      "Suffix: ing\n",
      "\n",
      "Text: The\n",
      "Prefix: T\n",
      "Suffix: The\n",
      "\n",
      "Text: difference\n",
      "Prefix: d\n",
      "Suffix: nce\n",
      "\n",
      "Text: is\n",
      "Prefix: i\n",
      "Suffix: is\n",
      "\n",
      "Text: spreading\n",
      "Prefix: s\n",
      "Suffix: ing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix_suffix = [(t.text, t.prefix_, t.suffix_) for t in doc if t.is_alpha]\n",
    "\n",
    "for i in prefix_suffix:\n",
    "    text, prefix, suffix = i[0], i[1], i[2]\n",
    "    print(\n",
    "        f\"Text: {text}\\nPrefix: {prefix}\\nSuffix: {suffix}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da769dd",
   "metadata": {},
   "source": [
    "And here are lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5df79847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A\n",
      "Lemma: a\n",
      "\n",
      "Text: kind\n",
      "Lemma: kind\n",
      "\n",
      "Text: in\n",
      "Lemma: in\n",
      "\n",
      "Text: glass\n",
      "Lemma: glass\n",
      "\n",
      "Text: and\n",
      "Lemma: and\n",
      "\n",
      "Text: a\n",
      "Lemma: a\n",
      "\n",
      "Text: cousin\n",
      "Lemma: cousin\n",
      "\n",
      "Text: a\n",
      "Lemma: a\n",
      "\n",
      "Text: spectacle\n",
      "Lemma: spectacle\n",
      "\n",
      "Text: and\n",
      "Lemma: and\n",
      "\n",
      "Text: nothing\n",
      "Lemma: nothing\n",
      "\n",
      "Text: strange\n",
      "Lemma: strange\n",
      "\n",
      "Text: a\n",
      "Lemma: a\n",
      "\n",
      "Text: single\n",
      "Lemma: single\n",
      "\n",
      "Text: hurt\n",
      "Lemma: hurt\n",
      "\n",
      "Text: color\n",
      "Lemma: color\n",
      "\n",
      "Text: and\n",
      "Lemma: and\n",
      "\n",
      "Text: an\n",
      "Lemma: an\n",
      "\n",
      "Text: arrangement\n",
      "Lemma: arrangement\n",
      "\n",
      "Text: in\n",
      "Lemma: in\n",
      "\n",
      "Text: a\n",
      "Lemma: a\n",
      "\n",
      "Text: system\n",
      "Lemma: system\n",
      "\n",
      "Text: to\n",
      "Lemma: to\n",
      "\n",
      "Text: pointing\n",
      "Lemma: point\n",
      "\n",
      "Text: All\n",
      "Lemma: all\n",
      "\n",
      "Text: this\n",
      "Lemma: this\n",
      "\n",
      "Text: and\n",
      "Lemma: and\n",
      "\n",
      "Text: not\n",
      "Lemma: not\n",
      "\n",
      "Text: ordinary\n",
      "Lemma: ordinary\n",
      "\n",
      "Text: not\n",
      "Lemma: not\n",
      "\n",
      "Text: unordered\n",
      "Lemma: unordere\n",
      "\n",
      "Text: in\n",
      "Lemma: in\n",
      "\n",
      "Text: not\n",
      "Lemma: not\n",
      "\n",
      "Text: resembling\n",
      "Lemma: resemble\n",
      "\n",
      "Text: The\n",
      "Lemma: the\n",
      "\n",
      "Text: difference\n",
      "Lemma: difference\n",
      "\n",
      "Text: is\n",
      "Lemma: be\n",
      "\n",
      "Text: spreading\n",
      "Lemma: spread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmas = [(t.text, t.lemma_) for t in doc if t.is_alpha]\n",
    "\n",
    "for i in lemmas:\n",
    "    text, lemma = i[0], i[1]\n",
    "    print(\n",
    "        f\"Text: {text}\\nLemma: {lemma}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e8401",
   "metadata": {},
   "source": [
    "With such attributes at your disposal, you might imagine how you could work `spaCy` into a text mining pipeline. Instead of using separate functions to clean your text, those stepps could all be accomplished by accessing attributes.\n",
    "\n",
    "Before you do this, however, you should consider two things: 1) whether the increased computational/memory overhead is worthwhile for your project; and 2) whether `spaCy`'s base models will work for the kind of text you're using. This second point is especially important. While `spaCy`'s base models are incredibly powerful, they are built for general purpose applications and may struggle with domain-specific language. Medical texts or early modern print are two such examples of where the base models interpret your text in unexpected ways, thereby complicating, if not ruining parts of a text mining pipeline that relies on them. Sometimes, in other words, it's just best to stick with a text mining pipeline that you know is effective.\n",
    "\n",
    "That all said, there are ways to train your own `spaCy` model on a specific domain. This can be an extensive process, one which exceeds the limits of our short workshop, but if you want to learn more about doing so, you can visit [this page]. There are also [third party models] available, which you might find useful, though your milage may vary.\n",
    "\n",
    "[this page]: https://spacy.io/usage/training\n",
    "[third party models]: https://spacy.io/universe/category/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92303cc5",
   "metadata": {},
   "source": [
    "Dependency Parsing\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271f0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
