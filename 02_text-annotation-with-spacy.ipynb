{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186f56bf",
   "metadata": {},
   "source": [
    "Text Annotation with spaCy\n",
    "======================\n",
    "\n",
    "TODO: introduce language models and spaCy's take on them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdaed4",
   "metadata": {},
   "source": [
    "spaCy Language Models\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c88450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28d91f",
   "metadata": {},
   "source": [
    "Annotations\n",
    "--------------\n",
    "\n",
    "To annotate a document with the `spaCy` model, simply run it through the core function, `nlp()`. We'll do so with a short poem by Gertrude Stein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7089ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/stein_carafe.txt', 'r') as f:\n",
    "    stein_poem = f.read()\n",
    "    \n",
    "carafe = nlp(stein_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffcfe0",
   "metadata": {},
   "source": [
    "With this done, we can inspect the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175c11aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing. All this and not ordinary, not unordered in not resembling. The difference is spreading."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131ea58",
   "metadata": {},
   "source": [
    "...which seems to be no different from a string representation! This output is a bit misleading, however. Our `carafe` object actually has a ton of extra information associated with it, even though, on the surface, it appears to be a plain old string.\n",
    "\n",
    "If you'd like, you can inspect all these attributes and methods with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938a8d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = [i for i in dir(carafe) if i.startswith(\"_\") is False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33821a4",
   "metadata": {},
   "source": [
    "We won't show them all here, but suffice it to say, there are a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a866c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes in a SpaCy doc: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes in a SpaCy doc:\", len(attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49406fb2",
   "metadata": {},
   "source": [
    "This high number of attributes indicates an important point to keep in mind when working with `spaCy`: as opposed to other forms of text preprocessing, like changing the case of tokens, removing punctuation, or stemming a corpus, `spaCy` aims to **maximally preserve information** about your document. It keeps documents intact and in fact adds much more information about them than Python's base string methods have. In this sense, we might say that `spaCy` is additive in nature, whereas text mining methods are subtractive, or reductive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c6083",
   "metadata": {},
   "source": [
    "### Document Annotations\n",
    "\n",
    "So, while the base representation of `carafe` looks like a string, under the surface there are all sorts of annotations about it. To access them, we use the attributes counted above. For example, `spaCy` adds extra segmentation information about a document, like which parts of it belong to different sentences. We can check to see whether this information has been attached to our text with the `.has_annotation()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa74c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe.has_annotation('SENT_START')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85abd99",
   "metadata": {},
   "source": [
    "We can use the same method to check for a few other annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae7610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies: True\n",
      "    Entities: True\n",
      "        Tags: True\n"
     ]
    }
   ],
   "source": [
    "annotation_types = {'Dependencies': 'DEP', 'Entities': 'ENT_IOB', 'Tags': 'TAG'}\n",
    "for a, t in annotation_types.items():\n",
    "    print(\n",
    "        f\"{a:>12}: {carafe.has_annotation(t)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3dcd9",
   "metadata": {},
   "source": [
    "Let's look at sentences. We can access them with `.sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d547a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x12578dae8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee2958",
   "metadata": {},
   "source": [
    "...but you can see that there's a small complication here: `.sents` returns a generator, not a list. The reason has to do with memory efficiency. Because `SpaCy` adds so much extra information about your document, this information could slow down your code or overwhelm your computer if the library didn't store it in an efficient manner. Of course this isn't a problem with our small poem, but you can imagine how it could become one with a big corpus.\n",
    "\n",
    "To access the actual sentences in `carafe`, we'll need to convert the generator to a list.\n",
    "\n",
    "```{margin} Want to learn more about generators?\n",
    "The DataLab has a workshop about them. See [this link].\n",
    "\n",
    "[this link]: https://datalab.ucdavis.edu/eventscalendar/intermediate-python-iterator-generator-crash-course/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d8a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an [...]\n",
      "All this and not ordinary, not unordered in not resembling.\n",
      "The difference is spreading.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "sentences = list(carafe.sents)\n",
    "for s in sentences:\n",
    "    s = textwrap.shorten(s.text, width=100)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba091428",
   "metadata": {},
   "source": [
    "One very useful attribute is `.noun_chunks`. It returns nouns and compound nouns in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce397c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kind\n",
      "glass\n",
      "a cousin\n",
      "a spectacle\n",
      "nothing\n",
      "a single hurt color\n",
      "an arrangement\n",
      "a system\n",
      "The difference\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = list(carafe.noun_chunks)\n",
    "\n",
    "for noun in noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b6c36",
   "metadata": {},
   "source": [
    "See how this picks up not only nouns, but articles and compound information? Articles could be helpful if you wanted to track singular/plural relationships, while compound nouns might tell you something about the way a document refers to the entities therein. The latter could have repeating patterns, and you might imagine how you could use noun chunks to create and count n-gram tokens and feed that into a classifier.\n",
    "\n",
    "Consider this example from _The Odyssey_. Homer used many epithets and repeating phrases throughout his epic. According to some theories, these act as mnemonic devices, helping a performer keep everything in their head during an oral performance (the poem wasn't written down in Homer's day). Using `.noun_chunks` in conjunction with a Python `Counter`, we may be able to identify these in Homer's text. Below, we'll do so with _The Odyssey_ Book XI.\n",
    "\n",
    "First, let's load and model the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff86244",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/session_one/odyssey_book_11.txt', 'r') as f:\n",
    "    book_eleven = f.read()\n",
    "    \n",
    "odyssey = nlp(book_eleven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fb8a0",
   "metadata": {},
   "source": [
    "Now we'll import a `Counter` and initialize it. Then we'll get the noun chunks from the document and populate them in the count dictionary with a list comprehension line. Be sure to only grab the text from each token. We'll explain why in a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e036a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counts = Counter([chunk.text for chunk in odyssey.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0cb42c",
   "metadata": {},
   "source": [
    "With that done, let's look for repeating noun chunks with three or more words.\n",
    "\n",
    "```{margin} What we're doing here...\n",
    "For every noun chunk in the counter:\n",
    "\n",
    "1. Split the chunk\n",
    "2. Check if the length of the chunk is more than two and the count is more than one\n",
    "3. If so, join the chunk back together and print it along with the chunk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f2faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase                  | Count\n",
      "-------------------------------\n",
      "the sea shore               2\n",
      "a fair wind                 2\n",
      "the poor feckless ghosts    2\n",
      "the same time               2\n",
      "the other side              2\n",
      "his golden sceptre          2\n",
      "your own house              2\n",
      "her own son                 2\n",
      "the Achaean land            2\n",
      "her own husband             2\n",
      "my wicked wife              2\n",
      "all the Danaans             2\n",
      "the poor creature           2\n"
     ]
    }
   ],
   "source": [
    "print(\"Phrase                  | Count\")\n",
    "print(\"-\" * 31)\n",
    "for chunk, count in noun_counts.items():\n",
    "    chunk = chunk.split()\n",
    "    if (len(chunk) > 2) and (count > 1):\n",
    "        joined = ' '.join(chunk)\n",
    "        print(f\"{joined:<25}{count:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b5b8f",
   "metadata": {},
   "source": [
    "Excellent! Looks like we turned up a few: \"the poor feckless ghosts,\" \"my wicked wife,\" and \"all the Danaans\" are likely the kind of repeating phrases scholars think of in Homer's text.\n",
    "\n",
    "Another way to look at entities in a text is with `.ents`. `spaCy` uses **named-entity recognition** to extract significant objects, or entities, in a document. In general, anything that has a proper name associated with it is considered an entity, but things expressions of time and geographic location are also often tagged. Here are the first five from Book XI above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82661f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circe\n",
      "Oceanus\n",
      "Cimmerians\n",
      "Circe\n",
      "Perimedes\n"
     ]
    }
   ],
   "source": [
    "entities = list(odyssey.ents)\n",
    "\n",
    "count = 0\n",
    "while count < 5:\n",
    "    print(entities[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae04b8",
   "metadata": {},
   "source": [
    "You can select particular entities using the `.label_` attribute. Here are all the temporal entities in Book XI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c27f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all night', 'to-morrow morning', 'the light of day']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text for e in odyssey.ents if e.label_ == 'TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07082a34",
   "metadata": {},
   "source": [
    "And here is a unique listing of all the people.\n",
    "\n",
    "```{margin} How many labels are there?\n",
    "This will depend on the model. Here's the [label scheme] for the one we're using.\n",
    "\n",
    "[label scheme]: https://spacy.io/models/en#en_core_web_md-labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08bf7198",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achilles',\n",
       " 'Aeson',\n",
       " 'Alcinous',\n",
       " 'Arete',\n",
       " 'Ariadne',\n",
       " 'Cassandra',\n",
       " 'Chloris',\n",
       " 'Circe',\n",
       " 'Clytemnestra',\n",
       " 'Diana',\n",
       " 'Echeneus',\n",
       " 'Epicaste',\n",
       " 'Eriphyle',\n",
       " 'Eurylochus',\n",
       " 'Helen',\n",
       " 'Iphicles',\n",
       " 'Iphimedeia',\n",
       " 'Jove',\n",
       " 'Leda',\n",
       " 'Leto',\n",
       " 'Maera',\n",
       " 'Megara',\n",
       " 'Memnon',\n",
       " 'Minerva',\n",
       " 'Neleus',\n",
       " 'Neoptolemus',\n",
       " 'Nestor',\n",
       " 'OEdipodes',\n",
       " 'Orestes',\n",
       " 'Ossa',\n",
       " 'Periclymenus',\n",
       " 'Perimedes',\n",
       " 'Pero',\n",
       " 'Pollux',\n",
       " 'Priam',\n",
       " 'Proserpine',\n",
       " 'Pylos',\n",
       " 'Pytho',\n",
       " 'Queen',\n",
       " 'Scyros',\n",
       " 'Sisyphus',\n",
       " 'Teiresias',\n",
       " 'Telemachus',\n",
       " 'Theban Teiresias',\n",
       " 'Ulysses'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(e.text for e in odyssey.ents if e.label_ == 'PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be80aaf",
   "metadata": {},
   "source": [
    "### Token Annotations\n",
    "\n",
    "In addition to storing all of this information about texts, `spaCy` creates a substantial amount of annotations for each of the tokens in that document. The same logic as above applies to accessing this information.\n",
    "\n",
    "Let's return to the Stein poem. Indexing `carafe` will return individual tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0feacad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glass"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122d323",
   "metadata": {},
   "source": [
    "Like `carafe`, each one has several attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "357d0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token attributes: 94\n"
     ]
    }
   ],
   "source": [
    "token_attributes = [i for i in dir(carafe[3]) if i.startswith(\"_\") is False]\n",
    "\n",
    "print(\"Number of token attributes:\", len(token_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3a618",
   "metadata": {},
   "source": [
    "That's a lot!\n",
    "\n",
    "These attributes range from simple booleans, like whether a token is an alphabetic character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60cc2a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].is_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45209904",
   "metadata": {},
   "source": [
    "...or whether it is a stop word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d34d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf005d21",
   "metadata": {},
   "source": [
    "...to more complex pieces of information, like tracking back to the sentence this token is part of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14fe0227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A kind in glass and a cousin, a spectacle and nothing strange a single hurt color and an arrangement in a system to pointing."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca85db",
   "metadata": {},
   "source": [
    "...sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d01ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ead8",
   "metadata": {},
   "source": [
    "...and even vector space representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b004fa79",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4859e-01, -1.7940e-01,  4.3666e-02,  1.5748e-01,  1.3568e-01,\n",
       "       -9.3666e-01, -6.8430e-01,  4.7692e-01, -4.1391e-01,  9.3575e-01,\n",
       "       -1.6360e-01,  6.7553e-02, -2.7843e-01, -5.6125e-01,  1.3088e-01,\n",
       "       -1.0006e-01,  7.0374e-03,  2.6217e+00,  5.4600e-02, -5.8931e-01,\n",
       "        2.5739e-04, -2.6791e-01,  4.6093e-01, -5.9145e-02, -1.0330e-01,\n",
       "       -3.7589e-01, -2.5343e-01,  1.4790e-02, -4.8031e-01, -4.4314e-01,\n",
       "        2.4685e-01, -8.6519e-04, -1.2361e-01,  9.1683e-02, -1.5880e-01,\n",
       "       -4.5974e-01,  3.3017e-01, -4.4124e-01,  3.3604e-01, -3.0438e-01,\n",
       "        4.4664e-01,  2.2697e-01,  2.9327e-02, -2.7025e-01,  3.1813e-01,\n",
       "       -1.5890e-01, -4.1371e-01, -9.0721e-01, -2.0866e-01,  3.6400e-01,\n",
       "        5.6862e-02, -2.6824e-01, -2.9722e-01,  6.2107e-02, -4.7908e-01,\n",
       "       -5.8164e-01, -1.4302e-01,  7.0109e-02, -1.2735e-01,  3.6194e-02,\n",
       "       -1.6634e-01, -2.2135e-01, -5.0446e-02,  4.3839e-01, -5.5363e-01,\n",
       "       -4.4219e-01, -1.3657e-01, -2.8472e-01, -5.0637e-01,  7.9913e-01,\n",
       "        3.8253e-02, -2.9499e-01,  4.3688e-01,  8.3770e-02, -1.4432e-01,\n",
       "        6.6395e-01, -2.1807e-01,  5.4256e-02, -3.6963e-01,  3.3715e-02,\n",
       "       -2.0889e-01,  3.8404e-01, -3.3217e-02,  6.1296e-05, -8.7465e-01,\n",
       "       -4.6473e-01,  9.2310e-01,  1.8017e+00, -6.3421e-01, -7.6744e-02,\n",
       "        1.8829e-01, -7.6714e-02,  5.7521e-01,  4.8993e-01,  4.5188e-01,\n",
       "       -3.1280e-01,  3.0696e-01, -1.8402e-01, -8.0179e-02,  1.0587e-01,\n",
       "        6.2712e-01,  2.5180e-02,  1.3250e-02,  2.0200e-01, -4.9301e-01,\n",
       "       -7.0543e-01,  1.7630e-01, -3.8573e-01,  1.4884e-01,  1.1469e-01,\n",
       "       -2.4512e-01,  4.0795e-01,  4.0503e-01, -7.7637e-01,  2.9981e-01,\n",
       "        8.6308e-02, -3.4265e-01, -1.0283e-01,  6.5428e-01,  1.0040e+00,\n",
       "        7.9215e-02,  1.4525e-01,  2.6923e-01,  3.0525e-01,  2.4649e-01,\n",
       "       -4.8161e-01,  3.2354e-01, -4.3063e-01,  1.3162e-01,  3.0885e-01,\n",
       "        2.8071e-02,  2.0262e-01,  5.5863e-02,  1.2160e-01,  9.4541e-02,\n",
       "        3.6149e-01, -2.4719e-01,  4.8192e-01,  1.7732e-02, -2.5866e-01,\n",
       "       -2.0020e+00,  3.9600e-01,  2.7223e-01,  2.7166e-01, -2.8302e-01,\n",
       "       -3.3678e-01, -5.5586e-01,  1.2634e-01,  6.2432e-01, -3.5482e-01,\n",
       "        1.2412e-01,  2.3334e-01,  1.4205e-01,  1.8260e-01, -2.7955e-01,\n",
       "       -2.7223e-01,  2.6309e-01,  1.9212e-01,  2.0547e-02,  4.1270e-01,\n",
       "       -9.5296e-02, -2.0779e-01, -4.3821e-01,  6.5274e-01,  5.6938e-01,\n",
       "       -3.7614e-01,  2.0610e-02, -2.3933e-01, -4.5018e-02,  7.8979e-01,\n",
       "       -5.6471e-02, -6.9630e-01, -3.7204e-01,  4.7623e-01, -4.0311e-01,\n",
       "       -2.2279e-01,  2.9097e-01, -3.1518e-02,  1.8166e-01, -1.2901e+00,\n",
       "        1.7859e-02,  1.4502e-01,  2.5328e-01,  1.4368e-01, -2.8549e-01,\n",
       "       -2.8093e-01,  3.4198e-01,  4.3326e-01, -1.8720e-01, -2.0026e-02,\n",
       "       -5.1639e-01, -1.8429e-01,  2.6677e-01, -5.4715e-01,  1.3708e-01,\n",
       "        8.5359e-01, -3.3253e-02,  6.5259e-02,  6.3762e-03, -2.0237e-01,\n",
       "       -2.0636e-01,  2.5313e-01,  2.4637e-01, -1.5723e-01,  1.2737e-01,\n",
       "       -1.3642e-01, -5.0911e-02,  8.9525e-02, -1.7082e-02, -1.1004e-02,\n",
       "       -3.0895e-01,  1.2306e-02,  2.2061e-01, -7.4971e-01,  5.6255e-01,\n",
       "        1.8285e-01,  1.0815e-01, -3.0618e-01, -2.2243e-01, -2.0343e-01,\n",
       "       -3.2494e-01, -5.1475e-02,  2.5458e-01,  4.1998e-01,  2.2291e-01,\n",
       "        2.6603e-01, -5.2339e-01, -2.4402e-01, -3.4416e-01,  1.7581e-01,\n",
       "       -1.6235e-01,  1.0313e-01, -7.9232e-02,  4.2340e-02, -2.1850e-01,\n",
       "        5.7816e-02,  5.7152e-01,  3.3476e-01,  4.1512e-01, -2.9936e-01,\n",
       "        6.5527e-01,  8.0372e-01,  7.6235e-02, -2.4887e-02,  3.0213e-01,\n",
       "        3.9550e-01,  6.1762e-02,  2.9353e-02, -3.6386e-01, -3.3995e-02,\n",
       "        4.1919e-01, -1.9895e-01, -1.2398e-02, -2.9315e-01,  2.4226e-01,\n",
       "        3.4726e-01,  3.7107e-03, -2.3358e-01,  5.1316e-02,  1.9872e-01,\n",
       "        4.9645e-01,  6.8326e-01,  1.9632e-01,  1.3301e-01,  1.4365e-01,\n",
       "        2.1953e-01, -3.8584e-01,  7.6862e-02, -3.8999e-01, -3.7050e-01,\n",
       "       -4.5012e-01,  2.6216e-02,  3.0185e-01, -4.0049e-01,  3.3771e-01,\n",
       "        7.1870e-02, -1.3996e-01,  1.5457e-01,  2.2269e-01, -1.5383e-02,\n",
       "        5.0255e-02,  1.2920e-01,  8.6803e-02,  1.0577e-01, -1.1371e-01,\n",
       "       -2.5309e-01,  5.2190e-01,  2.1234e-01, -3.2938e-02,  4.0497e-01,\n",
       "       -5.4544e-01,  1.2056e-01,  2.9463e-01,  1.4404e-01, -2.9775e-01,\n",
       "        4.5954e-01, -2.8968e-01,  2.5130e-01,  2.2969e-01,  4.5593e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carafe[3].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362911a6",
   "metadata": {},
   "source": [
    "Here's a listing of some attributes you might want to know about when text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63904114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dd151_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >TEXT</th>\n",
       "      <th class=\"col_heading level0 col1\" >INDEX</th>\n",
       "      <th class=\"col_heading level0 col2\" >LOWERCASE</th>\n",
       "      <th class=\"col_heading level0 col3\" >ALPHABETIC</th>\n",
       "      <th class=\"col_heading level0 col4\" >DIGIT</th>\n",
       "      <th class=\"col_heading level0 col5\" >PUNCTUATION</th>\n",
       "      <th class=\"col_heading level0 col6\" >STARTS SENTENCE</th>\n",
       "      <th class=\"col_heading level0 col7\" >LIKE URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row0_col0\" class=\"data row0 col0\" >A</td>\n",
       "      <td id=\"T_dd151_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_dd151_row0_col2\" class=\"data row0 col2\" >a</td>\n",
       "      <td id=\"T_dd151_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row0_col5\" class=\"data row0 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row0_col6\" class=\"data row0 col6\" >True</td>\n",
       "      <td id=\"T_dd151_row0_col7\" class=\"data row0 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row1_col0\" class=\"data row1 col0\" >kind</td>\n",
       "      <td id=\"T_dd151_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_dd151_row1_col2\" class=\"data row1 col2\" >kind</td>\n",
       "      <td id=\"T_dd151_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row1_col5\" class=\"data row1 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row1_col6\" class=\"data row1 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row1_col7\" class=\"data row1 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row2_col0\" class=\"data row2 col0\" >in</td>\n",
       "      <td id=\"T_dd151_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "      <td id=\"T_dd151_row2_col2\" class=\"data row2 col2\" >in</td>\n",
       "      <td id=\"T_dd151_row2_col3\" class=\"data row2 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row2_col5\" class=\"data row2 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row2_col6\" class=\"data row2 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row2_col7\" class=\"data row2 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row3_col0\" class=\"data row3 col0\" >glass</td>\n",
       "      <td id=\"T_dd151_row3_col1\" class=\"data row3 col1\" >3</td>\n",
       "      <td id=\"T_dd151_row3_col2\" class=\"data row3 col2\" >glass</td>\n",
       "      <td id=\"T_dd151_row3_col3\" class=\"data row3 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row3_col5\" class=\"data row3 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row3_col6\" class=\"data row3 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row3_col7\" class=\"data row3 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row4_col0\" class=\"data row4 col0\" >and</td>\n",
       "      <td id=\"T_dd151_row4_col1\" class=\"data row4 col1\" >4</td>\n",
       "      <td id=\"T_dd151_row4_col2\" class=\"data row4 col2\" >and</td>\n",
       "      <td id=\"T_dd151_row4_col3\" class=\"data row4 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row4_col5\" class=\"data row4 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row4_col6\" class=\"data row4 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row4_col7\" class=\"data row4 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row5_col0\" class=\"data row5 col0\" >a</td>\n",
       "      <td id=\"T_dd151_row5_col1\" class=\"data row5 col1\" >5</td>\n",
       "      <td id=\"T_dd151_row5_col2\" class=\"data row5 col2\" >a</td>\n",
       "      <td id=\"T_dd151_row5_col3\" class=\"data row5 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row5_col4\" class=\"data row5 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row5_col5\" class=\"data row5 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row5_col6\" class=\"data row5 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row5_col7\" class=\"data row5 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row6_col0\" class=\"data row6 col0\" >cousin</td>\n",
       "      <td id=\"T_dd151_row6_col1\" class=\"data row6 col1\" >6</td>\n",
       "      <td id=\"T_dd151_row6_col2\" class=\"data row6 col2\" >cousin</td>\n",
       "      <td id=\"T_dd151_row6_col3\" class=\"data row6 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row6_col4\" class=\"data row6 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row6_col5\" class=\"data row6 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row6_col6\" class=\"data row6 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row6_col7\" class=\"data row6 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row7_col0\" class=\"data row7 col0\" >,</td>\n",
       "      <td id=\"T_dd151_row7_col1\" class=\"data row7 col1\" >7</td>\n",
       "      <td id=\"T_dd151_row7_col2\" class=\"data row7 col2\" >,</td>\n",
       "      <td id=\"T_dd151_row7_col3\" class=\"data row7 col3\" >False</td>\n",
       "      <td id=\"T_dd151_row7_col4\" class=\"data row7 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row7_col5\" class=\"data row7 col5\" >True</td>\n",
       "      <td id=\"T_dd151_row7_col6\" class=\"data row7 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row7_col7\" class=\"data row7 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row8_col0\" class=\"data row8 col0\" >a</td>\n",
       "      <td id=\"T_dd151_row8_col1\" class=\"data row8 col1\" >8</td>\n",
       "      <td id=\"T_dd151_row8_col2\" class=\"data row8 col2\" >a</td>\n",
       "      <td id=\"T_dd151_row8_col3\" class=\"data row8 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row8_col4\" class=\"data row8 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row8_col5\" class=\"data row8 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row8_col6\" class=\"data row8 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row8_col7\" class=\"data row8 col7\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd151_row9_col0\" class=\"data row9 col0\" >spectacle</td>\n",
       "      <td id=\"T_dd151_row9_col1\" class=\"data row9 col1\" >9</td>\n",
       "      <td id=\"T_dd151_row9_col2\" class=\"data row9 col2\" >spectacle</td>\n",
       "      <td id=\"T_dd151_row9_col3\" class=\"data row9 col3\" >True</td>\n",
       "      <td id=\"T_dd151_row9_col4\" class=\"data row9 col4\" >False</td>\n",
       "      <td id=\"T_dd151_row9_col5\" class=\"data row9 col5\" >False</td>\n",
       "      <td id=\"T_dd151_row9_col6\" class=\"data row9 col6\" >False</td>\n",
       "      <td id=\"T_dd151_row9_col7\" class=\"data row9 col7\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12de06a58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_attributes = []\n",
    "\n",
    "for token in carafe:\n",
    "    sample_attributes.append({\n",
    "        'TEXT': token.text,\n",
    "        'INDEX': token.i,\n",
    "        'LOWERCASE': token.lower_,\n",
    "        'ALPHABETIC': token.is_alpha,\n",
    "        'DIGIT': token.is_digit,\n",
    "        'PUNCTUATION': token.is_punct,\n",
    "        'STARTS SENTENCE': token.is_sent_start,\n",
    "        'LIKE URL': token.like_url\n",
    "    })\n",
    "\n",
    "sample_attributes = pd.DataFrame(sample_attributes).head(10)\n",
    "sample_attributes.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbfa4",
   "metadata": {},
   "source": [
    "We'll discuss some of the more complex annotations later on, both in this session and others. For now, let's collect some simple information about each of the tokens in our document. We'll use list comprehension to do so. We'll also use the `.text` attribute for each token, since we only want the text representation. Otherwise, we'd be creating a list of generators, where each generator has all those attribute for every token! (This is why we made sure to only use `.text` in our work with _The Odyssey_ above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bde860e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "-----\n",
      "A kind in glass and a cousin a spectacle and nothing strange a single hurt color and an [...] \n",
      "\n",
      "Punctuation\n",
      "-----------\n",
      ", . , . .\n"
     ]
    }
   ],
   "source": [
    "words = ' '.join([token.text for token in carafe if token.is_alpha])\n",
    "punctuation = ' '.join([token.text for token in carafe if token.is_punct])\n",
    "\n",
    "print(\n",
    "    f\"Words\\n-----\\n{textwrap.shorten(words, width=100)}\",\n",
    "    f\"\\n\\nPunctuation\\n-----------\\n{punctuation}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a9135",
   "metadata": {},
   "source": [
    "Want some linguistic information? We can get that too. For example, here are prefixes and suffixes:\n",
    "\n",
    "```{margin} You might be wondering about those underscores...\n",
    "\n",
    "The syntax conventions of `spaCy` use an underscore to access the actual attribute information for a token. Using an attribute without the underscore will return an id, which the library uses internally to piece together output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21ba417c",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A\n",
      "+ Prefix: A\n",
      "+ Suffix: A\n",
      "\n",
      "Text: kind\n",
      "+ Prefix: k\n",
      "+ Suffix: ind\n",
      "\n",
      "Text: in\n",
      "+ Prefix: i\n",
      "+ Suffix: in\n",
      "\n",
      "Text: glass\n",
      "+ Prefix: g\n",
      "+ Suffix: ass\n",
      "\n",
      "Text: and\n",
      "+ Prefix: a\n",
      "+ Suffix: and\n",
      "\n",
      "Text: a\n",
      "+ Prefix: a\n",
      "+ Suffix: a\n",
      "\n",
      "Text: cousin\n",
      "+ Prefix: c\n",
      "+ Suffix: sin\n",
      "\n",
      "Text: a\n",
      "+ Prefix: a\n",
      "+ Suffix: a\n",
      "\n",
      "Text: spectacle\n",
      "+ Prefix: s\n",
      "+ Suffix: cle\n",
      "\n",
      "Text: and\n",
      "+ Prefix: a\n",
      "+ Suffix: and\n",
      "\n",
      "Text: nothing\n",
      "+ Prefix: n\n",
      "+ Suffix: ing\n",
      "\n",
      "Text: strange\n",
      "+ Prefix: s\n",
      "+ Suffix: nge\n",
      "\n",
      "Text: a\n",
      "+ Prefix: a\n",
      "+ Suffix: a\n",
      "\n",
      "Text: single\n",
      "+ Prefix: s\n",
      "+ Suffix: gle\n",
      "\n",
      "Text: hurt\n",
      "+ Prefix: h\n",
      "+ Suffix: urt\n",
      "\n",
      "Text: color\n",
      "+ Prefix: c\n",
      "+ Suffix: lor\n",
      "\n",
      "Text: and\n",
      "+ Prefix: a\n",
      "+ Suffix: and\n",
      "\n",
      "Text: an\n",
      "+ Prefix: a\n",
      "+ Suffix: an\n",
      "\n",
      "Text: arrangement\n",
      "+ Prefix: a\n",
      "+ Suffix: ent\n",
      "\n",
      "Text: in\n",
      "+ Prefix: i\n",
      "+ Suffix: in\n",
      "\n",
      "Text: a\n",
      "+ Prefix: a\n",
      "+ Suffix: a\n",
      "\n",
      "Text: system\n",
      "+ Prefix: s\n",
      "+ Suffix: tem\n",
      "\n",
      "Text: to\n",
      "+ Prefix: t\n",
      "+ Suffix: to\n",
      "\n",
      "Text: pointing\n",
      "+ Prefix: p\n",
      "+ Suffix: ing\n",
      "\n",
      "Text: All\n",
      "+ Prefix: A\n",
      "+ Suffix: All\n",
      "\n",
      "Text: this\n",
      "+ Prefix: t\n",
      "+ Suffix: his\n",
      "\n",
      "Text: and\n",
      "+ Prefix: a\n",
      "+ Suffix: and\n",
      "\n",
      "Text: not\n",
      "+ Prefix: n\n",
      "+ Suffix: not\n",
      "\n",
      "Text: ordinary\n",
      "+ Prefix: o\n",
      "+ Suffix: ary\n",
      "\n",
      "Text: not\n",
      "+ Prefix: n\n",
      "+ Suffix: not\n",
      "\n",
      "Text: unordered\n",
      "+ Prefix: u\n",
      "+ Suffix: red\n",
      "\n",
      "Text: in\n",
      "+ Prefix: i\n",
      "+ Suffix: in\n",
      "\n",
      "Text: not\n",
      "+ Prefix: n\n",
      "+ Suffix: not\n",
      "\n",
      "Text: resembling\n",
      "+ Prefix: r\n",
      "+ Suffix: ing\n",
      "\n",
      "Text: The\n",
      "+ Prefix: T\n",
      "+ Suffix: The\n",
      "\n",
      "Text: difference\n",
      "+ Prefix: d\n",
      "+ Suffix: nce\n",
      "\n",
      "Text: is\n",
      "+ Prefix: i\n",
      "+ Suffix: is\n",
      "\n",
      "Text: spreading\n",
      "+ Prefix: s\n",
      "+ Suffix: ing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix_suffix = [(token.text, token.prefix_, token.suffix_) for token in carafe if token.is_alpha]\n",
    "\n",
    "for i in prefix_suffix:\n",
    "    text, prefix, suffix = i[0], i[1], i[2]\n",
    "    print(\n",
    "        f\"Text: {text}\\n+ Prefix: {prefix}\\n+ Suffix: {suffix}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51d698",
   "metadata": {},
   "source": [
    "And here are lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5633040e",
   "metadata": {
    "scrolled": false,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text         | Lemma\n",
      "----------------------------\n",
      "A              a\n",
      "kind           kind\n",
      "in             in\n",
      "glass          glass\n",
      "and            and\n",
      "a              a\n",
      "cousin         cousin\n",
      "spectacle      spectacle\n",
      "nothing        nothing\n",
      "strange        strange\n",
      "single         single\n",
      "hurt           hurt\n",
      "color          color\n",
      "an             an\n",
      "arrangement    arrangement\n",
      "system         system\n",
      "to             to\n",
      "pointing       point\n",
      "All            all\n",
      "this           this\n",
      "not            not\n",
      "ordinary       ordinary\n",
      "unordered      unordere\n",
      "resembling     resemble\n",
      "The            the\n",
      "difference     difference\n",
      "is             be\n",
      "spreading      spread\n"
     ]
    }
   ],
   "source": [
    "lemmas = {token.text: token.lemma_ for token in carafe if token.is_alpha}\n",
    "\n",
    "print(\"Text         | Lemma\")\n",
    "print(\"-\" * 28)\n",
    "for text, lemma in lemmas.items():\n",
    "    print(\n",
    "        f\"{text:<15}{lemma}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9807b2",
   "metadata": {},
   "source": [
    "With such attributes at your disposal, you might imagine how you could work `spaCy` into a text mining pipeline. Instead of using separate functions to clean your corpus, those steps could all be accomplished by accessing attributes.\n",
    "\n",
    "Before you do this, however, you should consider two things: 1) whether the increased computational/memory overhead is worthwhile for your project; and 2) whether `spaCy`'s base models will work for the kind of text you're using. This second point is especially important. While `spaCy`'s base models are incredibly powerful, they are built for general purpose applications and may struggle with domain-specific language. Medical text and early modern print are two such examples of where the base models interpret your documents in unexpected ways, thereby complicating, maybe even ruining, parts of a text mining pipeline that relies on them. Sometimes, in other words, it's just best to stick with a text mining pipeline that you know to be effective.\n",
    "\n",
    "That all said, there are ways to train your own `spaCy` model on a specific domain. This can be an extensive process, one which exceeds the limits of our short workshop, but if you want to learn more about doing so, you can visit [this page]. There are also [third party models] available, which you might find useful, though your milage may vary.\n",
    "\n",
    "[this page]: https://spacy.io/usage/training\n",
    "[third party models]: https://spacy.io/universe/category/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577ce50",
   "metadata": {},
   "source": [
    "Part-of-Speech Tagging\n",
    "----------------------------\n",
    "\n",
    "One of the most common tasks in NLP involves assigning **part-of-speech, or POS, tags** to each token in a document. As we saw in the text mining series, these tags are a necessary step for certain text cleaning process, like lemmatization; you might also use them to identify subsets of your data, which you could separate out and model. Beyond text cleaning, POS tags can be useful for tasks like **word sense disambiguation**, where you try to determine which particular facet of meaning a given token represents.\n",
    "\n",
    "Regardless of the task, the process of getting POS tags from `spaCy` will be the same. Each token in a document has an associated tag, which is accessible as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6880cb70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text        | POS Tag\n",
      "---------------------\n",
      "A              DET\n",
      "kind           NOUN\n",
      "in             ADP\n",
      "glass          NOUN\n",
      "and            CCONJ\n",
      "a              DET\n",
      "cousin         NOUN\n",
      ",              PUNCT\n",
      "spectacle      NOUN\n",
      "nothing        PRON\n",
      "strange        ADJ\n",
      "single         ADJ\n",
      "hurt           NOUN\n",
      "color          NOUN\n",
      "an             DET\n",
      "arrangement    NOUN\n",
      "system         NOUN\n",
      "to             ADP\n",
      "pointing       VERB\n",
      ".              PUNCT\n",
      "All            DET\n",
      "this           DET\n",
      "not            PART\n",
      "ordinary       ADJ\n",
      "unordered      VERB\n",
      "resembling     VERB\n",
      "The            DET\n",
      "difference     NOUN\n",
      "is             AUX\n",
      "spreading      VERB\n"
     ]
    }
   ],
   "source": [
    "pos = {token.text: token.pos_ for token in carafe}\n",
    "\n",
    "print(\"Text        | POS Tag\")\n",
    "print(\"-\" * 21)\n",
    "for text, tag in pos.items():\n",
    "    print(f\"{text:<15}{tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ddd5d",
   "metadata": {},
   "source": [
    "If you don't know what a tag means, you can use `spacy.explain()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5d381a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coordinating conjunction'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('CCONJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429eccc9",
   "metadata": {},
   "source": [
    "`spaCy` actually has two types of POS tags. The ones accessible with the `.pos_` attribute are the basic tags, whereas those under `.tag_` are more detailed (these come from the [Penn Treebank project]). We'll print them out below, along with information about what they mean.\n",
    "\n",
    "[Penn Treebank project]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f11d6a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text        | POS Tag | Explanation\n",
      "--------------------------------------------------------------------------\n",
      "A               DT      determiner\n",
      "kind            NN      noun, singular or mass\n",
      "in              IN      conjunction, subordinating or preposition\n",
      "glass           NN      noun, singular or mass\n",
      "and             CC      conjunction, coordinating\n",
      "a               DT      determiner\n",
      "cousin          NN      noun, singular or mass\n",
      ",               ,       punctuation mark, comma\n",
      "spectacle       NN      noun, singular or mass\n",
      "nothing         NN      noun, singular or mass\n",
      "strange         JJ      adjective (English), other noun-modifier (Chinese)\n",
      "single          JJ      adjective (English), other noun-modifier (Chinese)\n",
      "hurt            NN      noun, singular or mass\n",
      "color           NN      noun, singular or mass\n",
      "an              DT      determiner\n",
      "arrangement     NN      noun, singular or mass\n",
      "system          NN      noun, singular or mass\n",
      "to              IN      conjunction, subordinating or preposition\n",
      "pointing        VBG     verb, gerund or present participle\n",
      ".               .       punctuation mark, sentence closer\n",
      "All             PDT     predeterminer\n",
      "this            DT      determiner\n",
      "not             RB      adverb\n",
      "ordinary        JJ      adjective (English), other noun-modifier (Chinese)\n",
      "unordered       VBN     verb, past participle\n",
      "resembling      VBG     verb, gerund or present participle\n",
      "The             DT      determiner\n",
      "difference      NN      noun, singular or mass\n",
      "is              VBZ     verb, 3rd person singular present\n",
      "spreading       VBG     verb, gerund or present participle\n"
     ]
    }
   ],
   "source": [
    "detailed_tags = {token.text: token.tag_ for token in carafe}\n",
    "\n",
    "print(\"Text        | POS Tag | Explanation\")\n",
    "print(\"-\" * 74)\n",
    "for text, tag in detailed_tags.items():\n",
    "    explanation = spacy.explain(tag)\n",
    "    print(\n",
    "        f\"{text:<16}{tag:<8}{explanation}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7307704",
   "metadata": {},
   "source": [
    "This is all well and good in the abstract, but the power of POS tags lies in how they support other kinds of analysis. We'll do a quick word sense disambiguation task here but will return to do something more complex in a little while.\n",
    "\n",
    "Between the two strings:\n",
    "\n",
    "1. \"I am not going to bank on that happening.\"\n",
    "2. \"I went down to the river bank.\"\n",
    "\n",
    "How can we tell which sense of the word \"bank\" is being used? Well, we can model each with `spaCy` and see whether the POS tags for these two tokens match. If they don't match, this will indicate that the tokens represent two different senses of the word \"bank.\"\n",
    "\n",
    "All this can be accomplished with a `for` loop and `nlp.pipe()`. The latter function enables you to process different documents with the `spaCy` model all at once. This can be great for working with a large corpus, though note that, because `.pipe()` is meant to work on text at scale, it will return a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c31aa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x12eef10c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banks = [\"I am not going to bank on that happening.\", \"I went down to the river bank.\"]\n",
    "\n",
    "nlp.pipe(banks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480afb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not going to bank on that happening.\n",
      "+ bank: VB (verb, base form)\n",
      "\n",
      "I went down to the river bank.\n",
      "+ bank: NN (noun, singular or mass)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(banks):\n",
    "    for token in doc:\n",
    "        if token.text == 'bank':\n",
    "            print(\n",
    "                f\"{doc.text}\\n+ {token.text}: \"\n",
    "                f\"{token.tag_} ({spacy.explain(token.tag_)})\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34942e8e",
   "metadata": {},
   "source": [
    "Dependency Parsing\n",
    "------------------------\n",
    "\n",
    "TODO: a discussion of dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c994f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6c85bdaa15f94ef48c37366586d2ebbb-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">difference</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">spreading.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c85bdaa15f94ef48c37366586d2ebbb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "to_render = list(carafe.sents)[2]\n",
    "displacy.render(to_render, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78735bd",
   "metadata": {},
   "source": [
    "Seeing these relationships are quite useful in and of themselves, but the real power of dependency parsing comes in all the extra data it can provide about a token. Using this technique, you can link tokens back to their heads, or find local groupings of tokens that all refer to the same head.\n",
    "\n",
    "Here's how you could formalize that with a dataframe. Given this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bed9a8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Then I tried to find some way of embracing my mother's ghost.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = odyssey[2246:2260]\n",
    "sentence.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee5276",
   "metadata": {},
   "source": [
    "We can construct a `for` loop, which rolls through each token and retrieves its dependency info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7453a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_55aff_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >INDEX</th>\n",
       "      <th class=\"col_heading level0 col1\" >TOKEN</th>\n",
       "      <th class=\"col_heading level0 col2\" >DEPENDENCY_SHORTCODE</th>\n",
       "      <th class=\"col_heading level0 col3\" >DEPENDENCY</th>\n",
       "      <th class=\"col_heading level0 col4\" >HEAD_INDEX</th>\n",
       "      <th class=\"col_heading level0 col5\" >HEAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row0_col0\" class=\"data row0 col0\" >2246</td>\n",
       "      <td id=\"T_55aff_row0_col1\" class=\"data row0 col1\" >Then</td>\n",
       "      <td id=\"T_55aff_row0_col2\" class=\"data row0 col2\" >advmod</td>\n",
       "      <td id=\"T_55aff_row0_col3\" class=\"data row0 col3\" >adverbial modifier</td>\n",
       "      <td id=\"T_55aff_row0_col4\" class=\"data row0 col4\" >2248</td>\n",
       "      <td id=\"T_55aff_row0_col5\" class=\"data row0 col5\" >tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row1_col0\" class=\"data row1 col0\" >2247</td>\n",
       "      <td id=\"T_55aff_row1_col1\" class=\"data row1 col1\" >I</td>\n",
       "      <td id=\"T_55aff_row1_col2\" class=\"data row1 col2\" >nsubj</td>\n",
       "      <td id=\"T_55aff_row1_col3\" class=\"data row1 col3\" >nominal subject</td>\n",
       "      <td id=\"T_55aff_row1_col4\" class=\"data row1 col4\" >2248</td>\n",
       "      <td id=\"T_55aff_row1_col5\" class=\"data row1 col5\" >tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row2_col0\" class=\"data row2 col0\" >2248</td>\n",
       "      <td id=\"T_55aff_row2_col1\" class=\"data row2 col1\" >tried</td>\n",
       "      <td id=\"T_55aff_row2_col2\" class=\"data row2 col2\" >ROOT</td>\n",
       "      <td id=\"T_55aff_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "      <td id=\"T_55aff_row2_col4\" class=\"data row2 col4\" >2248</td>\n",
       "      <td id=\"T_55aff_row2_col5\" class=\"data row2 col5\" >tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row3_col0\" class=\"data row3 col0\" >2249</td>\n",
       "      <td id=\"T_55aff_row3_col1\" class=\"data row3 col1\" >to</td>\n",
       "      <td id=\"T_55aff_row3_col2\" class=\"data row3 col2\" >aux</td>\n",
       "      <td id=\"T_55aff_row3_col3\" class=\"data row3 col3\" >auxiliary</td>\n",
       "      <td id=\"T_55aff_row3_col4\" class=\"data row3 col4\" >2250</td>\n",
       "      <td id=\"T_55aff_row3_col5\" class=\"data row3 col5\" >find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row4_col0\" class=\"data row4 col0\" >2250</td>\n",
       "      <td id=\"T_55aff_row4_col1\" class=\"data row4 col1\" >find</td>\n",
       "      <td id=\"T_55aff_row4_col2\" class=\"data row4 col2\" >xcomp</td>\n",
       "      <td id=\"T_55aff_row4_col3\" class=\"data row4 col3\" >open clausal complement</td>\n",
       "      <td id=\"T_55aff_row4_col4\" class=\"data row4 col4\" >2248</td>\n",
       "      <td id=\"T_55aff_row4_col5\" class=\"data row4 col5\" >tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row5_col0\" class=\"data row5 col0\" >2251</td>\n",
       "      <td id=\"T_55aff_row5_col1\" class=\"data row5 col1\" >some</td>\n",
       "      <td id=\"T_55aff_row5_col2\" class=\"data row5 col2\" >det</td>\n",
       "      <td id=\"T_55aff_row5_col3\" class=\"data row5 col3\" >determiner</td>\n",
       "      <td id=\"T_55aff_row5_col4\" class=\"data row5 col4\" >2252</td>\n",
       "      <td id=\"T_55aff_row5_col5\" class=\"data row5 col5\" >way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row6_col0\" class=\"data row6 col0\" >2252</td>\n",
       "      <td id=\"T_55aff_row6_col1\" class=\"data row6 col1\" >way</td>\n",
       "      <td id=\"T_55aff_row6_col2\" class=\"data row6 col2\" >dobj</td>\n",
       "      <td id=\"T_55aff_row6_col3\" class=\"data row6 col3\" >direct object</td>\n",
       "      <td id=\"T_55aff_row6_col4\" class=\"data row6 col4\" >2250</td>\n",
       "      <td id=\"T_55aff_row6_col5\" class=\"data row6 col5\" >find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row7_col0\" class=\"data row7 col0\" >2253</td>\n",
       "      <td id=\"T_55aff_row7_col1\" class=\"data row7 col1\" >of</td>\n",
       "      <td id=\"T_55aff_row7_col2\" class=\"data row7 col2\" >prep</td>\n",
       "      <td id=\"T_55aff_row7_col3\" class=\"data row7 col3\" >prepositional modifier</td>\n",
       "      <td id=\"T_55aff_row7_col4\" class=\"data row7 col4\" >2252</td>\n",
       "      <td id=\"T_55aff_row7_col5\" class=\"data row7 col5\" >way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row8_col0\" class=\"data row8 col0\" >2254</td>\n",
       "      <td id=\"T_55aff_row8_col1\" class=\"data row8 col1\" >embracing</td>\n",
       "      <td id=\"T_55aff_row8_col2\" class=\"data row8 col2\" >pcomp</td>\n",
       "      <td id=\"T_55aff_row8_col3\" class=\"data row8 col3\" >complement of preposition</td>\n",
       "      <td id=\"T_55aff_row8_col4\" class=\"data row8 col4\" >2253</td>\n",
       "      <td id=\"T_55aff_row8_col5\" class=\"data row8 col5\" >of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row9_col0\" class=\"data row9 col0\" >2255</td>\n",
       "      <td id=\"T_55aff_row9_col1\" class=\"data row9 col1\" >my</td>\n",
       "      <td id=\"T_55aff_row9_col2\" class=\"data row9 col2\" >poss</td>\n",
       "      <td id=\"T_55aff_row9_col3\" class=\"data row9 col3\" >possession modifier</td>\n",
       "      <td id=\"T_55aff_row9_col4\" class=\"data row9 col4\" >2256</td>\n",
       "      <td id=\"T_55aff_row9_col5\" class=\"data row9 col5\" >mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row10_col0\" class=\"data row10 col0\" >2256</td>\n",
       "      <td id=\"T_55aff_row10_col1\" class=\"data row10 col1\" >mother</td>\n",
       "      <td id=\"T_55aff_row10_col2\" class=\"data row10 col2\" >poss</td>\n",
       "      <td id=\"T_55aff_row10_col3\" class=\"data row10 col3\" >possession modifier</td>\n",
       "      <td id=\"T_55aff_row10_col4\" class=\"data row10 col4\" >2258</td>\n",
       "      <td id=\"T_55aff_row10_col5\" class=\"data row10 col5\" >ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row11_col0\" class=\"data row11 col0\" >2257</td>\n",
       "      <td id=\"T_55aff_row11_col1\" class=\"data row11 col1\" >'s</td>\n",
       "      <td id=\"T_55aff_row11_col2\" class=\"data row11 col2\" >case</td>\n",
       "      <td id=\"T_55aff_row11_col3\" class=\"data row11 col3\" >case marking</td>\n",
       "      <td id=\"T_55aff_row11_col4\" class=\"data row11 col4\" >2256</td>\n",
       "      <td id=\"T_55aff_row11_col5\" class=\"data row11 col5\" >mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row12_col0\" class=\"data row12 col0\" >2258</td>\n",
       "      <td id=\"T_55aff_row12_col1\" class=\"data row12 col1\" >ghost</td>\n",
       "      <td id=\"T_55aff_row12_col2\" class=\"data row12 col2\" >dobj</td>\n",
       "      <td id=\"T_55aff_row12_col3\" class=\"data row12 col3\" >direct object</td>\n",
       "      <td id=\"T_55aff_row12_col4\" class=\"data row12 col4\" >2254</td>\n",
       "      <td id=\"T_55aff_row12_col5\" class=\"data row12 col5\" >embracing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_55aff_row13_col0\" class=\"data row13 col0\" >2259</td>\n",
       "      <td id=\"T_55aff_row13_col1\" class=\"data row13 col1\" >.</td>\n",
       "      <td id=\"T_55aff_row13_col2\" class=\"data row13 col2\" >punct</td>\n",
       "      <td id=\"T_55aff_row13_col3\" class=\"data row13 col3\" >punctuation</td>\n",
       "      <td id=\"T_55aff_row13_col4\" class=\"data row13 col4\" >2248</td>\n",
       "      <td id=\"T_55aff_row13_col5\" class=\"data row13 col5\" >tried</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12eef29e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies = []\n",
    "for token in sentence:\n",
    "    dependencies.append({\n",
    "        'INDEX': token.i,\n",
    "        'TOKEN': token.text,\n",
    "        'DEPENDENCY_SHORTCODE': token.dep_,\n",
    "        'DEPENDENCY': spacy.explain(token.dep_),\n",
    "        'HEAD_INDEX': token.head.i,\n",
    "        'HEAD': token.head\n",
    "    })\n",
    "    \n",
    "dependencies = pd.DataFrame(dependencies)\n",
    "dependencies.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3c7b8",
   "metadata": {},
   "source": [
    "How many tokens are associated with each head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5b7503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HEAD\n",
       "tried        5\n",
       "find         2\n",
       "way          2\n",
       "of           1\n",
       "embracing    1\n",
       "mother       2\n",
       "ghost        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies.groupby('HEAD').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fa432",
   "metadata": {},
   "source": [
    "Which tokens are in each of these groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88d32fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head     | Group\n",
      "----------------------------------------------\n",
      "tried     ['Then', 'I', 'tried', 'find', '.']\n",
      "find      ['to', 'way']\n",
      "way       ['some', 'of']\n",
      "of        ['embracing']\n",
      "embracing ['ghost']\n",
      "mother    ['my', \"'s\"]\n",
      "ghost     ['mother']\n"
     ]
    }
   ],
   "source": [
    "print(\"Head     | Group\")\n",
    "print(\"-\" * 46)\n",
    "for group in dependencies.groupby('HEAD'):\n",
    "    head, tokens = group[0].text, group[1]['TOKEN'].tolist()\n",
    "    print(f\"{head:<10}{tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301f967",
   "metadata": {},
   "source": [
    "`sciPy` also has a special `.subtree` attribute for each token, which will also produce a similar set of local groupings. Note however that `.subtree` captures all tokens that hold a dependent relationship with the one in question, meaning that when you find the subtree of the root, you're going to print out the entire sentence.\n",
    "\n",
    "As you might expect by now, `.subtree` returns a generator, so convert it to a list or use list comprehension to extract the tokens. We'll do this in a separate function. Within this function, we're going to use the `.text_with_ws` attribute of each token in the subtree to return an exact, string-like representation of the tree (this will include any whitespace characters that are attached to a token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0439f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head      | Dep.  | Subtree\n",
      "----------------------------------------------------------------------------------\n",
      "Then        advmod  Then\n",
      "I           nsubj   I\n",
      "tried       ROOT    \"Then I tried to find some way of embracing my mother's ghost.\n",
      "to          aux     to\n",
      "find        xcomp   to find some way of embracing my mother's ghost\n",
      "some        det     some\n",
      "way         dobj    some way of embracing my mother's ghost\n",
      "of          prep    of embracing my mother's ghost\n",
      "embracing   pcomp   embracing my mother's ghost\n",
      "my          poss    my\n",
      "mother      poss    my mother's\n",
      "'s          case    's\n",
      "ghost       dobj    my mother's ghost\n",
      ".           punct   .\n"
     ]
    }
   ],
   "source": [
    "def subtree_to_text(subtree):\n",
    "    subtree = [token.text_with_ws for token in token.subtree]\n",
    "    return ''.join(subtree)\n",
    "\n",
    "print(\"Head      | Dep.  | Subtree\")\n",
    "print(\"-\" * 82)\n",
    "for token in sentence:\n",
    "    subtree = subtree_to_text(token.subtree)\n",
    "    subtree = subtree.strip()\n",
    "    print(f\"{token.text:<12}{token.dep_:<8}{subtree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fb35d",
   "metadata": {},
   "source": [
    "Putting Everything Together\n",
    "---------------------------------\n",
    "\n",
    "Now that we've walked through all these options (which are really only a small sliver of what you can do with `spaCy`!), let's put them into action. Below, we'll construct two short examples of how you might combine different aspects of token attributes to analyze a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e946e62",
   "metadata": {},
   "source": [
    "### Finding lemmas\n",
    "\n",
    "In the first, we'll use the `.lemma_` attribute to search through Book XI of _The Odyssey_ and match its tokens to a few key words. If you've read _The Odyssey_, you'll know that Book XI is where Odysseus and his fellow sailors have to travel down to the underworld Hades, where they speak with the dead. We already saw one example of this: Odysseus attempts to embrace his dead mother after communing with her. The whole trip to Hades is an emotionally tumultuous experience for the travelers, and peppered throughout Book XI are expressions of grief.\n",
    "\n",
    "With `.lemma`, we can search for these expressions. We'll roll through the text and determine whether a token lemma matches one of a selected set. When we find a match, we'll get the subtree of this token's _head_. That is, we'll find the head upon which this token depends, and then we'll use that to reconstruct the local context for the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22ebc67d",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weeping\n",
      "+ weeping and in great distress of mind\n",
      "\n",
      "cried\n",
      "+ cried when I saw him: 'Elpenor\n",
      "\n",
      "sad\n",
      "+ sad \n",
      "\n",
      "tears\n",
      "+ tears \n",
      "\n",
      "sorrow\n",
      "+ all my sorrow \n",
      "\n",
      "sad\n",
      "+ sad \n",
      "\n",
      "tears\n",
      "+ tears \n",
      "\n",
      "grieves\n",
      "+ He grieves continually about your never having come home, and suffers more and more as he grows older. \n",
      "\n",
      "sad\n",
      "+ sad \n",
      "\n",
      "sorrows\n",
      "+ our sorrows \n",
      "\n",
      "grief\n",
      "+ grief \n",
      "\n",
      "grief\n",
      "+ great grief \n",
      "\n",
      "grief\n",
      "+ grief\n",
      "\n",
      "sadder\n",
      "+ still sadder \n",
      "\n",
      "weeping\n",
      "+ weeping \n",
      "\n",
      "wept\n",
      "+ I too wept and pitied him as I beheld him. '\n",
      "\n",
      "weeping\n",
      "+ weeping and talking thus sadly with one another \n",
      "\n",
      "tear\n",
      "+ a tear \n",
      "\n",
      "cries\n",
      "+ such appalling cries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in odyssey:\n",
    "    if token.lemma_ in ('cry', 'grief', 'grieve', 'sad', 'sorrow', 'tear', 'weep'):\n",
    "        subtree = subtree_to_text(token.head.subtree)\n",
    "        print(token.text)\n",
    "        print(f\"+ {subtree}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134e32f",
   "metadata": {},
   "source": [
    "### Verb-subject relations\n",
    "\n",
    "For this next example, we'll use dependency tags to find the subject sentences in Book XI. As before, we'll go through each token in the document, this time checking to see whether it has the `nsubj` or `nsubjpass` tag for its `.dep_` attribute. These refer to the subjects of the sentence's root. We'll also check to see whether a token is a noun (otherwise we'd get a lot of articles like 'who', 'them', etc.). If a token matches these two conditions, we'll find its head verb as well as the token's subtree. Note that this time, the subtree will refer directly to the token in question, not to the head. This will let us capture some descriptive information about each sentence subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3315a72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e73cd_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >SUBJECT</th>\n",
       "      <th class=\"col_heading level0 col1\" >HEAD</th>\n",
       "      <th class=\"col_heading level0 col2\" >HEAD_LEMMA</th>\n",
       "      <th class=\"col_heading level0 col3\" >SUBTREE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row0_col0\" class=\"data row0 col0\" >Circe</td>\n",
       "      <td id=\"T_e73cd_row0_col1\" class=\"data row0 col1\" >sent</td>\n",
       "      <td id=\"T_e73cd_row0_col2\" class=\"data row0 col2\" >send</td>\n",
       "      <td id=\"T_e73cd_row0_col3\" class=\"data row0 col3\" >Circe, that great and cunning goddess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row1_col0\" class=\"data row1 col0\" >sails</td>\n",
       "      <td id=\"T_e73cd_row1_col1\" class=\"data row1 col1\" >were</td>\n",
       "      <td id=\"T_e73cd_row1_col2\" class=\"data row1 col2\" >be</td>\n",
       "      <td id=\"T_e73cd_row1_col3\" class=\"data row1 col3\" >her sails </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row2_col0\" class=\"data row2 col0\" >sun</td>\n",
       "      <td id=\"T_e73cd_row2_col1\" class=\"data row2 col1\" >went</td>\n",
       "      <td id=\"T_e73cd_row2_col2\" class=\"data row2 col2\" >go</td>\n",
       "      <td id=\"T_e73cd_row2_col3\" class=\"data row2 col3\" >the sun </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row3_col0\" class=\"data row3 col0\" >darkness</td>\n",
       "      <td id=\"T_e73cd_row3_col1\" class=\"data row3 col1\" >was</td>\n",
       "      <td id=\"T_e73cd_row3_col2\" class=\"data row3 col2\" >be</td>\n",
       "      <td id=\"T_e73cd_row3_col3\" class=\"data row3 col3\" >darkness </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row4_col0\" class=\"data row4 col0\" >rays</td>\n",
       "      <td id=\"T_e73cd_row4_col1\" class=\"data row4 col1\" >pierce</td>\n",
       "      <td id=\"T_e73cd_row4_col2\" class=\"data row4 col2\" >pierce</td>\n",
       "      <td id=\"T_e73cd_row4_col3\" class=\"data row4 col3\" >the rays of the sun </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row5_col0\" class=\"data row5 col0\" >wretches</td>\n",
       "      <td id=\"T_e73cd_row5_col1\" class=\"data row5 col1\" >live</td>\n",
       "      <td id=\"T_e73cd_row5_col2\" class=\"data row5 col2\" >live</td>\n",
       "      <td id=\"T_e73cd_row5_col3\" class=\"data row5 col3\" >the poor wretches </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row6_col0\" class=\"data row6 col0\" >Circe</td>\n",
       "      <td id=\"T_e73cd_row6_col1\" class=\"data row6 col1\" >told</td>\n",
       "      <td id=\"T_e73cd_row6_col2\" class=\"data row6 col2\" >tell</td>\n",
       "      <td id=\"T_e73cd_row6_col3\" class=\"data row6 col3\" >Circe </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row7_col0\" class=\"data row7 col0\" >Perimedes</td>\n",
       "      <td id=\"T_e73cd_row7_col1\" class=\"data row7 col1\" >held</td>\n",
       "      <td id=\"T_e73cd_row7_col2\" class=\"data row7 col2\" >hold</td>\n",
       "      <td id=\"T_e73cd_row7_col3\" class=\"data row7 col3\" >Perimedes and Eurylochus </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row8_col0\" class=\"data row8 col0\" >Teiresias</td>\n",
       "      <td id=\"T_e73cd_row8_col1\" class=\"data row8 col1\" >have</td>\n",
       "      <td id=\"T_e73cd_row8_col2\" class=\"data row8 col2\" >have</td>\n",
       "      <td id=\"T_e73cd_row8_col3\" class=\"data row8 col3\" >Teiresias </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row9_col0\" class=\"data row9 col0\" >blood</td>\n",
       "      <td id=\"T_e73cd_row9_col1\" class=\"data row9 col1\" >run</td>\n",
       "      <td id=\"T_e73cd_row9_col2\" class=\"data row9 col2\" >run</td>\n",
       "      <td id=\"T_e73cd_row9_col3\" class=\"data row9 col3\" >the blood </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row10_col0\" class=\"data row10 col0\" >armour</td>\n",
       "      <td id=\"T_e73cd_row10_col1\" class=\"data row10 col1\" >smirched</td>\n",
       "      <td id=\"T_e73cd_row10_col2\" class=\"data row10 col2\" >smirch</td>\n",
       "      <td id=\"T_e73cd_row10_col3\" class=\"data row10 col3\" >their armour </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row11_col0\" class=\"data row11 col0\" >ghosts</td>\n",
       "      <td id=\"T_e73cd_row11_col1\" class=\"data row11 col1\" >come</td>\n",
       "      <td id=\"T_e73cd_row11_col2\" class=\"data row11 col2\" >come</td>\n",
       "      <td id=\"T_e73cd_row11_col3\" class=\"data row11 col3\" >the poor feckless ghosts </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row12_col0\" class=\"data row12 col0\" >Teiresias</td>\n",
       "      <td id=\"T_e73cd_row12_col1\" class=\"data row12 col1\" >answered</td>\n",
       "      <td id=\"T_e73cd_row12_col2\" class=\"data row12 col2\" >answer</td>\n",
       "      <td id=\"T_e73cd_row12_col3\" class=\"data row12 col3\" >Teiresias </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row13_col0\" class=\"data row13 col0\" >ghost</td>\n",
       "      <td id=\"T_e73cd_row13_col1\" class=\"data row13 col1\" >was</td>\n",
       "      <td id=\"T_e73cd_row13_col2\" class=\"data row13 col2\" >be</td>\n",
       "      <td id=\"T_e73cd_row13_col3\" class=\"data row13 col3\" >The first ghost 'that came </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row14_col0\" class=\"data row14 col0\" >ghost</td>\n",
       "      <td id=\"T_e73cd_row14_col1\" class=\"data row14 col1\" >saying</td>\n",
       "      <td id=\"T_e73cd_row14_col2\" class=\"data row14 col2\" >say</td>\n",
       "      <td id=\"T_e73cd_row14_col3\" class=\"data row14 col3\" >the ghost of my comrade </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row15_col0\" class=\"data row15 col0\" >ghost</td>\n",
       "      <td id=\"T_e73cd_row15_col1\" class=\"data row15 col1\" >came</td>\n",
       "      <td id=\"T_e73cd_row15_col2\" class=\"data row15 col2\" >come</td>\n",
       "      <td id=\"T_e73cd_row15_col3\" class=\"data row15 col3\" >the ghost of my dead mother Anticlea, daughter to Autolycus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row16_col0\" class=\"data row16 col0\" >heaven</td>\n",
       "      <td id=\"T_e73cd_row16_col1\" class=\"data row16 col1\" >make</td>\n",
       "      <td id=\"T_e73cd_row16_col2\" class=\"data row16 col2\" >make</td>\n",
       "      <td id=\"T_e73cd_row16_col3\" class=\"data row16 col3\" >heaven </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row17_col0\" class=\"data row17 col0\" >ship</td>\n",
       "      <td id=\"T_e73cd_row17_col1\" class=\"data row17 col1\" >reaches</td>\n",
       "      <td id=\"T_e73cd_row17_col2\" class=\"data row17 col2\" >reach</td>\n",
       "      <td id=\"T_e73cd_row17_col3\" class=\"data row17 col3\" >your ship </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row18_col0\" class=\"data row18 col0\" >hardship</td>\n",
       "      <td id=\"T_e73cd_row18_col1\" class=\"data row18 col1\" >reach</td>\n",
       "      <td id=\"T_e73cd_row18_col2\" class=\"data row18 col2\" >reach</td>\n",
       "      <td id=\"T_e73cd_row18_col3\" class=\"data row18 col3\" >much hardship </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row19_col0\" class=\"data row19 col0\" >people</td>\n",
       "      <td id=\"T_e73cd_row19_col1\" class=\"data row19 col1\" >heard</td>\n",
       "      <td id=\"T_e73cd_row19_col2\" class=\"data row19 col2\" >hear</td>\n",
       "      <td id=\"T_e73cd_row19_col3\" class=\"data row19 col3\" >the people </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row20_col0\" class=\"data row20 col0\" >wayfarer</td>\n",
       "      <td id=\"T_e73cd_row20_col1\" class=\"data row20 col1\" >meet</td>\n",
       "      <td id=\"T_e73cd_row20_col2\" class=\"data row20 col2\" >meet</td>\n",
       "      <td id=\"T_e73cd_row20_col3\" class=\"data row20 col3\" >A wayfarer </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row21_col0\" class=\"data row21 col0\" >death</td>\n",
       "      <td id=\"T_e73cd_row21_col1\" class=\"data row21 col1\" >come</td>\n",
       "      <td id=\"T_e73cd_row21_col2\" class=\"data row21 col2\" >come</td>\n",
       "      <td id=\"T_e73cd_row21_col3\" class=\"data row21 col3\" >death </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row22_col0\" class=\"data row22 col0\" >life</td>\n",
       "      <td id=\"T_e73cd_row22_col1\" class=\"data row22 col1\" >ebb</td>\n",
       "      <td id=\"T_e73cd_row22_col2\" class=\"data row22 col2\" >ebb</td>\n",
       "      <td id=\"T_e73cd_row22_col3\" class=\"data row22 col3\" >your life </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row23_col0\" class=\"data row23 col0\" >people</td>\n",
       "      <td id=\"T_e73cd_row23_col1\" class=\"data row23 col1\" >bless</td>\n",
       "      <td id=\"T_e73cd_row23_col2\" class=\"data row23 col2\" >bless</td>\n",
       "      <td id=\"T_e73cd_row23_col3\" class=\"data row23 col3\" >your people </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e73cd_row24_col0\" class=\"data row24 col0\" >ghost</td>\n",
       "      <td id=\"T_e73cd_row24_col1\" class=\"data row24 col1\" >close</td>\n",
       "      <td id=\"T_e73cd_row24_col2\" class=\"data row24 col2\" >close</td>\n",
       "      <td id=\"T_e73cd_row24_col3\" class=\"data row24 col3\" >my poor mother's ghost </td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13194a9b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj = []\n",
    "for token in odyssey:\n",
    "    if token.dep_ in ('nsubj', 'nsubjpass') and token.pos_ in ('NOUN', 'PROPN'):\n",
    "        nsubj.append({\n",
    "            'SUBJECT': token.text,\n",
    "            'HEAD': token.head.text,\n",
    "            'HEAD_LEMMA': token.head.lemma_,\n",
    "            'SUBTREE': subtree_to_text(token.subtree)\n",
    "        })\n",
    "\n",
    "nsubj_df = pd.DataFrame(nsubj)\n",
    "nsubj_df.head(25).style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fe7f1",
   "metadata": {},
   "source": [
    "How many time do each of our selected subjects appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7f4e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT\n",
       "ghost           8\n",
       "Ulysses         5\n",
       "ghosts          4\n",
       "heaven          4\n",
       "wife            4\n",
       "Proserpine      4\n",
       "man             4\n",
       "one             4\n",
       "Alcinous        3\n",
       "people          3\n",
       "Neleus          2\n",
       "ship            2\n",
       "judgement       2\n",
       "Theseus         2\n",
       "Teiresias       2\n",
       "gods            2\n",
       "mother          2\n",
       "life            2\n",
       "wind            2\n",
       "Circe           2\n",
       "Clytemnestra    2\n",
       "Hercules        2\n",
       "Jove            2\n",
       "creature        2\n",
       "prisoners       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj_df.groupby('SUBJECT').size().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd680bc",
   "metadata": {},
   "source": [
    "What heads are associated with each subject? (Note that we're using the lemmatized form of the verbs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ada4b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT     HEAD_LEMMA\n",
       "ghost       come          3\n",
       "Ulysses     answer        2\n",
       "Proserpine  send          2\n",
       "one         invite        1\n",
       "man         do            1\n",
       "            kill          1\n",
       "mother      answer        1\n",
       "            come          1\n",
       "one         be            1\n",
       "            get           1\n",
       "Aegisthus   be            1\n",
       "man         be            1\n",
       "one         tell          1\n",
       "others      fall          1\n",
       "people      be            1\n",
       "            bless         1\n",
       "            hear          1\n",
       "man         cross         1\n",
       "limbs       fail          1\n",
       "property    be            1\n",
       "heaven      take          1\n",
       "ground      reek          1\n",
       "guest       be            1\n",
       "guests      sit           1\n",
       "hardship    reach         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsubj_df.groupby(['SUBJECT', 'HEAD_LEMMA']).size().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8326545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
